{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Get started","text":"<p> Declarai, turning Python code into LLM tasks, easy to use, and production-ready. </p>"},{"location":"#introduction","title":"Introduction","text":"<p>Declarai turns your Python code into LLM tasks, utilizing python's native syntax, like type hints and docstrings, to instruct an AI model on what to do.</p> <p>Designed with a clear focus on developer experience, you simply write Python code as you normally would, and Declarai handles the rest.</p> <p>poem_generator.py<pre><code>import declarai\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n@gpt_35.task\ndef generate_poem(title: str) -&gt; str:\n\"\"\"\n    Write a 4 line poem on the provided title\n    \"\"\"\n\n\nres = generate_poem(\n    title=\"Declarai, the declarative framework for LLMs\"\n)\nprint(res)\n</code></pre> <pre><code>&gt;&gt;&gt; Declarai, the AI framework,\n... Empowers LLMs with declarative power,\n... Efficiently transforming data and knowledge,\n... Unlocking insights in every hour.\n</code></pre></p>"},{"location":"#why-use-declarai","title":"Why use Declarai?","text":"<ul> <li> <p>Pythonic Interface to LLM: - Leverage your existing Python skills instead of spending unnecessary time creating complex prompts.</p> </li> <li> <p>Lightweight: - Declarai is written almost solely in python 3.6 using only pydantic and openai SDKs, so there's no need to worry about dependency spaghetti.</p> </li> <li> <p>Extendable: - Declarai is designed to be easily extendable, the interface is simple and accessible by design so   you can easily override or customize the behavior of the framework to your specific needs.</p> </li> <li> <p>Type-Driven Prompt Design: - Through the application of Python's type annotations,    Declarai constructs detailed prompts that guide Large Language Models (LLMs) to generate the desired output type.</p> </li> <li> <p>Context-Informed Prompts via Docstrings: - Implement function docstrings to supply contextual data to LLMs,    augmenting their comprehension of the designated task, thereby boosting their performance.</p> </li> <li> <p>Automated Execution of LLM Tasks: - Declarai takes care of the execution code, letting you concentrate on the core business logic.</p> </li> </ul> <p>Utilizing Declarai leads to improved code readability, maintainability, and predictability.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>$ pip install declarai\n\n---&gt; 100%\nDone!\n</code></pre>"},{"location":"#feature-highlight","title":"Feature highlight","text":""},{"location":"#python-native-syntax","title":"Python native syntax","text":"<p>Integrating deeply into python's native syntax, declarai understands your code and generates the prompt accordingly.</p> Simple Syntax<pre><code>import declarai\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n@gpt_35.task # (1)!\ndef rank_by_severity(message: str) -&gt; int: # (2)!\n\"\"\"\n    Rank the severity of the provided message by it's urgency.\n    Urgency is ranked on a scale of 1-5, with 5 being the most urgent.\n    :param message: The message to rank\n    :return: The urgency of the message\n    \"\"\" # (3)!\n\n\nprint(rank_by_severity(message=\"The server is down!\"))\n#&gt; 5\nprint(rank_by_severity(message=\"How was your weekend?\"))\n#&gt; 1\n</code></pre> <ol> <li>The <code>@openai.task</code> decorator marks the function as a Declarai prompt task.</li> <li>The type hint <code>int</code> is used to parse the output of the llm into a integer.</li> <li>The docstring represents the task's description which is used to generate the prompt.<ul> <li><code>description</code> - the context of the task</li> <li><code>:param</code> - The function's parameters and their description</li> <li><code>:return</code> - The output description</li> </ul> </li> </ol>"},{"location":"#support-python-typing-and-pydantic-models","title":"Support Python typing and pydantic models","text":"<p>Declarai will return a serialized object as defined by the type hints at runtime. Builtins<pre><code>from typing import List\nimport declarai\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n@gpt_35.task\ndef extract_phone_number(email_content: str) -&gt; List[str]:\n\"\"\"\n    Extract the phone numbers from the provided email_content\n    :param email_content: Text that represents the email content \n    :return: The phone numbers that are used in the email\n    \"\"\"\n\nprint(extract_phone_number(email_content=\"Hi, my phone number is 123-456-7890\"))\n#&gt; ['123-456-7890']\n</code></pre></p> Builtins<pre><code>from datetime import datetime\nimport declarai\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n@gpt_35.task\ndef datetime_parser(raw_date: str) -&gt; datetime:\n\"\"\"\n    Parse the input into a valid datetime string of the format YYYY-mm-ddThh:mm:ss\n    :param raw_date: The provided raw date\n    :return: The parsed datetime output\n    \"\"\"\n\n\nprint(datetime_parser(raw_date=\"Janury 1st 2020\"))\n#&gt; 2020-01-01 00:00:00\n</code></pre> <p>Pydantic models<pre><code>from pydantic import BaseModel, Field\nfrom typing import Dict, List\nimport declarai\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\nclass Animal(BaseModel):\n    name: str\n    family: str\n    leg_count: int = Field(description=\"The number of legs\")\n\n\n@gpt_35.task\ndef suggest_animals(location: str) -&gt; Dict[int, List[Animal]]:\n\"\"\"\n    Create a list of numbers from 0 to 5\n    for each number, suggest a list of animals with that number of legs\n    :param location: The location where the animals can be found\n    :return: A list of animal leg count and for each count, the corresponding animals\n    \"\"\"\n\n\nprint(suggest_animals(location=\"jungle\"))\n#&gt; {\n#       0: [\n#           Animal(name='snake', family='reptile', leg_count=0)\n#       ], \n#       2: [\n#           Animal(name='monkey', family='mammal', leg_count=2), \n#           Animal(name='parrot', family='bird', leg_count=2)\n#       ], \n#       4: [\n#          Animal(name='tiger', family='mammal', leg_count=4), \n#          Animal(name='elephant', family='mammal', leg_count=4)\n#       ]\n# }\n</code></pre> </p>"},{"location":"#chat-interface","title":"Chat interface","text":"<p>Create chat interfaces with ease, simply by writing a class with docstrings</p> <p>Info</p> <p>Notice that <code>chat</code> is exposed under the <code>experimental</code> namespace, noting this interface is still work in progress.</p> <p><pre><code>import declarai\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n@gpt_35.experimental.chat\nclass CalculatorBot:\n\"\"\"\n    You a calculator bot,\n    given a request, you will return the result of the calculation\n    \"\"\"\n\n    def send(self, message: str) -&gt; int: ...\n\n\ncalc_bot = CalculatorBot()\nprint(calc_bot.send(message=\"1 + 1\"))\n#&gt; 2\n</code></pre> </p>"},{"location":"#task-middlewares","title":"Task Middlewares","text":"<p>Easy to use middlewares provided out of the box as well as the ability to easily create your own.</p> <p>Logging Middleware<pre><code>from declarai.middleware import LoggingMiddleware\nfrom typing import Dict\nimport declarai\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n@gpt_35.task(middlewares=[LoggingMiddleware])\ndef extract_info(text: str) -&gt; Dict[str, str]:\n\"\"\"\n    Extract the phone number, name and email from the provided text\n    :param text: content to extract the info from\n    :return: The info extracted from the text\n    \"\"\"\n    return declarai.magic(text=text)\n\nres = extract_info(\n    text=\"Hey jenny,\"\n    \"you can call me at 124-3435-132.\"\n    \"You can also email me at georgia@coolmail.com\"\n    \"Have a great week!\"\n)\nprint(res)\n</code></pre> Result: <pre><code>{'task_name': 'extract_info', 'llm_model': 'gpt-3.5-turbo-0301', 'template': '{input_instructions}\\n{input_placeholder}\\n{output_instructions}', 'template_args': {'input_instructions': 'Extract the phone number, name and email from the provided text', 'input_placeholder': 'Inputs:\\ntext: {text}\\n', 'output_instructions': 'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \\'```json\\' and \\'```\\':\\n```json\\n{{\\n    \"declarai_result\": Dict[str, str]  # The info extracted from the text\\n}}\\n```'}, 'prompt_config': {'structured': True, 'multi_results': False, 'return_name': 'declarai_result', 'temperature': 0.0, 'max_tokens': 2000, 'top_p': 1.0, 'frequency_penalty': 0, 'presence_penalty': 0}, 'call_kwargs': {'text': 'Hey jenny,you can call me at 124-3435-132.You can also email me at georgia@coolmail.comHave a great week!'}, 'result': {'phone_number': '124-3435-132', 'name': 'jenny', 'email': 'georgia@coolmail.com'}, 'time': 2.192906141281128}\n{'phone_number': '124-3435-132', 'name': 'jenny', 'email': 'georgia@coolmail.com'}\n</code></pre></p> <p>We highly recommend you to go through the beginner's guide to get a better understanding of the library and its capabilities - Beginner's Guide</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v0110","title":"v0.1.10","text":"<p>View full release on GitHub and PyPi</p> <p>Streaming, Docstring Templates, and System Message Fixes</p> <p>New features around streaming for chat tasks and the use of Jinja templates in docstrings, plus a crucial fix for system messages in chat.</p> <p>Features:</p> <ul> <li>Streaming for chat tasks.</li> <li>Support for Jinja templates in docstrings.</li> </ul> <p>Fixes:</p> <ul> <li>System message correctly set when passed through the chat decorator.</li> </ul> <p>Changes:</p> <ul> <li>Added streaming support with middleware for chat tasks as well. Closes #112.</li> <li>Typos in README fixed.</li> <li>Jinja templates in docstrings supported. Closes #118.</li> <li>Fixed issue with passing system messages in chat decorator. Closes #121.</li> </ul> <p>Contributors:</p> <ul> <li>@matankley</li> </ul>"},{"location":"changelog/#v019","title":"v0.1.9","text":"<p>View full release on GitHub and PyPi</p> <p>Streaming, Middleware, and API Refactoring</p> <p>Introducing the support for streaming responses and middlewares in Declarai chats, along with API improvements.</p> <p>Features:</p> <ul> <li>Streaming responses for Declarai tasks enabled.</li> <li>Middleware support for Declarai chats.</li> </ul> <p>Changes:</p> <ul> <li>Introduced middleware support to chat. Closes #105.</li> <li>Added streaming for OpenAI tasks. Closes #107.</li> <li>API flattened and refactored, thanks to @giladbarnea.</li> </ul> <p>New Contributors:</p> <ul> <li>@giladbarnea made their first contribution in #104</li> </ul> <p>Contributors:</p> <ul> <li>@giladbarnea</li> <li>@matankley</li> </ul>"},{"location":"changelog/#v018","title":"v0.1.8","text":"<p>View full release on GitHub and PyPi</p> <p>Changes:</p> <ul> <li>Dual support for DECLARAI_OPEN_AI_KEY and OPENAI_API_KEY.</li> <li>Introduced support for pydantic description field.</li> <li>Enabled Azure OpenAI LLM. Closes #67 and #72.</li> <li>Default Azure API version fixed.</li> </ul> <p>Contributors:</p> <ul> <li>@helmanofer</li> <li>@matankley</li> <li>@shobhit9957</li> </ul>"},{"location":"changelog/#v017","title":"v0.1.7","text":"<p>View full release on GitHub and PyPi</p> <p>Improvements and Refactoring</p> <p>A major focus on refining API Reference docs and refactoring the internal structure of the project.</p> <p>Changes:</p> <ul> <li>Introduction of gpt-3.5-turbo-16k support.</li> <li>Internal structure refactored and API reference generation in the docs.</li> <li>Fixes and updates in documentation.</li> </ul> <p>Contributors:</p> <ul> <li>@helmanofer</li> <li>@matankley</li> </ul>"},{"location":"changelog/#v016","title":"v0.1.6","text":"<p>View full release on GitHub and PyPi</p> <p>Bug Fix Release</p> <p>Changes:</p> <ul> <li>Fixed chat greeting duplication bug. Closes #84.</li> </ul> <p>Contributors:</p> <ul> <li>@matankley</li> </ul>"},{"location":"changelog/#v015","title":"v0.1.5","text":"<p>View full release on GitHub and PyPi</p> <p>Extended Database Support</p> <p>In this update, Declarai extends its support to PostgreSQL, Redis, and MongoDB databases for saving chat message history.</p> <p>Changes:</p> <ul> <li>Support for PostgreSQL, Redis, and MongoDB databases to save chat messages. Closes #77.</li> </ul> <p>Contributors:</p> <ul> <li>@matankley</li> </ul>"},{"location":"changelog/#v014","title":"v0.1.4","text":"<p>View full release on GitHub and PyPi</p> <p>Introduction of Chat Memory</p> <p>Declarai now supports retaining message history across chat sessions, ensuring continuity and context.</p> <p>Changes:</p> <ul> <li>Removal of operator inheritance from llm params type.</li> <li>Added chat memory to preserve message history between sessions. Closes #62.</li> </ul> <p>Contributors:</p> <ul> <li>@matankley</li> </ul>"},{"location":"changelog/#v013","title":"v0.1.3","text":"<p>View full release on GitHub and PyPi</p> <p>Enhanced Control Over LLM Params</p> <p>In this version, users gain the ability to finetune the parameters of the Language Model like temperature and max tokens. </p> <p>Changes:</p> <ul> <li>Control the llm params such as temperature, max tokens, and more. Closes #54.</li> <li>Deployment example with FastAPI for Declarai tasks added. Closes #63.</li> <li>Support for passing LLM parameters directly to tasks. Closes #70.</li> <li>Comprehensive documentation added for LLMParams control.</li> <li>Updated package version.</li> </ul> <p>Contributors:</p> <ul> <li>@matankley</li> </ul>"},{"location":"changelog/#v012","title":"v0.1.2","text":"<p>View full release on GitHub and PyPi</p> <p>Minor bug fixes</p> <p>Changes:</p> <ul> <li>Updates to documentation</li> <li>Updates to dependencies with reported vulnerabilities</li> <li>Fix typing and improve support for IDE autocompletion</li> <li>Fix issue with initialization failing when passed the <code>openai_token</code> at runtime.</li> </ul>"},{"location":"changelog/#v011","title":"v0.1.1","text":"<p>View full release on GitHub and PyPi</p> <p>Announcing the first release of Declarai! \ud83e\udd73 \ud83e\udd73</p> <p>Declarai was born out of the awe and excitement of LLMs, along with our passion for excellent engineering and real-world applications at scale.</p> <p>We hope this project will help introduce more developers into the world of LLMs and enable them to more easily and reliably integrate these amazing capabilities into their production systems.</p> <p>Main features:</p> <ul> <li>Task interface</li> <li>Chat interface</li> <li>Middlewares</li> <li>Exhaustive documentation</li> <li>OpenAI support</li> <li>LLM prompt best practices</li> </ul>"},{"location":"contribute/","title":"Contribute","text":"<p>Do you like Declarai?</p> <p>Spread the word!</p> <ul> <li>Star  the repository</li> <li>Share the link to the repository with your friends and colleagues</li> <li>Watch the github repository to get notified about new releases.</li> </ul>"},{"location":"contribute/#development","title":"Development","text":"<p>Once you have cloned the repository, install the requirements:</p> <p>Using <code>venv</code></p> PoetryVenv <pre><code>poetry install\n</code></pre> <pre><code>python -m venv env\nsource env/bin/activate\npython -m pip install --upgrade pip\npip install -r requirements.txt\n</code></pre>"},{"location":"contribute/#documentation","title":"Documentation","text":"<p>The documentation is built using MkDocs. To view the documentation locally, run the following command:</p> <pre><code>$ cd docs\n$ mkdocs serve\nINFO    -  [11:37:30] Serving on http://127.0.0.1:8000/\n</code></pre>"},{"location":"contribute/#testing","title":"Testing","text":"<p>The testing framework used is pytest. To run the tests, run the following command:</p> <pre><code>pytest --cov=src   </code></pre>"},{"location":"contribute/#pull-requests","title":"Pull Requests","text":"<p>It should be extermly easy to contribute to this project. If you have any ideas, just open an pull request and we will discuss it.</p> <pre><code>git checkout -b my-new-feature\ngit commit -am 'Add some feature'\ngit push origin my-new-feature\n</code></pre>"},{"location":"newsletter/","title":"Newsletter","text":"<p>Subscribe to our newsletter to stay up to date with the latest news about the declarai, and other cool stuff \ud83d\udcec</p> Subscribe * indicates required Email Address *Company / Organization Full name           /* real people should not fill this in and expect good things - do not remove this or risk form bot signups */          <p></p>"},{"location":"beginners-guide/","title":"Tutorial - Beginners guide","text":"<p>This tutorial is a step-by-step guide to using Declarai. It walks you through the most basic features of the library.</p> <p>Each section gradually builds on the previous one while sections are structured by topic,  so that you can skip to whichever part is relevant to you. </p>"},{"location":"beginners-guide/#before-we-start","title":"Before we start","text":"<p>If you haven't already, install the Declarai library as follows:</p> <pre><code>$ pip install declarai\n</code></pre> <p>Info</p> <p>For this tutorial you will need an openai token. This token is completely your's and is not shared, stored or managed anywhere but on your machine! you can see more information about obtaining a token here: openai</p> <p>After installation, open a python file and start with setting up your declarai app:</p> <p>Once completed, the rest of the examples in this module should be as simple as copy/paste.</p> declarai_tutorial.py<pre><code>import declarai\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\", openai_token=\"&lt;your-openai-token&gt;\")\n</code></pre> <p>Info</p> <p>Do your best to copy, run and edit the code in your editor to really understand how powerful Declarai is.</p>          Lets go!"},{"location":"beginners-guide/#advanced","title":"Advanced","text":"<p>If you feel this tutorial is too easy, feel free to jump to our Features section, which covers more complex  topics like middlewares, running evaluations and building multi provider flows.</p> <p>We recommend you read the tutorial first, and then the advanced guide if you want to learn more.</p>"},{"location":"beginners-guide/controlling-task-behavior/","title":"Controlling task behavior","text":"<p>Task behavior can be controlled by any of the available interfaces in Python. Controlling these parameters is key to achieving the desired results from the model.</p>"},{"location":"beginners-guide/controlling-task-behavior/#passing-parameters-to-the-task","title":"Passing parameters to the task","text":"<p>In the following example, we'll create a task that suggests movies to watch based on a given input.</p> <pre><code>import declarai\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n\n@gpt_35.task\ndef movie_recommender(user_input: str):  # (1)!\n\"\"\"\n    Recommend a movie to watch based on the user input\n    :param user_input: The user's input\n    \"\"\"  # (2)!\n</code></pre> <ol> <li>Notice how providing a type hint for the <code>user_input</code> parameter allows declarai to understand the expected input    type.</li> <li>Adding the param to the docstring allows declarai to communicate the meaning of this parameter to the model.</li> </ol> <pre><code>print(movie_recommender(user_input=\"I want to watch a movie about space\"))\n&gt; 'Interstellar'\n</code></pre>"},{"location":"beginners-guide/controlling-task-behavior/#using-return-types-to-control-the-output","title":"Using return types to control the output","text":"<p>This is a good start, but let's say we want to have a selection of movies instead of a single suggestion.</p> <pre><code>from typing import List\nimport declarai\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n\n@gpt_35.task\ndef movie_recommender(user_input: str) -&gt; List[str]:  # (1)!\n\"\"\"\n    Recommend a selection of movies to watch based on the user input\n    :param user_input: The user's input\n    :return: A list of movie recommendations\n    \"\"\"  # (2)!\n</code></pre> <ol> <li>Adding a return type hint allows declarai to parse the output of the llm into the provided type,    in our case a list of strings.</li> <li>Explaining the return value aids the model in returning the expected output and avoiding hallucinations.</li> </ol> <pre><code>print(movie_recommender(user_input=\"I want to watch a movie about space\"))\n&gt; ['Interstellar', 'Gravity', 'The Martian', 'Apollo 13', '2001: A Space Odyssey', 'Moon', 'Sunshine', 'Contact',\n   'The Right Stuff', 'Hidden Figures']\n</code></pre> <p>Info</p> <p>Notice How the text in our documentation has changed from singular to plural form.  Maintaining consistency between the task's description and the return type is important for the model to understand the expected output. For more best-practices, see here. </p> <p>Awesome!</p> <p>Now we have a list of movies to choose from!</p> <p>But what if we want to go even further ?  Let's say we want the model to also provide a short description of each movie.</p> <pre><code>from typing import Dict\nimport declarai\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n\n@gpt_35.task\ndef movie_recommender(user_input: str) -&gt; Dict[str, str]:  # (1)!\n\"\"\"\n    Recommend a selection of movies to watch based on the user input\n    For each movie provide a short description as well\n    :param user_input: The user's input\n    :return: A dictionary of movie names and descriptions\n    \"\"\"  # (2)!\n</code></pre> <ol> <li>We've updated the return value to allow for the creation of a dictionary of movie names and descriptions.</li> <li>We re-enforce the description of the return value to ensure the model understands the expected output.</li> </ol> <pre><code>print(movie_recommender(user_input=\"I want to watch a movie about space\"))\n&gt; {\n    'Interstellar': \"A team of explorers travel through a wormhole in space in an attempt to ensure humanity's survival.\",\n    'Gravity': 'Two astronauts work together to survive after an accident leaves them stranded in space.',\n    'The Martian': 'An astronaut is left behind on Mars after his team assumes he is dead and must find a way to survive and signal for rescue.',\n    'Apollo 13': 'The true story of the Apollo 13 mission, where an explosion in space jeopardizes the lives of the crew and their safe return to Earth.',\n    '2001: A Space Odyssey': \"A journey through human evolution and the discovery of a mysterious black monolith that may hold the key to humanity's future.\"\n}\n</code></pre> <p>Info</p> <p>A good practice for code readability as well as great performing models is to use type hints and context in the docstrings. The better you describe the task, <code>:params</code> and <code>:return</code> sections within the docstring, the better the results will be.</p> <p>Tip</p> <p>Try experimenting with various descriptions and see how far you can push the model's understanding! who knows what you'll find !</p>          Previous           Next"},{"location":"beginners-guide/debugging-tasks/","title":"Debugging tasks","text":"<p>So it all seems pretty magical up to this point, but what if you want to see what's going on behind the scenes? Being able to debug your tasks is a very important part of the development process, and Declarai makes it easy for you.</p>"},{"location":"beginners-guide/debugging-tasks/#compiling-tasks","title":"Compiling tasks","text":"<p>The first and simplest tool to better understand what's happening under the hood is the <code>compile</code> method. Declarai has an <code>evals</code> module as well for advanced debugging and benchmarking which you can review later here: evals</p> <p>Let's take the last task from the previous section and add a call to the <code>compile</code> method: <pre><code>from typing import Dict\nimport declarai\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n@gpt_35.task\ndef movie_recommender(user_input: str) -&gt; Dict[str, str]:\n\"\"\"\n    Recommend a selection of movies to watch based on the user input\n    For each movie provide a short description as well\n    :param user_input: The user's input\n    :return: A dictionary of movie names and descriptions\n    \"\"\"\n\nmovie_recommender.compile()\n\n&gt; {\n    'messages': [ # (1)!\n        # (2)!        \n        system: You are a REST api endpoint. \n                You only answer in JSON structures with a single key named 'declarai_result', nothing else. \n                The expected format is: \"declarai_result\": Dict[string, string]  # A dictionary of movie names and descriptions,\n        # (3)!\n        user: Recommend a selection of movies to watch based on the user input  \n              For each movie provide a short description as well.\n              Inputs: user_input: {user_input} # (4)!\n    ]\n}\n</code></pre></p> <ol> <li>As we are working with the openai llm provider, which exposes a chat interface, we translate the task into messages as defined by openai's API.</li> <li>In order to guide the task with the correct output format, we provide a system message that explains LLM's role and expected responses</li> <li>The user message is the actual translation of the task at hand, with the user's input as a placeholder for the actual value.</li> <li>{user_input} will be populated with the actual value when the task is being called at runtime.</li> </ol> <p>What we're seeing here is the template for this specific task. It is built so that when called at runtime,  it will be populated with the real values passed to our task.</p> <p>Warning</p> <p>As you can see, that the actual prompt being sent to the model is a bit different than the original docstring.  Even though Declarai incorporates best practices for prompt engineering while maintaining as little interference as possible with user prompts,   it is still possible that the model will not generate the desired output. For this reason it is important to be able to debug your tasks and understand what actually got sent to the model</p>"},{"location":"beginners-guide/debugging-tasks/#compiling-tasks-with-real-values","title":"Compiling tasks with real values","text":"<p>The <code>compile</code> method can also be used to view the prompt with the real values provided to the task. This is useful when prompts might behave differently for different inputs.</p> <pre><code>print(movie_recommender.compile(user_input=\"I want to watch a movie about space\"))\n\n&gt; {\n    'messages': [     \n        system: You are a REST api endpoint. \n                You only answer in JSON structures with a single key named 'declarai_result', nothing else. \n                The expected format is: \"declarai_result\": Dict[string, string]  # A dictionary of movie names and descriptions,\n        user: Recommend a selection of movies to watch based on the user input  \n              For each movie provide a short description as well.\nInputs: user_input: I want to watch a movie about space # (1)!\n]}\n</code></pre> <ol> <li>The actual value of the parameter is now populated in the placeholder and we have our final prompt!</li> </ol> <p>Tip</p> <p>With the <code>compile</code> method, you can always take your prompts anywhere you like,   if it's for monitoring, debugging or just for documentation, we've got you covered!</p>          Previous           Next"},{"location":"beginners-guide/recap/","title":"Recap","text":"<p>In this tutorial you've covered the basics of Declarai! You should now be able to easily:</p> <ul> <li>Create a declarai task.</li> <li>control your task's behavior with native python</li> <li>Use the <code>compile</code> method to view and debug your task template and final prompt!</li> </ul>          Previous"},{"location":"beginners-guide/recap/#next-steps","title":"Next steps","text":"<p>You are welcome to explore our Features section, where you can find the full list of supported features and how to use them.</p>"},{"location":"beginners-guide/simple-task/","title":"Simple task","text":"<p>The simplest Declarai usage is a function decorated with <code>@task</code>:</p> <p><pre><code>import declarai\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n@gpt_35.task\ndef say_something() -&gt; str:\n\"\"\"\n    Say something short to the world\n    \"\"\"\n\nprint(say_something())\n\n&gt; \"Spread love and kindness to make the world a better place.\"\n</code></pre> In Declarai, The docstring represents the task's description and is used to generate the prompt.</p> <p>By explaining what you want the task to do, the model will be able to understand it and reply with the proper result.</p>          Next"},{"location":"best-practices/","title":"Best practices","text":"<p>Prompt engineering is no simple task and there are various things to consider when creating a prompt. In this page we provide our view and understanding of the best practices for prompt engineering. These will help you create reliably performing tasks and chatbots that won't surprise you when deploying in production.</p> <p>Warning</p> <p>While this guide will should help in creating reliable prompts for most cases, it is still possible that the model will not generate the desired output.  For this reason we strongly recommend you test your tasks and bots on various inputs before deploying to production.  You can acheive this by writing integration tests or using our provided <code>evals</code> library to discover which models and wich  versions perform best for your specific use case.</p>"},{"location":"best-practices/#explicit-is-better-than-implicit","title":"Explicit is better than implicit","text":"<p>When creating a prompt, it is important to be as explicit as possible. Declarai provide various interfaces to provide context and guidance to the model.</p> <p>Reviewing the movie recommender example from the beginner's guide, we can see a collection of techniques to provide context to the model: <pre><code>from typing import Dict\nimport declarai\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n@gpt_35.task\ndef movie_recommender(user_input: str) -&gt; Dict[str, str]:\n\"\"\"\n    Recommend a selection of movies to watch based on the user input\n    For each movie provide a short description as well\n    :param user_input: The user's input\n    :return: A dictionary of movie names and descriptions\n    \"\"\"\n</code></pre></p> <p>Using type annotations in the input and output create predictability in software and enforce a strict interface with the model. The types are read and enforced by Declarai at runtime so that a produced result of the wrong type will raise an error instead of returned and causing unexpected behavior down the line.</p> <p>Docstrings are used to provide context to the model and to the user.</p> <ul> <li> <p>Task description - The first part of the docstring is the task itself, make sure to address the expected inputs and how to use them     You can implement various popular techniques into the prompt such as <code>few-shot</code>, which means providing example inputs and outputs for the model to learn from.</p> </li> <li> <p>Param descriptions - Explaining the meaning of the input parameters helps the model better perform with the provided inputs.     For example. when passing an argument called <code>input</code>, if you know that the expected input will be an email, or user message, it is best to explain this to the model.</p> </li> <li> <p>Return description - While typing are a great base layer for declaring the expected output,      explaining the exact structure and logic behind this structure will help the model better perform.     For example, given a return type of <code>Dict[str, str]</code>, explaining that this object will contain a mapping of movie names to their respective description      will help to model properly populate the resulting object.</p> </li> </ul>"},{"location":"best-practices/#language-consistency-and-ambiguity","title":"Language consistency and ambiguity","text":"<p>When providing prompts to the model, it is best practice to use language that correlates with the expected input and output. For example, in the following, the prompt is written in single form, while the resulting output is in plural form. (i.e. a list) <pre><code>from typing import List\nimport declarai\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n@gpt_35.task\ndef movie_recommender(user_input: str) -&gt; List[str]:\n\"\"\"\n    Recommend a movie to watch based on the user input\n    :param user_input: The user's input\n    :return: Recommended movie\n    \"\"\"\n</code></pre> This may easily confuse the model and cause it to produce unexpected results which will fail when parsing the results. Instead, we could write the prompt as follows: <pre><code>from typing import List\nimport declarai\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n@gpt_35.task\ndef movie_recommender(user_input: str) -&gt; List[str]:\n\"\"\"\n    Recommend a selection of movies to watch based on the user input\n    :param user_input: The user's input\n    :return: A list of recommended movies\n    \"\"\"\n</code></pre> This way it is clear to the model that we are expecting a list of movies and not a single movie.</p>"},{"location":"best-practices/#falling-back-to-string","title":"Falling back to string","text":"<p>In some cases, you might be working on a task or chat that has a mixture of behaviors that may not be consistent. For example in this implementation of a calculator bot, the bot usually returns numbers, but for the scenario that an error occurs, it returns a string. <pre><code>from typing import Union\nimport declarai\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n@gpt_35.experimental.chat\nclass CalculatorBot:\n\"\"\"\n    You a calculator bot,\n    given a request, you will return the result of the calculation\n    If you have a problem with the provided input, you should return an error explaining the problem.\n    For example, for the input: \"1 + a\" where 'a' is unknown to you, you should return: \"Unknown symbol 'a'\"\n    \"\"\"\n    def send(self, message: str) -&gt; Union[str, int]:\n        ...\n</code></pre> When using the created bot it should look like this: <pre><code>calc_bot = CalculatorBot()\nprint(calc_bot.send(message=\"1 + 3\"))\n#&gt; 4\nprint(calc_bot.send(message=\"34 * b\"))\n#&gt; Unknown symbol 'b'\n</code></pre> This way, instead of raising an error, the bot returns a string that explains the problem and allows the user to recover from the 'broken' state.</p>"},{"location":"examples/deployments/","title":"Deployments \u2692\ufe0f","text":"<p>Ready to deploy your code? Here are some resources to help you get started:</p>"},{"location":"examples/deployments/#fastapi","title":"FastAPI","text":"<p>Deploying business logic as a REST API is a common pattern. FastAPI is an ultimate solution. Here's how you can deploy your Declarai code behind a REST API using FastAPI:</p> <pre><code>from typing import Dict\nfrom pydantic import BaseModel\nfrom fastapi import FastAPI, APIRouter\nimport declarai\napp = FastAPI()\nrouter = APIRouter()\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n\n@gpt_35.task\ndef movie_recommender(user_input: str) -&gt; Dict[str, str]:\n\"\"\"\n    Recommend a selection of real movies to watch based on the user input\n    For each movie provide its name and a short description as well.\n    :param user_input: The user's input\n    :return: A mapping between movie names and descriptions\n    \"\"\"\n\n\nclass MovieRecommendationRequest(BaseModel):\n    user_input: str\n\n\n@router.post(\"/movie_recommender\", response_model=Dict[str, str])\ndef run_movie_recommender(request: MovieRecommendationRequest) -&gt; Dict[str, str]:\n\"\"\"\n    Run the movie recommender task behind a post request\n    \"\"\"\n    return movie_recommender(user_input=request.user_input)\n\n\napp.include_router(router)\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(app)\n</code></pre> <p>You can now run the server with <code>python app.py</code> and send a POST request to <code>http://localhost:8000/movie_recommender</code>:</p> <pre><code>import requests\n\nres = requests.post(\"http://localhost:8000/movie_recommender\",\n              json={\"user_input\": \"I want to watch a movie about space\"})\n&gt;&gt;&gt; res.json()\n\n{'Gravity': 'Two astronauts work together to survive after an accident leaves '\n            'them stranded in space.',\n 'Interstellar': 'A team of explorers travel through a wormhole in space in an '\n                 \"attempt to ensure humanity's survival.\",\n 'The Martian': 'An astronaut is left stranded on Mars and must find a way to '\n                'survive until rescue is possible.'}\n</code></pre>"},{"location":"examples/deployments/#streamlit-app","title":"Streamlit app","text":"<p>Streamlit is a great tool for quickly building interactive web apps. Assuming you have deployed your Declarai code as a REST API, you can use the following snippet to build a Streamlit app that interacts with it: <pre><code>import streamlit as st\nimport requests\n\nBACKEND_URL = \"http://localhost:8000\"\nst.title(\"Welcome to Movie Recommender System\")\nst.write(\"This is a demo of a movie recommender system built using Declarai\")\n\nuser_input = st.text_input(\"What kind of movies do you like?\")\nbutton = st.button(\"Submit\")\nif button:\n    print(user_input)\n    with st.spinner(\"Thinking..\"):\n        res = requests.post(f\"{BACKEND_URL}/movie_recommender\", json={\"user_input\": user_input})\n    st.write(res.json())\n</code></pre> </p>"},{"location":"features/","title":"Features","text":"<p>As Declarai is aimed at being completely extensible and configurable, we provide interfaces to override and interact with any of the default behaviours if you choose.</p> <p>We are still actively working on exposing all the necessary interfaces to make this possible, so if there are any interfaces you would like to see exposed, please vote or open an issue on our GitHub</p>"},{"location":"features/jinja_templating/","title":"Jinja templating","text":""},{"location":"features/jinja_templating/#jinja-templating","title":"Jinja Templating","text":"<p>Jinja is a templating language for Python.</p> <p>We can use Jinja to create templates for our tasks. This is useful when: - Task has a lot of boilerplate code - Task has a lot of parameters. - You want to control the task's prompt structure.</p> <p>For example, let's say we want to create a task that takes in a string and ranks its sentiment. We can use Jinja to create a template for this task:</p> <pre><code>import declarai\nfrom typing import List\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n\n@gpt_35.task\ndef sentiment_classification(string: str, examples: List[str, int]) -&gt; int:\n\"\"\"\n    Classify the sentiment of the provided string, based on the provided examples.\n    The sentiment is ranked on a scale of 1-5, with 5 being the most positive.\n    {% for example in examples %}\n    {{ example[0] }} // {{ example[1] }}\n    {% endfor %}\n    {{ string }} //\n    \"\"\"\n\n\nsentiment_classification.compile(string=\"I love this product but there are some annoying bugs\",\n                         examples=[[\"I love this product\", 5], [\"I hate this product\", 1]])\n\n&gt;&gt;&gt; {'messages': [\n    system: respond only with the value of type int:, # (1)!\n    user: Classify the sentiment of the provided string, based on the provided examples. The sentiment is ranked on a scale of 1-5, with 5 being the most positive. # (2)!\n          I love this product // 5\n          I hate this product // 1\n          I love this product //\n    ]\n}\n\nsentiment_classification(string=\"I love this product but there are some annoying bugs\",\n                         examples=[[\"I love this product\", 5], [\"I hate this product\", 1]])\n\n&gt;&gt;&gt; 4\n</code></pre> <ol> <li>The system message is generated based on the return type <code>int</code> of the function.</li> <li>The user message is generated based on the docstring of the function. The Jinja template is rendered with the provided parameters.</li> </ol> <p>Same thing can be done with the <code>chat</code> decorator:</p> <pre><code>import declarai\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n\n@gpt_35.experimental.chat\nclass TranslatorBot:\n\"\"\"\n    You are a translator bot,\n    You will translate the provided text from English to {{ language }}.\n    Do not translate the following categories of words: {{ exclude_words }}\n    \"\"\"\n\n\nbot = TranslatorBot(language=\"French\", exclude_words=[\"bad words\"])\n\nbot.compile()\n\n&gt;&gt;&gt; {'messages': [\n    system: You are a translator bot, You will translate the provided text from English to French.\n            Do not translate the following categories of words: ['bad words']\n    ]\n}\n</code></pre>"},{"location":"features/language-model-parameters/","title":"Control LLM params","text":"<p>Language models have various parameters that can be tuned to control the output of the model. To see the parameters for a specific LLM, see the corresponding provider.</p> <p>Here is an example of how to control these parameters in a declarai task/chat:</p>"},{"location":"features/language-model-parameters/#set-at-declaration","title":"Set at declaration","text":"<pre><code>import declarai\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\", openai_token=\"&lt;your API key&gt;\")\n\n\n@gpt_35.task(llm_params={\"temperature\": 0.5, \"max_tokens\": 1000})\ndef generate_song():\n\"\"\"\n    Generate a song about declarai\n    \"\"\"\n</code></pre>"},{"location":"features/language-model-parameters/#set-at-runtime","title":"Set at runtime","text":"<p>We can also pass parameters to the declarai task/chat interface at runtime:</p> <pre><code>import declarai\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\", openai_token=\"&lt;your API key&gt;\")\n\n@gpt_35.task\ndef generate_song():\n\"\"\"\n    Generate a song about declarai\n    \"\"\"\n\ngenerate_song(llm_params={\"temperature\": 0.5, \"max_tokens\": 1000}) # (1)!\n</code></pre> <ol> <li>The <code>llm_params</code> argument is passed at runtime instead of at declaration.</li> </ol>"},{"location":"features/language-model-parameters/#override-at-runtime","title":"Override at runtime","text":"<p>Furthermore, we can pass parameters to the declarai task/chat interface at runtime and override the parameters passed at declaration:</p> <pre><code>import declarai\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\", openai_token=\"&lt;your API key&gt;\")\n\n@gpt_35.task(llm_params={\"temperature\": 0.5, \"max_tokens\": 1000})\ndef generate_song():\n\"\"\"\n    Generate a song about declarai\n    \"\"\"\n\ngenerate_song(llm_params={\"temperature\": 0.3, \"max_tokens\": 500})\n</code></pre> <p>In this case, the <code>llm_params</code> argument passed at runtime will override the <code>llm_params</code> argument passed at declaration.</p>"},{"location":"features/language-model-parameters/#set-for-chat-interface","title":"Set for Chat interface","text":"<p>Same as with tasks, we can pass parameters to the declarai chat interface at declaration, at runtime, or override the parameters passed at declaration at runtime.</p> <p><pre><code>import declarai\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\", openai_token=\"&lt;your API key&gt;\")\n\n@gpt_35.experimental.chat(llm_params={\"temperature\": 0.5, \"max_tokens\": 1000})\nclass SQLAdvisor:\n\"\"\"\n    You are a proficient sql adivsor.\n    Your goal is to help user's with sql related questions.\n    \"\"\"\n\nsql_advisor = SQLAdvisor()\n</code></pre> In the case above, all messages sent to the chat interface will use the parameters passed at declaration.</p>"},{"location":"features/magic/","title":"Magic","text":"<p>The Magic callable is an \"empty\" function that can be used for two main scenarios:</p> <ul> <li>A placeholder for typing, so to simplify interaction with static typing without hacing to mark all Declarai functions with <code># type: ignore</code>:</li> <li>A replacement for the docstring content, if for some reason you don't want to use the docstring for the task description.</li> </ul>"},{"location":"features/magic/#magic-as-a-placeholder-for-typing","title":"Magic as a placeholder for typing","text":"<p>Without magic: <pre><code>@openai.task\ndef suggest_nickname(real_name: str) -&gt; str: # (1)!\n\"\"\"\n    Suggest a nickname for a person\n    :param real_name: The person's real name\n    :return: A nickname for the person\n    \"\"\"\n</code></pre></p> <ol> <li>type hinter warning on unused argument <code>real_name</code> in function.</li> </ol> <p>with magic: <pre><code>@openai.task\ndef suggest_nickname(real_name: str) -&gt; str:\n\"\"\"\n    Suggest a nickname for a person\n    :param real_name: The person's real name\n    :return: A nickname for the person\n    \"\"\"\n    return declarai.magic(real_name=real_name) # (1)!\n</code></pre></p> <ol> <li>type hint warning is resolved.</li> </ol>"},{"location":"features/magic/#replacement-for-docstring","title":"Replacement for docstring","text":"<p>In the scenario that you do not wan't to rely on the docstring for prompt generation, you can use the magic function to provide the description and parameters.</p> <pre><code>import declarai\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n@gpt_35.task\ndef suggest_nickname(real_name: str) -&gt; str:\n    return declarai.magic(\n        real_name=real_name,\n        description=\"Suggest a nickname for a person\",\n        params={\"real_name\": \"The person's real name\"},\n        returns=\"A nickname for the person\",\n    )\n</code></pre> <p>This does take some of Declarai's magic out of the equation, but the result should be all the same.</p>"},{"location":"features/multi-model-multi-provider/","title":"Multiple models / Multiple providers","text":"<p>Declarai allows you to use multiple models from different providers in the same project. All you need to do is configure seperate Declarai instances for each model and provider.</p> <pre><code>import declarai\n\n# Configure the first Declarai instance\ndeclarai_gpt35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n# Configure the second Declarai instance\ndeclarai_gpt4 = declarai.openai(model=\"gpt-4\")\n\n# Now use the instances to create tasks\n@declarai_gpt35.task\ndef say_something() -&gt; str:\n\"\"\"\n    Say something short to the world\n    \"\"\"\n\n@declarai_gpt4.task\ndef say_something() -&gt; str:\n\"\"\"\n    Say something short to the world\n    \"\"\"\n</code></pre>"},{"location":"features/planning-future-tasks/","title":"Planning future tasks","text":""},{"location":"features/planning-future-tasks/#plan-task","title":"Plan task","text":"<p>Once you have defined your task, you can create a plan for it that is already populated with the real values of the parameters.</p> <p>The plan is an object you call and get the results. This is very helpful when you want to populate the task with the real values of the parameters but delay the execution of it. </p> <pre><code>import declarai\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\") \n\n@gpt_35.task\ndef say_something_about_movie(movie: str) -&gt; str:  \n\"\"\"\n    Say something short about the following movie\n    :param movie: The movie name\n    \"\"\"\n\n    return declarai.magic(movie)\n\nplan = say_something_about_movie.plan(movie=\"Avengers\")\n\nprint(plan)\n&gt; #&lt;declarai.tasks.base_llm_task.LLMTaskFuture object at 0x106795790&gt;\n\n\n# Execute the task by calling the plan\nplan()\n&gt; ['I liked the action-packed storyline and the epic battle scenes.',\n   \"I didn't like the lack of character development for some of the Avengers.\"]\n</code></pre> <p>Important</p> <p>The plan is an object you call and get the results. This is very helpful when you want to populate the task with the real values of the parameters but delay the execution of it. If you just want to execute the task, you can call the task directly.</p> <pre><code>res = say_something_about_movie(movie=\"Avengers\")\n\n&gt; ['I liked the action-packed storyline and the epic battle scenes.',\n\"I didn't like the lack of character development for some of the Avengers.\"]\n</code></pre>"},{"location":"features/streaming/","title":"Streaming","text":"<p>Some LLM providers support streaming of the LLM responses. This is very useful when you want to get the results as soon they are generated, and not wait for the entire response to be generated.</p>"},{"location":"features/streaming/#streaming-example","title":"Streaming example","text":"<pre><code>import declarai\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n\n@gpt_35.task(streaming=True)  # (1)!\ndef say_something_about_movie(movie: str) -&gt; str:\n\"\"\"\n    Say something short about the following movie\n    :param movie: The movie name\n    \"\"\"\n\n    return declarai.magic(movie)\n\n\nres = say_something_about_movie(movie=\"Avengers\") # (2)!_\n\nfor chunk in res:\n    print(chunk.response)\n\n\"Av\"\n\"Avengers\"\n\"Avengers is\"\n\"Avengers is an\"\n\"Avengers is an action\"\n\"Avengers is an action-packed\"\n\"Avengers is an action-packed superhero\"\n\"Avengers is an action-packed superhero extrav\"\n\"Avengers is an action-packed superhero extravagan\"\n\"Avengers is an action-packed superhero extravaganza\"\n\"Avengers is an action-packed superhero extravaganza that\"\n\"Avengers is an action-packed superhero extravaganza that brings\"\n\"Avengers is an action-packed superhero extravaganza that brings together\"\n\"Avengers is an action-packed superhero extravaganza that brings together Earth\"\n\"Avengers is an action-packed superhero extravaganza that brings together Earth's\"\n\"Avengers is an action-packed superhero extravaganza that brings together Earth's might\"\n\"Avengers is an action-packed superhero extravaganza that brings together Earth's mightiest\"\n\"Avengers is an action-packed superhero extravaganza that brings together Earth's mightiest heroes\"\n\"Avengers is an action-packed superhero extravaganza that brings together Earth's mightiest heroes to\"\n\"Avengers is an action-packed superhero extravaganza that brings together Earth's mightiest heroes to save\"\n\"Avengers is an action-packed superhero extravaganza that brings together Earth's mightiest heroes to save the\"\n\"Avengers is an action-packed superhero extravaganza that brings together Earth's mightiest heroes to save the world\"\n\"Avengers is an action-packed superhero extravaganza that brings together Earth's mightiest heroes to save the world.\"\n\"Avengers is an action-packed superhero extravaganza that brings together Earth's mightiest heroes to save the world.\"\n</code></pre> <ol> <li>Set the <code>streaming</code> flag to <code>True</code> when defining the task</li> <li><code>res</code> is a generator. You can iterate over the generator to get the results.</li> </ol> <p>Currently only OpenAI &amp; Azure OpenAI support streaming.</p>"},{"location":"features/streaming/#turn-on-streaming","title":"Turn on streaming","text":"<p>In order to enable streaming, all you have to do is set the <code>streaming</code> flag to <code>True</code> when defining the task.</p> <pre><code>import declarai\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n@gpt_35.task(streaming=True)  # (1)!\ndef my_task()\n    ...\n\n\n@gpt_35.experimental.chat(streaming=True) # (2)!\nclass MyChat\n    ...\n</code></pre> <ol> <li>Set the <code>streaming</code> flag to <code>True</code> when defining the task</li> <li>Set the <code>streaming</code> flag to <code>True</code> when defining the chat class</li> </ol> <p>You can also enable streaming globally by settings <code>stream=True</code> when initializing the <code>declarai</code> object.</p> <pre><code>import declarai\n\ngpt_35_with_streaming = declarai.openai(\n    model=\"gpt-3.5-turbo\",\n    stream=True\n)\n\nazure_gpt_35_with_streaming = declarai.azure_openai(\n    model=\"gpt-3.5-turbo\",\n    stream=True\n)\n</code></pre>"},{"location":"features/streaming/#accessing-the-results","title":"Accessing the results","text":"<p>The results are returned as a generator. You can iterate over the generator to get the results.</p> <pre><code>import declarai\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n\n@gpt_35.task(streaming=True)\ndef say_something_about_movie(movie: str) -&gt; str:\n\"\"\"\n    Say something short about the following movie\n    :param movie: The movie name\n    \"\"\"\n\n    return declarai.magic(movie)\n\n\nres_stream = say_something_about_movie(movie=\"Avengers\")\n\ntype(res_stream)  # &lt;class 'generator'&gt;\n\nfor chunk in res_stream:\n    type(chunk)  # &lt;class 'declarai.operators.llm.LLMResponse'&gt;\n</code></pre> <p>The responses are also saved on the task object. </p> <pre><code>import declarai\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n\n@gpt_35.task(streaming=True)\ndef say_something_about_movie(movie: str) -&gt; str:\n\"\"\"\n    Say something short about the following movie\n    :param movie: The movie name\n    \"\"\"\n\n    return declarai.magic(movie)\n\n\nres_stream = say_something_about_movie(movie=\"Avengers\")\n\nsay_something_about_movie.llm_response  # Empty unless you call next on the generator\n\nsay_something_about_movie.llm_stream_response  # &lt;generator object BaseTask.stream_handler at ...&gt; \n</code></pre>"},{"location":"features/streaming/#access-the-delta-of-the-response","title":"Access the delta of the response","text":"<p>You can access the delta of the response by accessing the <code>raw_response</code> attribute of the <code>LLMResponse</code> object.</p> <p>The delta is the difference between the current response and the previous response.</p> <p>This is particularly useful when you want to stream the response to a chatbot, and there is no need to send the entire response all over again and again.</p> <pre><code>```py\nimport declarai\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\", stream=True)\n\n@gpt_35.task\ndef say_something_about_movie(movie: str) -&gt; str:\n\"\"\"\n    Say something short about the following movie\n    :param movie: The movie name\n    \"\"\"\n\n    return declarai.magic(movie)\n\n\nstream_res = say_something_about_movie(\"Avengers\")\n\nfor chunk in stream_res:\n    print(chunk.raw_response[\"choices\"][0][\"delta\"])\n\n# Output\n{'role': 'assistant', 'content': ''}\n{'content': '\"'}\n{'content': 'Av'}\n{'content': 'engers'}\n{'content': ' is'}\n{'content': ' an'}\n{'content': ' action'}\n{'content': '-packed'}\n{'content': ' superhero'}\n{'content': ' extrav'}\n{'content': 'agan'}\n{'content': 'za'}\n{'content': ' that'}\n{'content': ' brings'}\n{'content': ' together'}\n{'content': ' Earth'}\n{'content': \"'s\"}\n{'content': ' might'}\n{'content': 'iest'}\n{'content': ' heroes'}\n{'content': ' to'}\n{'content': ' save'}\n{'content': ' the'}\n{'content': ' world'}\n{'content': '.\"'}\n{}\n</code></pre>"},{"location":"features/chat/","title":"Chatbots","text":"<p>Unlike tasks, chatbots are meant to keep the conversation going.  Instead of executing a single operation, they are built to manage conversation context over time.</p> <p>Declarai can be used to create chatbots. The simplest way to do this is to use the <code>@declarai.experimental.chat</code> decorator.</p> <p>We declare a \"system prompt\" in the docstring of the class definition. The system prompt is the initial command that instructs the bot on who they are and what's expected in the conversation. </p> <pre><code>import declarai\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n@gpt_35.experimental.chat\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL related questions \n    \"\"\" # (1)!\n</code></pre> <ol> <li>The docstring represents the chatbot's description and is used to generate the prompt.</li> </ol> <pre><code>sql_bot = SQLBot()\nsql_bot.send(\"When should I use a LEFT JOIN?\") # (1)!\n\n&gt; \"You should use a LEFT JOIN when you want to return all rows from the left table, and the matched rows from the right table.\"\n</code></pre> <ol> <li>The created bot exposes a <code>send</code> method, by which you can interact and send messages.     Every call to send results with a response from the bot.</li> </ol> <p>Tip</p> <p>You can also declare the chatbot system prompt by doing the following <pre><code>@declarai.experimental.chat\nclass SQLBot:\n    pass\nsql_bot = SQLBot(system=\"You are a sql assistant. You help with SQL related questions with one-line answers.\")\n</code></pre></p>"},{"location":"features/chat/advanced-initialization/","title":"Initialization","text":"<p>Although using the docstring and class properties is the recommended way to initialize a chatbot, it is not the only way. In cases were relying on the class docstring and properties is problematic, we allow manually passing the chat arguments to the class constructor. This takes away from the magic that Declarai provides, but we are aware not everyone may be comfortable with it.</p>"},{"location":"features/chat/advanced-initialization/#initialization-by-passing-parameters","title":"Initialization by passing parameters","text":"<p>Let's see how we can initialize a chatbot by passing the <code>system</code> and <code>greeting</code> parameters as arguments.</p> <pre><code>import declarai\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n@gpt_35.experimental.chat\nclass SQLBot:\n    ...\n\n\nsql_bot = SQLBot(\n    system=\"You are a sql assistant. You help with SQL queries with one-line answers.\",\n    greeting=\"Hello, I am a SQL assistant. How can I assist you today?\",\n)\n\nprint(sql_bot.send(\"Tell me your preferred SQL operation\"))\n</code></pre> <pre><code>&gt; \"As an SQL assistant, I don't have a preferred SQL operation. I am here to assist with any SQL operation you need help with.\"\n</code></pre>"},{"location":"features/chat/advanced-initialization/#next-steps","title":"Next steps","text":"<p>You are welcome to explore our Features section, where you can find the full list of supported features and how to use them.</p>"},{"location":"features/chat/controlling-chat-behavior/","title":"Controlling chat behavior","text":""},{"location":"features/chat/controlling-chat-behavior/#greetings","title":"Greetings","text":"<p>Greetings are used to start the conversation with a bot message instead of a user message. The <code>greeting</code> attribute defines this first message and is added to the conversation on initialization.</p> <pre><code>import declarai\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n@gpt_35.experimental.chat\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL queries with one-line answers.\n    \"\"\"\n    greeting = \"Hello, I am a SQL assistant. How can I assist you today?\"\n</code></pre> <p>The greeting attribute is later available as a property of the chatbot instance to use when implementing your interface. <pre><code>sql_bot = SQLBot()\nsql_bot.greeting\n\n&gt; \"Hello, I am a SQL assistant. How can I assist you today?\"\n</code></pre></p> <pre><code>sql_bot.send(\"When should I use a LEFT JOIN?\")\n\n&gt; 'You should use a LEFT JOIN when you want to retrieve all records from the left table and matching records from the right table.'\n\nsql_bot.conversation\n\n&gt; [ # (1)!\n    assistant: Hello, I am a SQL assistant. How can I assist you today?,\n    user: When should I use a LEFT JOIN?,\n    assistant: You should use a LEFT JOIN when you want to retrieve all records from the left table and matching records from the right table.\n] \n</code></pre> <ol> <li>We can see here that the greeting, initiated by the assistant, is the first message in the conversation.</li> </ol>"},{"location":"features/chat/controlling-chat-behavior/#inject-a-message-to-the-memory","title":"Inject a message to the memory","text":"<p>Declarai enables injecting custom messages into the conversation history by using the <code>add_message</code> method.</p> <p>This is super useful when you want to intervene with the conversation flow without necessarily triggering another response from the model.</p> <p>Consider using it for:  </p> <ul> <li>Creating a prefilled conversation even before the user's interaction.  </li> <li>Modifying the chatbot memory after the chatbot has generated a response.  </li> <li>Modifying the chatbot system prompt.</li> <li>Guiding the conversation flow given certain criteria met in the user-bot interaction.</li> </ul> <pre><code>sql_bot = SQLBot()\nsql_bot.add_message(\"From now on, answer I DONT KNOW on any question asked by the user\", role=\"system\") \n# (1)!\nsql_bot.send(\"What is your favorite SQL operation?\")\n\n&gt; \"I don't know.\"\n</code></pre> <ol> <li>The chatbot's conversation history now contains the injected message and reacts accordingly.</li> </ol>"},{"location":"features/chat/controlling-chat-behavior/#dynamic-system-prompting","title":"Dynamic system prompting","text":"<p>In the following example, we will pass a parameter to the chatbot system prompt. This value will be populated at runtime and will allow us to easily create base chatbots with varying behaviors.</p> <pre><code>import declarai\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n@gpt_35.experimental.chat\nclass JokeGenerator:\n\"\"\"\n    You are a joke generator. You generate jokes that a {character} would tell.\n    \"\"\" # (1)!\n\n\ngenerator = JokeGenerator()\nfavorite_joke = generator.send(character=\"Spongebob\", message=\"What is your favorite joke?\")\nsquidward_joke = generator.send(message=\"What jokes can you tell about squidward?\")\n\nprint(favorite_joke)\nprint(squidward_joke)\n</code></pre> <ol> <li>The system prompt now contains the parameter <code>{character}</code>. This parameter will be replaced by the value passed to the <code>send</code> method.</li> </ol> <pre><code>&gt; \"Why did the jellyfish go to school? Because it wanted to improve its \"sting-uage\" skills!\"\n&gt; \"Why did Squidward bring a ladder to work? Because he wanted to climb up the corporate \"sour-cules\"!\"\n</code></pre>"},{"location":"features/chat/customizing-chat-response/","title":"Customizing the Chat Response","text":"<p>The default response type of the language model messages is <code>str</code>. However, you can overwrite the <code>send</code> method to return a different type. Just like tasks, you can control the type hints by declaring the return type of the <code>send</code> method.</p> <pre><code>from typing import List\nimport declarai\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n@gpt_35.experimental.chat\nclass SQLBot:\n\"\"\"\n    You are a sql assistant.\"\"\"\n    ...\n\n    def send(self, operation: str) -&gt; List[str]:\n        ...\n\nsql_bot = SQLBot()\nprint(sql_bot.send(message=\"Offer two sql queries that use the 'SELECT' operation\"))\n&gt; [\n    \"SELECT * FROM table_name;\",\n    \"SELECT column_name FROM table_name;\"\n]\n</code></pre> <p>Warning</p> <p>As with tasks, the message is sent along with the expected return types.  This means that if not careful, a message conflicting with the expected results could cause weird behavior in the llm responses.  For more best-practices, see here.</p>"},{"location":"features/chat/debugging-chat/","title":"Debugging Chat","text":"<p>Similarly to debugging tasks, understanding the prompts being sent to the llm is crucial to debugging chatbots. Declarai exposes the <code>compile</code> method for chat instances as well!</p>"},{"location":"features/chat/debugging-chat/#compiling-chat","title":"Compiling chat","text":"<p><pre><code>import declarai\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n@gpt_35.experimental.chat\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL queries with one-line answers.\n    \"\"\"\n    greeting = \"Hello, I am a SQL assistant. How can I assist you today?\"\n\nsql_bot = SQLBot()\nprint(sql_bot.compile())\n</code></pre> <pre><code>&gt; {\n    'messages': \n        [\n            \"system: You are a sql assistant. You help with SQL queries with one-line answers.\", \n            \"assistant: Hello, I am a SQL assistant. How can I assist you today?\"\n        ]\n}\n</code></pre> Wonderful right? We can view the chatbot's messages in the format they will be sent to the language model.</p>"},{"location":"features/chat/chat-memory/","title":"Chat memory","text":"<p>A chat instance saves the message history and uses it to future responses. Here is an example of a chatbot that retains conversation history across multiple <code>send</code> requests. <pre><code>@declarai.experimental.chat\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL related questions with one-line answers.\n    \"\"\"\n\nsql_bot = SQLBot()\n\nsql_bot.send(\"When should I use a LEFT JOIN?\") # (1)!\n&gt; \"You should use a LEFT JOIN when you want to retrieve all records from the left table and matching records from the right table.\"\n\nsql_bot.send(\"But how is it different from a RIGHT JOIN?\") # (2)!\n&gt; \"A LEFT JOIN retrieves all records from the left table and matching records from the right table, while a RIGHT JOIN retrieves all records from the right table and matching records from the left table.\"\n</code></pre></p> <ol> <li>The first message is sent with the system prompt.</li> <li>The second message is sent with the previous conversation and therefore the model is aware of the first question.</li> </ol>"},{"location":"features/chat/chat-memory/#conversation-history","title":"Conversation History","text":"<p>You can view the conversation history by accessing the <code>conversation</code> attribute.</p> <pre><code>sql_bot.conversation\n\n&gt; [\n    user: When should I use a LEFT JOIN?, \n    assistant: You should use a LEFT JOIN when you want to retrieve all records from the left table and matching records from the right table.,\n    user: But how is it different from a RIGHT JOIN?,\n    assistant: A LEFT JOIN retrieves all records from the left table and matching records from the right table, while a RIGHT JOIN retrieves all records from the right table and matching records from the left table.\n]\n</code></pre> <p>Warning</p> <p>Keep in mind that the conversation history does not contain the system prompt. It only contains the user messages and the chatbot responses.</p> <p>If you want to access the system message, you can use the <code>system</code> attribute.</p> <pre><code>sql_bot.system\n\n&gt; \"system: You are a sql assistant. You help with SQL related questions with one-line answers.\\n\"\n</code></pre>"},{"location":"features/chat/chat-memory/#default-memory","title":"Default Memory","text":"<p>The default message history of a chat is a simple in-memory list. This means that history exists only for the duration of the chatbot session.</p> <p>If you prefer to have a persistent history, you can use the <code>FileMessageHistory</code> class from the <code>declarai.memory</code> module.</p>"},{"location":"features/chat/chat-memory/#setting-up-a-memory","title":"Setting up a memory","text":"<p>Setting up a memory is done by passing <code>chat_history</code> as a keyword argument to the <code>declarai.experimental.chat</code> decorator.</p> <pre><code>import declarai\nfrom declarai.memory import FileMessageHistory\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n@gpt_35.experimental.chat(chat_history=FileMessageHistory(\"sql_bot_history.txt\")) # (1)!\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL related questions with one-line answers.\n    \"\"\"\n</code></pre> <ol> <li>file path is not mandatory. If you do not provide a file path, the default file path is stored in a tmp directory.</li> </ol> <p>We can also initialize the chat_history at runtime</p> <pre><code>import declarai\nfrom declarai.memory import FileMessageHistory\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n@gpt_35.experimental.chat\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL related questions with one-line answers.\n    \"\"\"\nsql_bot = SQLBot(chat_history=FileMessageHistory(\"sql_bot_history.txt\"))\n</code></pre>"},{"location":"features/chat/chat-memory/file-memory/","title":"File Memory","text":"<p>For chat that requires a persistent message history, you can use a file to store the conversation history.</p>"},{"location":"features/chat/chat-memory/file-memory/#set-file-memory","title":"Set file memory","text":"<pre><code>import declarai\nfrom declarai.memory import FileMessageHistory\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n@gpt_35.experimental.chat(chat_history=FileMessageHistory(\"sql_bot_history.txt\")) # (1)!\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL related questions with one-line answers.\n    \"\"\"\n\nsql_bot = SQLBot()\n</code></pre> <ol> <li>file path is not mandatory. If you do not provide a file path, the default file path is stored in a tmp directory.</li> </ol> <p>We can also initialize the <code>FileMessageHistory</code> class with a custom file path.</p>"},{"location":"features/chat/chat-memory/file-memory/#set-file-memory-at-runtime","title":"Set file memory at runtime","text":"<p>In case you want to set the file memory at runtime, you can use the <code>set_memory</code> method.</p> <pre><code>import declarai\nfrom declarai.memory import FileMessageHistory\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n@gpt_35.experimental.chat\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL related questions with one-line answers.\n    \"\"\"\n\nsql_bot = SQLBot(chat_history=FileMessageHistory(\"sql_bot_history.txt\"))\n</code></pre>"},{"location":"features/chat/chat-memory/mongodb-memory/","title":"MongoDB Memory","text":"<p>For chat that requires a persistent and scalable message history, you can use a MongoDB database to store the conversation history.</p>"},{"location":"features/chat/chat-memory/mongodb-memory/#set-mongodb-memory","title":"Set MongoDB memory","text":"<pre><code>import declarai\nfrom declarai.memory import MongoDBMessageHistory\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n\n@gpt_35.experimental.chat(\n    chat_history=MongoDBMessageHistory(\n        connection_string=\"mongodb://localhost:27017/mydatabase\",\n        session_id=\"unique_chat_id\")\n)  # (1)!\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL related questions with one-line answers.\n    \"\"\"\n\n\nsql_bot = SQLBot()\n</code></pre> <ol> <li>The <code>connection_string</code> parameter specifies the connection details for the MongoDB database.    Replace <code>localhost</code>, <code>27017</code>, and <code>mydatabase</code> with your specific MongoDB connection details. The <code>session_id</code>    parameter uniquely identifies the chat session for which the history is being stored.</li> </ol>"},{"location":"features/chat/chat-memory/mongodb-memory/#set-mongodb-memory-at-runtime","title":"Set MongoDB memory at runtime","text":"<p>In case you want to set the MongoDB memory at runtime, you can use the <code>set_memory</code> method.</p> <pre><code>import declarai\nfrom declarai.memory import MongoDBMessageHistory\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n\n@gpt_35.experimental.chat\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL related questions with one-line answers.\n    \"\"\"\n\n\nsql_bot = SQLBot(chat_history=MongoDBMessageHistory(connection_string=\"mongodb://localhost:27017/mydatabase\",\n                                                    session_id=\"unique_chat_id\"))\n</code></pre>"},{"location":"features/chat/chat-memory/mongodb-memory/#dependencies","title":"Dependencies","text":"<p>Make sure to install the following dependencies before using MongoDB memory.</p> <pre><code>pip install declarai[mongodb]\n</code></pre>"},{"location":"features/chat/chat-memory/postgresql-memory/","title":"PostgreSQL Memory","text":"<p>For chat that requires a persistent message history with the advantages of scalability and robustness, you can use a PostgreSQL database to store the conversation history.</p>"},{"location":"features/chat/chat-memory/postgresql-memory/#set-postgresql-memory","title":"Set PostgreSQL memory","text":"<pre><code>import declarai\nfrom declarai.memory import PostgresMessageHistory\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n\n@gpt_35.experimental.chat(\n    chat_history=PostgresMessageHistory(\n        connection_string=\"postgresql://username:password@localhost:5432/mydatabase\",\n        session_id=\"unique_chat_id\")\n) # (1)!\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL related questions with one-line answers.\n    \"\"\"\n\nsql_bot = SQLBot()\n</code></pre> <ol> <li>The <code>connection_string</code> parameter specifies the connection details for the PostgreSQL database. Replace <code>username</code>, <code>password</code>, <code>localhost</code>, <code>5432</code>, and <code>mydatabase</code> with your specific PostgreSQL connection details. The <code>session_id</code> parameter uniquely identifies the chat session for which the history is being stored.</li> </ol>"},{"location":"features/chat/chat-memory/postgresql-memory/#set-postgresql-memory-at-runtime","title":"Set PostgreSQL memory at runtime","text":"<p>In case you want to set the PostgreSQL memory at runtime, you can use the <code>set_memory</code> method.</p> <pre><code>import declarai\nfrom declarai.memory import PostgresMessageHistory\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n@gpt_35.experimental.chat\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL related questions with one-line answers.\n    \"\"\"\n\nsql_bot = SQLBot(chat_history=PostgresMessageHistory(connection_string=\"postgresql://username:password@localhost:5432/mydatabase\", session_id=\"unique_chat_id\"))\n</code></pre>"},{"location":"features/chat/chat-memory/postgresql-memory/#dependencies","title":"Dependencies","text":"<p>Make sure to install the following dependencies before using PostgreSQL memory.</p> <pre><code>pip install declarai[postgresql]\n</code></pre>"},{"location":"features/chat/chat-memory/redis-memory/","title":"Redis Memory","text":"<p>For chat that requires a fast and scalable message history, you can use a Redis database to store the conversation history.</p>"},{"location":"features/chat/chat-memory/redis-memory/#set-redis-memory","title":"Set Redis memory","text":"<pre><code>import declarai\nfrom declarai.memory import RedisMessageHistory\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n@gpt_35.experimental.chat(\n    chat_history=RedisMessageHistory(\n        session_id=\"unique_chat_id\",\n        url=\"redis://localhost:6379/0\"\n    )\n) # (1)!\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL related questions with one-line answers.\n    \"\"\"\n\nsql_bot = SQLBot()\n</code></pre> <ol> <li>The <code>url</code> parameter specifies the connection details for the Redis server. Replace <code>localhost</code> and <code>6379</code> with your specific Redis connection details. The <code>session_id</code> parameter uniquely identifies the chat session for which the history is being stored.</li> </ol> <p>We can also initialize the <code>RedisMessageHistory</code> class with custom connection details.</p>"},{"location":"features/chat/chat-memory/redis-memory/#set-redis-memory-at-runtime","title":"Set Redis memory at runtime","text":"<p>In case you want to set the Redis memory at runtime, you can use the <code>set_memory</code> method.</p> <pre><code>import declarai\nfrom declarai.memory import RedisMessageHistory\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\n@gpt_35.experimental.chat\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL related questions with one-line answers.\n    \"\"\"\n\nsql_bot = SQLBot(chat_history=RedisMessageHistory(session_id=\"unique_chat_id\", url=\"redis://localhost:6379/0\"))\n</code></pre>"},{"location":"features/chat/chat-memory/redis-memory/#dependencies","title":"Dependencies","text":"<p>Make sure to install the following dependencies before using Redis memory.</p> <pre><code>pip install declarai[redis]\n</code></pre>"},{"location":"features/evals/","title":"Evaluations","text":"<p>The <code>evals</code> library is an addition over the base <code>declarai</code> library that provides tools to track and benchmark the performance of prompt strategies across models and providers.</p> <p>We understand that a major challenge in the field of prompt engineering is the lack of a standardised way to evaluate along with the continuously evolving nature of the field. As such, we have designed the <code>evals</code> library to be a lean wrapper over the <code>declarai</code> library that allows users to easily track and benchmark changes in prompts and models.</p>"},{"location":"features/evals/#usage","title":"Usage","text":"<pre><code>$ python -m declarai.evals.evaluator\nRunning Extraction scenarios...\nsingle_value_extraction... \n---&gt; 100%\nmulti_value_extraction...\n---&gt; 100%\nmulti_value_multi_type_extraction...\n---&gt; 100%\n...\nDone!\n</code></pre>"},{"location":"features/evals/#evaluations_1","title":"Evaluations","text":"<p>The output table will allow you to review the performance of your task across models and provides and make an informed decision on which model and provider to use for your task.</p> Provider Model version Scenario runtime output openai gpt-3.5-turbo latest generate_a_poem_no_metadata 1.235s Using LLMs is fun! openai gpt-3.5-turbo 0301 generate_a_poem_no_metadata 0.891s Using LLMs is fun! It's like playing with words Creating models that learn And watching them fly like birds openai gpt-3.5-turbo 0613 generate_a_poem_no_metadata 1.071s Using LLMs is fun! openai gpt-4 latest generate_a_poem_no_metadata 3.494s {'poem': 'Using LLMs, a joyous run,\\nIn the world of AI, under the sun.\\nWith every task, they stun,\\nIndeed, using LLMs is fun!'} openai gpt-4 0613 generate_a_poem_no_metadata 4.992s {'title': 'Using LLMs is fun!', 'poem': \"With LLMs, the fun's just begun, \\nCoding and learning, second to none. \\nComplex tasks become a simple run, \\nOh, the joy when the work is done!\"} openai gpt-3.5-turbo latest generate_a_poem_only_return_type 2.1s Learning with LLMs, a delightful run, Exploring new knowledge, it's never done. With every challenge, we rise and we stun, Using LLMs, the learning is always fun!"},{"location":"integrations/","title":"Integrations","text":"<p>Declarai comes with minimal dependencies out of the box, to keep the core of the library clean and simple. If you would like to extend the functionality of Declarai, you can install one of the following integrations.</p>"},{"location":"integrations/#wandb","title":"Wandb","text":"<p>Weights &amp; Biases is a popular tool for tracking machine learning experiments. Recently they have provided an API for their tracking prompts in their platform. The platform has a free tier which you can use to experiment!</p> <pre><code>pip install declarai[wandb]\n</code></pre> <p>Info</p> <p>To use this integration you will need to create an account at wandb. Once created,   you can create a new project and get your API key from the settings page.</p> <p>Once set up, you can use the <code>WandDBMonitorCreator</code> to track your prompts in the platform.</p> <p><pre><code>from typing import Dict\nimport declarai\nfrom declarai.middleware import WandDBMonitorCreator\n\n\ngpt_35 = declarai.openai(model=\"gpt-3.5-turbo\")\n\nWandDBMonitor = WandDBMonitorCreator(\n    name=\"&lt;context-name&gt;\",\n    project=\"&lt;project-name&gt;\",\n    key=\"&lt;your-decorators-key&gt;\",\n)\n\n\n@gpt_35.task(middlewares=[WandDBMonitor])\ndef extract_info(text: str) -&gt; Dict[str, str]:\n\"\"\"\n    Extract the phone number, name and email from the provided text\n    :param text: content to extract the info from\n    :return: The info extracted from the text\n    \"\"\"\n    return declarai.magic(text=text)\n</code></pre> The tracked prompts should look like this:</p> <p> </p>"},{"location":"providers/","title":"Index","text":"<p>Declarai supports the following providers:</p> <ul> <li>OpenAI</li> <li>Azure OpenAI</li> </ul>"},{"location":"providers/azure_openai/","title":"Azure OpenAI","text":"<p>To use Azure OpenAI models, you can set the following configuration options:</p> <pre><code>import declarai\n\nazure_model = declarai.azure_openai(\n    azure_openai_key=\"&lt;api-token&gt;\",\n    azure_openai_api_base=\"&lt;api-base&gt;\",\n    deployment_name=\"&lt;deployment-name&gt;\",\n    api_version=\"&lt;api-version&gt;\",\n    headers={\"&lt;header-key&gt;\": \"&lt;header-value&gt;\"},\n    timeout=\"&lt;timeout&gt;\",\n    request_timeout=\"&lt;request_timeout&gt;\",\n    stream=\"&lt;stream&gt;\",\n)\n</code></pre> Argument Env Variable Required? azure_openai_key <code>DECLARAI_AZURE_OPENAI_KEY</code> \u2705 azure_openai_api_base <code>DECLARAI_AZURE_OPENAI_API_BASE</code> \u2705 deployment_name <code>DECLARAI_AZURE_OPENAI_DEPLOYMENT_NAME</code> \u2705 api_version <code>DECLARAI_AZURE_OPENAI_API_VERSION</code> headers timeout request_timeout stream"},{"location":"providers/azure_openai/#getting-an-api-key-api-base-and-deployment-name","title":"Getting an API key, API base, and Deployment name","text":"<p>To obtain the above settings, you will need to create an account on the Azure OpenAI website. Once you have created an account, you will need to create a resource.</p> <p>Please follow the instructions on the Azure OpenAI</p>"},{"location":"providers/azure_openai/#setting-the-api-key","title":"Setting the API key","text":"<p>You can set your API key at runtime like this:</p> <pre><code>import declarai\n\nmy_azure_model = declarai.azure_openai(\n    deployment_name=\"my-model\",\n    azure_openai_key=\"&lt;your API key&gt;\",\n    azure_openai_api_base=\"https://&lt;my-azure-domain&gt;.com\",\n)\n</code></pre> <p>However, it is preferable to pass sensitive settings as an environment variable: <code>DECLARAI_AZURE_OPENAI_API_KEY</code>.</p> <p>To establish your Azure OpenAI API key as an environment variable, launch your terminal and execute the following command, substituting  with your actual key: <pre><code>export DECLARAI_AZURE_OPENAI_KEY=&lt;your API key&gt;\n</code></pre> <p>This action will maintain the key for the duration of your terminal session. To ensure a longer retention, modify your terminal's settings or corresponding environment files.</p>"},{"location":"providers/azure_openai/#control-llm-parameters","title":"Control LLM Parameters","text":"<p>OpenAI models have a number of parameters that can be tuned to control the output of the model. These parameters are passed to the declarai task/chat interface as a dictionary. The following parameters are supported:</p> Parameter Type Description Default <code>temperature</code> <code>float</code> Controls the randomness of the model. Lower values make the model more deterministic and repetitive. Higher values make the model more random and creative. <code>0</code> <code>max_tokens</code> <code>int</code> Controls the length of the output. <code>3000</code> <code>top_p</code> <code>float</code> Controls the diversity of the model. Lower values make the model more repetitive and conservative. Higher values make the model more random and creative. <code>1</code> <code>frequency_penalty</code> <code>float</code> Controls how often the model repeats itself. Lower values make the model more repetitive and conservative. Higher values make the model more random and creative. <code>0</code> <code>presence_penalty</code> <code>float</code> Controls how often the model generates new topics. Lower values make the model more repetitive and conservative. Higher values make the model more random and creative. <code>0</code> <p>Pass your custom parameters to the declarai task/chat interface as a dictionary:</p> <pre><code>import declarai\n\nazure_model = declarai.azure_openai(\n    deployment_name=\"my-model\",\n    azure_openai_key=\"&lt;your API key&gt;\",\n    azure_openai_api_base=\"https://&lt;my-azure-domain&gt;.com\",\n    headers=\"&lt;my-headers&gt;\"\n)\n\n\n@azure_model.task(llm_params={\"temperature\": 0.5, \"max_tokens\": 1000})  # (1)!\ndef generate_song():\n\"\"\"\n    Generate a song about declarai\n    \"\"\"\n</code></pre> <ol> <li>Pass only the parameters you want to change. The rest will be set to their default values.</li> </ol>"},{"location":"providers/openai/","title":"OpenAI","text":"<p>To use OpenAI models, you can set the following configuration options:</p> <pre><code>import declarai\n\nopenai_model = declarai.openai(\n    model=\"&lt;model&gt;\",\n    openai_token=\"&lt;api-token&gt;\",\n    headers={\"&lt;header-key&gt;\": \"&lt;header-value&gt;\"},\n    timeout=\"&lt;timeout&gt;\",\n    request_timeout=\"&lt;request_timeout&gt;\",\n    stream=\"&lt;stream&gt;\", \n )\n</code></pre> Setting Env Variable Required? Model \u2705 API key <code>OPENAI_API_KEY</code> \u2705 Headers Timeout Request timeout Stream"},{"location":"providers/openai/#getting-an-api-key","title":"Getting an API key","text":"<p>To obtain an OpenAI API key, follow these steps:</p> <ol> <li>Log in to your OpenAI account (sign up if you don't have one)</li> <li>Go to the \"API Keys\" page under your account settings.</li> <li>Click \"Create new secret key.\" A new API key will be generated. Make sure to copy the key to your clipboard, as you    will not be able to see it again.</li> </ol>"},{"location":"providers/openai/#setting-the-api-key","title":"Setting the API key","text":"<p>You can set your API key at runtime like this:</p> <pre><code>import declarai\n\ngpt4 = declarai.openai(model=\"gpt4\", openai_token=\"&lt;your API key&gt;\")\n</code></pre> <p>However, it is preferable to pass sensitive settings as an environment variable: <code>OPENAI_API_KEY</code>.</p> <p>To establish your OpenAI API key as an environment variable, launch your terminal and execute the following command, substituting  with your actual key: <pre><code>export OPENAI_API_KEY=&lt;your API key&gt;\n</code></pre> <p>This action will maintain the key for the duration of your terminal session. To ensure a longer retention, modify your terminal's settings or corresponding environment files.</p>"},{"location":"providers/openai/#control-llm-parameters","title":"Control LLM Parameters","text":"<p>OpenAI models have a number of parameters that can be tuned to control the output of the model. These parameters are passed to the declarai task/chat interface as a dictionary. The following parameters are supported:</p> Parameter Type Description Default <code>temperature</code> <code>float</code> Controls the randomness of the model. Lower values make the model more deterministic and repetitive. Higher values make the model more random and creative. <code>0</code> <code>max_tokens</code> <code>int</code> Controls the length of the output. <code>3000</code> <code>top_p</code> <code>float</code> Controls the diversity of the model. Lower values make the model more repetitive and conservative. Higher values make the model more random and creative. <code>1</code> <code>frequency_penalty</code> <code>float</code> Controls how often the model repeats itself. Lower values make the model more repetitive and conservative. Higher values make the model more random and creative. <code>0</code> <code>presence_penalty</code> <code>float</code> Controls how often the model generates new topics. Lower values make the model more repetitive and conservative. Higher values make the model more random and creative. <code>0</code> <p>Pass your custom parameters to the declarai task/chat interface as a dictionary:</p> <pre><code>import declarai\n\ngpt4 = declarai.openai(model=\"gpt-4\", openai_token=\"&lt;your API key&gt;\")\n\n\n@gpt4.task(llm_params={\"temperature\": 0.5, \"max_tokens\": 1000})  # (1)!\ndef generate_song():\n\"\"\"\n    Generate a song about declarai\n    \"\"\"\n</code></pre> <ol> <li>Pass only the parameters you want to change. The rest will be set to their default values.</li> </ol>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li> declarai<ul> <li> _base</li> <li> chat</li> <li> core<ul> <li> core_settings</li> </ul> </li> <li> declarai</li> <li> memory<ul> <li> base</li> <li> file</li> <li> in_memory</li> <li> mongodb</li> <li> postgres</li> <li> redis</li> </ul> </li> <li> middleware<ul> <li> base</li> <li> internal<ul> <li> log_middleware</li> </ul> </li> <li> third_party<ul> <li> wandb_monitor</li> </ul> </li> </ul> </li> <li> operators<ul> <li> llm</li> <li> message</li> <li> openai_operators<ul> <li> chat_operator</li> <li> openai_llm</li> <li> settings</li> <li> task_operator</li> </ul> </li> <li> operator</li> <li> registry</li> <li> templates<ul> <li> chain_of_thought</li> <li> instruct_function</li> <li> output_prompt</li> <li> output_structure</li> </ul> </li> <li> utils</li> </ul> </li> <li> python_parser<ul> <li> docstring_parsers<ul> <li> reST<ul> <li> parser</li> </ul> </li> <li> types</li> </ul> </li> <li> magic_parser</li> <li> parser</li> <li> type_annotation_to_schema</li> <li> types</li> </ul> </li> <li> task</li> </ul> </li> </ul>"},{"location":"reference/declarai/","title":"Index","text":""},{"location":"reference/declarai/#declarai","title":"declarai","text":"<p>Modules:</p> Name Description <code>_base</code> <p>Base classes for declarai tasks.</p> <code>chat</code> <p>Chat tasks definition.</p> <code>core</code> <p>Core settings for Declarai.</p> <code>declarai</code> <p>Main interface for declarai.</p> <code>evals</code> <p>Evaluation suite module for DeclarAI.</p> <code>memory</code> <p>Memory module for Declarai interactions that includes message history.</p> <code>middleware</code> <p>Middleware module for Declarai.</p> <code>operators</code> <p>Operators are the main interface that interacts internally with the LLMs.</p> <code>python_parser</code> <p>Internal package for parsing User Python code into a ParsedFunction object.</p> <code>task</code> <p>Task interface</p>"},{"location":"reference/declarai/_base/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> _base","text":""},{"location":"reference/declarai/_base/#declarai._base","title":"_base","text":"<p>Base classes for declarai tasks.</p> <p>Classes:</p> Name Description <code>BaseTask</code> <p>Base class for tasks.</p>"},{"location":"reference/declarai/_base/#declarai._base.BaseTask","title":"BaseTask","text":"<p>Base class for tasks.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Orchestrates the execution of the task</p> <code>_exec</code> <p>Execute the task</p> <code>_exec_middlewares</code> <p>Execute the task middlewares and the task itself</p> <code>compile</code> <p>Compile the task to get the prompt sent to the LLM</p> <code>stream_handler</code> <p>A generator that yields each chunk from the stream and collects them in a buffer.</p> <p>Attributes:</p> Name Type Description <code>llm_params</code> <code>LLMParamsType</code> <p>Return the LLM parameters that are saved on the operator. These parameters are sent to the LLM when the task is</p> <code>llm_response</code> <code>LLMResponse</code> <p>The response from the LLM</p> <code>llm_stream_response</code> <code>Iterator[LLMResponse]</code> <p>The response from the LLM when streaming</p> <code>operator</code> <code>BaseOperator</code> <p>The operator to use for the task</p>"},{"location":"reference/declarai/_base/#declarai._base.BaseTask.llm_params","title":"llm_params  <code>property</code>","text":"<pre><code>llm_params: LLMParamsType\n</code></pre> <p>Return the LLM parameters that are saved on the operator. These parameters are sent to the LLM when the task is executed. Returns: The LLM parameters</p>"},{"location":"reference/declarai/_base/#declarai._base.BaseTask.llm_response","title":"llm_response  <code>instance-attribute</code>","text":"<pre><code>llm_response: LLMResponse\n</code></pre> <p>The response from the LLM</p>"},{"location":"reference/declarai/_base/#declarai._base.BaseTask.llm_stream_response","title":"llm_stream_response  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>llm_stream_response: Iterator[LLMResponse] = None\n</code></pre> <p>The response from the LLM when streaming</p>"},{"location":"reference/declarai/_base/#declarai._base.BaseTask.operator","title":"operator  <code>instance-attribute</code>","text":"<pre><code>operator: BaseOperator\n</code></pre> <p>The operator to use for the task</p>"},{"location":"reference/declarai/_base/#declarai._base.BaseTask.__call__","title":"__call__","text":"<pre><code>__call__(*args, **kwargs)\n</code></pre> <p>Orchestrates the execution of the task Args:     args: Depends on the inherited class     *kwargs: Depends on the inherited class</p> <p>Returns: The result of the task, after parsing the result of the llm.</p> Source code in <code>src/declarai/_base.py</code> <pre><code>def __call__(self, *args, **kwargs):\n\"\"\"\n    Orchestrates the execution of the task\n    Args:\n        *args: Depends on the inherited class\n        **kwargs: Depends on the inherited class\n\n    Returns: The result of the task, after parsing the result of the llm.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/declarai/_base/#declarai._base.BaseTask._exec","title":"_exec  <code>abstractmethod</code>","text":"<pre><code>_exec(kwargs: dict) -&gt; Any\n</code></pre> <p>Execute the task Args:     kwargs: the runtime keyword arguments that are used to compile the task prompt.</p> <p>Returns: The result of the task, which is the result of the operator.</p> Source code in <code>src/declarai/_base.py</code> <pre><code>@abstractmethod\ndef _exec(self, kwargs: dict) -&gt; Any:\n\"\"\"\n    Execute the task\n    Args:\n        kwargs: the runtime keyword arguments that are used to compile the task prompt.\n\n    Returns: The result of the task, which is the result of the operator.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/declarai/_base/#declarai._base.BaseTask._exec_middlewares","title":"_exec_middlewares  <code>abstractmethod</code>","text":"<pre><code>_exec_middlewares(kwargs) -&gt; Any\n</code></pre> <p>Execute the task middlewares and the task itself Args:     kwargs: the runtime keyword arguments that are used to compile the task prompt.</p> <p>Returns: The result of the task, which is the result of the operator. Same as <code>_exec</code>.</p> Source code in <code>src/declarai/_base.py</code> <pre><code>@abstractmethod\ndef _exec_middlewares(self, kwargs) -&gt; Any:\n\"\"\"\n    Execute the task middlewares and the task itself\n    Args:\n        kwargs: the runtime keyword arguments that are used to compile the task prompt.\n\n    Returns: The result of the task, which is the result of the operator. Same as `_exec`.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/declarai/_base/#declarai._base.BaseTask.compile","title":"compile  <code>abstractmethod</code>","text":"<pre><code>compile(**kwargs) -&gt; str\n</code></pre> <p>Compile the task to get the prompt sent to the LLM Args:     **kwargs: the runtime keyword arguments that are placed within the prompt string.</p> <p>Returns: The prompt string that is sent to the LLM</p> Source code in <code>src/declarai/_base.py</code> <pre><code>@abstractmethod\ndef compile(self, **kwargs) -&gt; str:\n\"\"\"\n    Compile the task to get the prompt sent to the LLM\n    Args:\n        **kwargs: the runtime keyword arguments that are placed within the prompt string.\n\n    Returns: The prompt string that is sent to the LLM\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/declarai/_base/#declarai._base.BaseTask.stream_handler","title":"stream_handler","text":"<pre><code>stream_handler(\n    stream: Iterator[LLMResponse],\n) -&gt; Iterator[LLMResponse]\n</code></pre> <p>A generator that yields each chunk from the stream and collects them in a buffer. After the stream is exhausted, it runs the cleanup logic.</p> Source code in <code>src/declarai/_base.py</code> <pre><code>def stream_handler(self, stream: Iterator[LLMResponse]) -&gt; Iterator[LLMResponse]:\n\"\"\"\n    A generator that yields each chunk from the stream and collects them in a buffer.\n    After the stream is exhausted, it runs the cleanup logic.\n    \"\"\"\n    response_buffer = []\n    for chunk in stream:\n        response_buffer.append(chunk)\n        yield chunk\n\n    # After the stream is exhausted, run the cleanup logic\n    self.stream_cleanup(response_buffer[-1])\n</code></pre>"},{"location":"reference/declarai/chat/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> chat","text":""},{"location":"reference/declarai/chat/#declarai.chat","title":"chat","text":"<p>Chat tasks definition.</p> <p>Chat tasks are tasks that are meant to be used in an iterative fashion, where the user and the assistant exchange  messages.</p> <p>Unlike tasks, chat tasks are storing the message history in a <code>BaseChatMessageHistory</code> object, which is used to compile  the prompt sent to the LLM.</p> <p>At every iteration, the user message is added to the message history, and the prompt is compiled using the message  history. The prompt is then sent to the LLM, and the response is parsed and added to the message history.</p> <p>Classes:</p> Name Description <code>Chat</code> <p>Chat class used for creating chat tasks.</p> <code>ChatDecorator</code> <p>A decorator class for receiving a chat class, fulfilled with the provided parameters, and returning a Chat object.</p> <code>ChatMeta</code> <p>Metaclass for Chat classes. Used to enable the users to receive the chat instance when using the @chat decorator,</p>"},{"location":"reference/declarai/chat/#declarai.chat.Chat","title":"Chat","text":"<pre><code>Chat(\n    *,\n    operator: BaseChatOperator,\n    middlewares: List[Type[TaskMiddleware]] = None,\n    chat_history: BaseChatMessageHistory = None,\n    greeting: str = None,\n    system: str = None,\n    **kwargs: str\n)\n</code></pre> <p>             Bases: <code>BaseTask</code></p> <p>Chat class used for creating chat tasks.</p> <p>Chat tasks are tasks that are meant to be used in an iterative fashion, where the user and the assistant exchange messages.</p> <p>Attributes:</p> Name Type Description <code>is_declarai</code> <code>bool</code> <p>A class-level attribute indicating if the chat is of type 'declarai'. Always set to <code>True</code>.</p> <code>_call_kwargs</code> <code>Dict[str, Any]</code> <p>A dictionary to store additional keyword arguments, used for passing kwargs between the execution of the chat and the execution of the middlewares.</p> <code>middlewares</code> <code>List[TaskMiddleware] or None</code> <p>Middlewares used for every iteration of the chat.</p> <code>operator</code> <code>BaseChatOperator</code> <p>The operator used for the chat.</p> <code>conversation</code> <code>List[Message]</code> <p>Property that returns a list of messages exchanged in the chat. Keep in mind this list does not include the first system message. The system message is stored in the <code>system</code> attribute.</p> <code>_chat_history</code> <code>BaseChatMessageHistory</code> <p>The chat history mechanism for the chat.</p> <code>greeting</code> <code>str</code> <p>The greeting message for the chat.</p> <code>system</code> <code>str</code> <p>The system message for the chat.</p> <p>Parameters:</p> Name Type Description Default <code>operator</code> <code>BaseChatOperator</code> <p>The operator to use for the chat.</p> required <code>middlewares</code> <code>List[TaskMiddleware]</code> <p>Middlewares to use for every iteration of the chat. Defaults to</p> <code>None</code> <code>chat_history</code> <code>BaseChatMessageHistory</code> <p>Chat history mechanism to use. Defaults to <code>DEFAULT_CHAT_HISTORY()</code>.</p> <code>None</code> <code>greeting</code> <code>str</code> <p>Greeting message to use. Defaults to operator's greeting or None.</p> <code>None</code> <code>system</code> <code>str</code> <p>System message to use. Defaults to operator's system message or None.</p> <code>None</code> <code>stream</code> <code>bool</code> <p>Whether to stream the response from the LLM or not. Defaults to False.</p> required <code>**kwargs</code> <p>Additional keyword arguments to pass to the formatting of the system message.</p> <code>{}</code> <p>Methods:</p> Name Description <code>__call__</code> <p>Executes the call to the LLM, based on the messages passed as argument, and the llm_params.</p> <code>_exec</code> <p>Executes the call to the LLM.</p> <code>add_message</code> <p>Interface to add a message to the chat history.</p> <code>compile</code> <p>Compiles a list of messages to be sent to the LLM by the operator.</p> <code>send</code> <p>Interface that allows the user to send a message to the LLM. It takes a raw string as input, and returns</p> <code>stream_cleanup</code> <p>Add the combined response to the database and run any other cleanup logic.</p> <code>stream_handler</code> <p>A generator that yields each chunk from the stream and collects them in a buffer.</p> <p>Attributes:</p> Name Type Description <code>conversation</code> <code>List[Message]</code> <p>Returns:</p> <code>llm_params</code> <code>LLMParamsType</code> <p>Return the LLM parameters that are saved on the operator. These parameters are sent to the LLM when the task is</p> <code>llm_response</code> <code>LLMResponse</code> <p>The response from the LLM</p> <code>llm_stream_response</code> <code>Iterator[LLMResponse]</code> <p>The response from the LLM when streaming</p> Source code in <code>src/declarai/chat.py</code> <pre><code>def __init__(\n    self,\n    *,\n    operator: BaseChatOperator,\n    middlewares: List[Type[TaskMiddleware]] = None,\n    chat_history: BaseChatMessageHistory = None,\n    greeting: str = None,\n    system: str = None,\n    **kwargs,\n):\n    self.middlewares = middlewares\n    self.operator = operator\n    self._chat_history = chat_history or DEFAULT_CHAT_HISTORY()\n    self.greeting = greeting or self.operator.greeting\n    self.system = self.__set_system_prompt(system=system, **kwargs)\n    self.__set_memory()\n</code></pre>"},{"location":"reference/declarai/chat/#declarai.chat.Chat.conversation","title":"conversation  <code>property</code>","text":"<pre><code>conversation: List[Message]\n</code></pre> <p>Returns:</p> Type Description <code>List[Message]</code> <p>a list of messages exchanged in the chat. Keep in mind this list does not include the first system message.</p>"},{"location":"reference/declarai/chat/#declarai.chat.Chat.llm_params","title":"llm_params  <code>property</code>","text":"<pre><code>llm_params: LLMParamsType\n</code></pre> <p>Return the LLM parameters that are saved on the operator. These parameters are sent to the LLM when the task is executed. Returns: The LLM parameters</p>"},{"location":"reference/declarai/chat/#declarai.chat.Chat.llm_response","title":"llm_response  <code>instance-attribute</code>","text":"<pre><code>llm_response: LLMResponse\n</code></pre> <p>The response from the LLM</p>"},{"location":"reference/declarai/chat/#declarai.chat.Chat.llm_stream_response","title":"llm_stream_response  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>llm_stream_response: Iterator[LLMResponse] = None\n</code></pre> <p>The response from the LLM when streaming</p>"},{"location":"reference/declarai/chat/#declarai.chat.Chat.__call__","title":"__call__","text":"<pre><code>__call__(\n    *,\n    messages: List[Message],\n    llm_params: LLMParamsType = None\n) -&gt; Any\n</code></pre> <p>Executes the call to the LLM, based on the messages passed as argument, and the llm_params. The llm_params are passed as a dictionary, and they are used to override the default llm_params of the operator. The llm_params also have priority over the params that were used to initialize the chat within the decorator. Args:     messages: The messages to pass to the LLM.     llm_params: The llm_params to use for the call to the LLM.</p> <p>Returns:</p> Type Description <code>Any</code> <p>The parsed response from the LLM.</p> Source code in <code>src/declarai/chat.py</code> <pre><code>def __call__(\n    self, *, messages: List[Message], llm_params: LLMParamsType = None\n) -&gt; Any:\n\"\"\"\n    Executes the call to the LLM, based on the messages passed as argument, and the llm_params.\n    The llm_params are passed as a dictionary, and they are used to override the default llm_params of the operator.\n    The llm_params also have priority over the params that were used to initialize the chat within the decorator.\n    Args:\n        messages: The messages to pass to the LLM.\n        llm_params: The llm_params to use for the call to the LLM.\n\n    Returns:\n        The parsed response from the LLM.\n\n    \"\"\"\n    runtime_kwargs = dict(messages=messages)\n    runtime_llm_params = (\n        llm_params or self.llm_params\n    )  # order is important! We prioritize runtime params that\n    if runtime_llm_params:\n        runtime_kwargs[\"llm_params\"] = runtime_llm_params\n\n    self._call_kwargs = runtime_kwargs\n    return self._exec_middlewares(runtime_kwargs)\n</code></pre>"},{"location":"reference/declarai/chat/#declarai.chat.Chat._exec","title":"_exec","text":"<pre><code>_exec(kwargs) -&gt; Any\n</code></pre> <p>Executes the call to the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>Keyword arguments to pass to the LLM like <code>temperature</code>, <code>max_tokens</code>, etc.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The raw response from the LLM, together with the metadata.</p> Source code in <code>src/declarai/chat.py</code> <pre><code>def _exec(self, kwargs) -&gt; Any:\n\"\"\"\n    Executes the call to the LLM.\n\n    Args:\n        kwargs: Keyword arguments to pass to the LLM like `temperature`, `max_tokens`, etc.\n\n    Returns:\n         The raw response from the LLM, together with the metadata.\n    \"\"\"\n    if self.operator.streaming:\n        # Use the stream_handler generator if streaming is enabled\n        stream = self.stream_handler(self.operator.predict(**kwargs))\n        self.llm_stream_response = stream\n        return self.llm_stream_response\n    else:\n        self.llm_response = self.operator.predict(**kwargs)\n        self.add_message(self.llm_response.response, role=MessageRole.assistant)\n        if self.operator.parsed_send_func:\n            return self.operator.parsed_send_func.parse(self.llm_response.response)\n        return self.llm_response.response\n</code></pre>"},{"location":"reference/declarai/chat/#declarai.chat.Chat.add_message","title":"add_message","text":"<pre><code>add_message(message: str, role: MessageRole) -&gt; None\n</code></pre> <p>Interface to add a message to the chat history. Args:     message (str): The message to add to the chat history.     role (MessageRole): The role of the message (assistant, user, system, etc.)</p> Source code in <code>src/declarai/chat.py</code> <pre><code>def add_message(self, message: str, role: MessageRole) -&gt; None:\n\"\"\"\n    Interface to add a message to the chat history.\n    Args:\n        message (str): The message to add to the chat history.\n        role (MessageRole): The role of the message (assistant, user, system, etc.)\n    \"\"\"\n    self._chat_history.add_message(Message(message=message, role=role))\n</code></pre>"},{"location":"reference/declarai/chat/#declarai.chat.Chat.compile","title":"compile","text":"<pre><code>compile(**kwargs) -&gt; List[Message]\n</code></pre> <p>Compiles a list of messages to be sent to the LLM by the operator. This is done by accessing the ._chat_history.history attribute. The kwargs that are passed to the compile method are onlu used to populate the system message prompt. Args:     **kwargs: System message prompt kwargs.</p> <p>Returns: List[Message] - The compiled messages that will be sent to the LLM.</p> Source code in <code>src/declarai/chat.py</code> <pre><code>def compile(self, **kwargs) -&gt; List[Message]:\n\"\"\"\n    Compiles a list of messages to be sent to the LLM by the operator.\n    This is done by accessing the ._chat_history.history attribute.\n    The kwargs that are passed to the compile method are onlu used to populate the system message prompt.\n    Args:\n        **kwargs: System message prompt kwargs.\n\n    Returns: List[Message] - The compiled messages that will be sent to the LLM.\n\n    \"\"\"\n    messages = kwargs.pop(\"messages\", None) or self._chat_history.history\n    compiled = self.operator.compile(messages=messages, **kwargs)\n    return compiled\n</code></pre>"},{"location":"reference/declarai/chat/#declarai.chat.Chat.send","title":"send","text":"<pre><code>send(\n    message: str,\n    llm_params: Union[LLMParamsType, Dict[str, Any]] = None,\n    **kwargs: Union[LLMParamsType, Dict[str, Any]]\n) -&gt; Any\n</code></pre> <p>Interface that allows the user to send a message to the LLM. It takes a raw string as input, and returns  the raw response from the LLM. Args:     message:     llm_params:     **kwargs:</p> <p>Returns:</p> Type Description <code>Any</code> <p>Final response from the LLM, after parsing.</p> Source code in <code>src/declarai/chat.py</code> <pre><code>def send(\n    self,\n    message: str,\n    llm_params: Union[LLMParamsType, Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; Any:\n\"\"\"\n    Interface that allows the user to send a message to the LLM. It takes a raw string as input, and returns\n     the raw response from the LLM.\n    Args:\n        message:\n        llm_params:\n        **kwargs:\n\n    Returns:\n        Final response from the LLM, after parsing.\n\n    \"\"\"\n    self.add_message(message, role=MessageRole.user)\n    return self(\n        messages=self._chat_history.history, llm_params=llm_params, **kwargs\n    )\n</code></pre>"},{"location":"reference/declarai/chat/#declarai.chat.Chat.stream_cleanup","title":"stream_cleanup","text":"<pre><code>stream_cleanup(last_chunk: LLMResponse) -&gt; None\n</code></pre> <p>Add the combined response to the database and run any other cleanup logic.</p> Source code in <code>src/declarai/chat.py</code> <pre><code>def stream_cleanup(self, last_chunk: LLMResponse) -&gt; None:\n\"\"\"\n    Add the combined response to the database and run any other cleanup logic.\n    \"\"\"\n    super().stream_cleanup(last_chunk)\n    self.add_message(last_chunk.response, role=MessageRole.assistant)\n</code></pre>"},{"location":"reference/declarai/chat/#declarai.chat.Chat.stream_handler","title":"stream_handler","text":"<pre><code>stream_handler(\n    stream: Iterator[LLMResponse],\n) -&gt; Iterator[LLMResponse]\n</code></pre> <p>A generator that yields each chunk from the stream and collects them in a buffer. After the stream is exhausted, it runs the cleanup logic.</p> Source code in <code>src/declarai/_base.py</code> <pre><code>def stream_handler(self, stream: Iterator[LLMResponse]) -&gt; Iterator[LLMResponse]:\n\"\"\"\n    A generator that yields each chunk from the stream and collects them in a buffer.\n    After the stream is exhausted, it runs the cleanup logic.\n    \"\"\"\n    response_buffer = []\n    for chunk in stream:\n        response_buffer.append(chunk)\n        yield chunk\n\n    # After the stream is exhausted, run the cleanup logic\n    self.stream_cleanup(response_buffer[-1])\n</code></pre>"},{"location":"reference/declarai/chat/#declarai.chat.ChatDecorator","title":"ChatDecorator","text":"<pre><code>ChatDecorator(llm: LLM)\n</code></pre> <p>A decorator class for receiving a chat class, fulfilled with the provided parameters, and returning a Chat object.</p> <p>This class provides the <code>chat</code> method which acts as a decorator to create a Chat object.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>LLM</code> <p>Resolved LLM object.</p> required <p>Attributes:</p> Name Type Description <code>llm</code> <code>LLM</code> <p>Resolved LLM object.</p> <p>Methods:</p> Name Description <code>chat</code> <p>Decorator method that converts a class into a chat task class.</p> Source code in <code>src/declarai/chat.py</code> <pre><code>def __init__(self, llm: LLM):\n    self.llm = llm\n</code></pre>"},{"location":"reference/declarai/chat/#declarai.chat.ChatDecorator.chat","title":"chat","text":"<pre><code>chat(\n    cls: Type = None,\n    *,\n    middlewares: List[TaskMiddleware] = None,\n    llm_params: LLMParamsType = None,\n    chat_history: BaseChatMessageHistory = None,\n    greeting: str = None,\n    system: str = None,\n    streaming: bool = None\n)\n</code></pre> <p>Decorator method that converts a class into a chat task class.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Type</code> <p>The original class that is being decorated.</p> <code>None</code> <code>middlewares</code> <code>List[TaskMiddleware]</code> <p>Middlewares to use for every iteration of the chat. Defaults to None.</p> <code>None</code> <code>llm_params</code> <code>LLMParamsType</code> <p>Parameters for the LLM. Defaults to None.</p> <code>None</code> <code>chat_history</code> <code>BaseChatMessageHistory</code> <p>Chat history mechanism to use. Defaults to None.</p> <code>None</code> <code>greeting</code> <code>str</code> <p>Greeting message to use. Defaults to None.</p> <code>None</code> <code>system</code> <code>str</code> <p>System message to use. Defaults to None.</p> <code>None</code> <code>streaming</code> <code>bool</code> <p>Whether to use streaming or not. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Type[Chat]</code> <p>A new Chat class that inherits from the original class and has chat capabilities.</p> Example <pre><code> @ChatDecorator.chat(llm_params={\"temperature\": 0.5})\n class MyChat:\n    ...\n\n @ChatDecorator.chat\n class MyChat:\n    ...\n</code></pre> Source code in <code>src/declarai/chat.py</code> <pre><code>def chat(\n    self,\n    cls: Type = None,\n    *,\n    middlewares: List[TaskMiddleware] = None,\n    llm_params: LLMParamsType = None,\n    chat_history: BaseChatMessageHistory = None,\n    greeting: str = None,\n    system: str = None,\n    streaming: bool = None,\n):\n\"\"\"\n    Decorator method that converts a class into a chat task class.\n\n    Args:\n        cls (Type, optional): The original class that is being decorated.\n        middlewares (List[TaskMiddleware], optional): Middlewares to use for every iteration of the chat.\n         Defaults to None.\n        llm_params (LLMParamsType, optional): Parameters for the LLM. Defaults to None.\n        chat_history (BaseChatMessageHistory, optional): Chat history mechanism to use. Defaults to None.\n        greeting (str, optional): Greeting message to use. Defaults to None.\n        system (str, optional): System message to use. Defaults to None.\n        streaming (bool, optional): Whether to use streaming or not. Defaults to None.\n\n    Returns:\n        (Type[Chat]): A new Chat class that inherits from the original class and has chat capabilities.\n\n\n    Example:\n        ```python\n         @ChatDecorator.chat(llm_params={\"temperature\": 0.5})\n         class MyChat:\n            ...\n\n         @ChatDecorator.chat\n         class MyChat:\n            ...\n        ```\n\n    \"\"\"\n    operator_type = resolve_operator(self.llm, operator_type=\"chat\")\n\n    def wrap(cls) -&gt; Type[Chat]:\n        non_private_methods = {\n            method_name: method\n            for method_name, method in cls.__dict__.items()\n            if not method_name.startswith(\"__\") and callable(method)\n        }\n        if \"send\" in non_private_methods:\n            non_private_methods.pop(\"send\")\n\n        parsed_cls = PythonParser(cls)\n\n        _decorator_kwargs = dict(\n            operator=operator_type(\n                llm=self.llm,\n                parsed=parsed_cls,\n                llm_params=llm_params,\n                streaming=streaming,\n            ),\n            middlewares=middlewares,\n            chat_history=chat_history,\n            greeting=greeting,\n            system=system,\n        )\n\n        new_chat: Type[Chat] = type(cls.__name__, (Chat,), {})  # noqa\n        new_chat.__name__ = cls.__name__\n        new_chat._init_args = ()  # any positional arguments\n        new_chat._init_kwargs = _decorator_kwargs\n        for method_name, method in non_private_methods.items():\n            if isinstance(method, Task):\n                _method = method\n            else:\n                _method = partial(method, new_chat)\n            setattr(new_chat, method_name, _method)\n        return new_chat\n\n    if cls is None:\n        return wrap\n    return wrap(cls)\n</code></pre>"},{"location":"reference/declarai/chat/#declarai.chat.ChatMeta","title":"ChatMeta","text":"<p>             Bases: <code>type</code></p> <p>Metaclass for Chat classes. Used to enable the users to receive the chat instance when using the @chat decorator, and still be able to \"instantiate\" the class.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Initialize the Chat instance for the second time, after the decorator has been applied. The parameters are</p>"},{"location":"reference/declarai/chat/#declarai.chat.ChatMeta.__call__","title":"__call__","text":"<pre><code>__call__(*args, **kwargs)\n</code></pre> <p>Initialize the Chat instance for the second time, after the decorator has been applied. The parameters are the same as the ones used for the decorator, but the ones used for the class initialization are precedence. Returns: Chat instance</p> Source code in <code>src/declarai/chat.py</code> <pre><code>def __call__(cls, *args, **kwargs):\n\"\"\"\n    Initialize the Chat instance for the second time, after the decorator has been applied. The parameters are\n    the same as the ones used for the decorator, but the ones used for the class initialization are precedence.\n    Returns: Chat instance\n\n    \"\"\"\n    # Determine which arguments to use for initialization\n    final_args = args if args else cls._init_args\n    final_kwargs = {**cls._init_kwargs, **kwargs}\n\n    # Create and initialize the instance\n    instance = super().__call__(*final_args, **final_kwargs)\n\n    # Always set the __name__ attribute on the instance\n    instance.__name__ = cls.__name__\n\n    return instance\n</code></pre>"},{"location":"reference/declarai/declarai/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> declarai","text":""},{"location":"reference/declarai/declarai/#declarai.declarai","title":"declarai","text":"<p>Main interface for declarai.</p> <p>Decorates the package functionalities and serve as the main interface for the user.</p> <p>Classes:</p> Name Description <code>Declarai</code> <p>Context manager for the Declarai root interface.</p> <p>Functions:</p> Name Description <code>azure_openai</code> <p>Sets up a Declarai context for the Azure OpenAI provider.</p> <code>magic</code> <p>This is an empty method used as a potential replacement for using the docstring for passing</p> <code>openai</code> <p>Sets up a Declarai context for the OpenAI provider.</p> <code>register_llm</code> <p>Registers an LLM.</p> <code>register_operator</code> <p>Registers an operator.</p>"},{"location":"reference/declarai/declarai/#declarai.declarai.Declarai","title":"Declarai","text":"<pre><code>Declarai(provider: str, model: str, **kwargs: str)\n</code></pre> <p>Context manager for the Declarai root interface. This class is responsible for setting up tasks and initializing experimental features provided by Declarai.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>The provider name.</p> required <code>model</code> <code>str</code> <p>The model name.</p> required <code>**kwargs</code> <p>Additional keyword arguments passed to the LLM resolver.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>llm</code> <code>LLM</code> <p>Resolved LLM.</p> <code>task</code> <code>Callable</code> <p>A decorator for task creation.</p> <code>experimental</code> <p>A namespace for experimental features.</p> <code>experimental.chat</code> <code>Callable</code> <p>A decorator for chat operators.</p> Source code in <code>src/declarai/declarai.py</code> <pre><code>def __init__(self, provider: str, model: str, **kwargs):\n    self.llm = resolve_llm(provider, model, **kwargs)\n    self.task = TaskDecorator(self.llm).task\n\n    class Experimental:\n        chat = ChatDecorator(self.llm).chat\n\n    self.experimental = Experimental\n</code></pre>"},{"location":"reference/declarai/declarai/#declarai.declarai.azure_openai","title":"azure_openai","text":"<pre><code>azure_openai(\n    deployment_name: str,\n    azure_openai_key: str = None,\n    azure_openai_api_base: str = None,\n    api_version: str = None,\n    headers: dict = None,\n    timeout: int = None,\n    stream: bool = None,\n    request_timeout: int = None,\n) -&gt; Declarai\n</code></pre> <p>Sets up a Declarai context for the Azure OpenAI provider.</p> <p>Parameters:</p> Name Type Description Default <code>deployment_name</code> <code>str</code> <p>Name of the deployment.</p> required <code>azure_openai_key</code> <code>str</code> <p>Azure OpenAI key.</p> <code>None</code> <code>azure_openai_api_base</code> <code>str</code> <p>Base API URL for Azure OpenAI.</p> <code>None</code> <code>api_version</code> <code>str</code> <p>API version.</p> <code>None</code> <code>headers</code> <code>dict</code> <p>Additional headers for the request.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>Timeout for the request.</p> <code>None</code> <code>stream</code> <code>bool</code> <p>Whether to stream the response.</p> <code>None</code> <code>request_timeout</code> <code>int</code> <p>Request timeout duration.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DeclaraiContext</code> <code>Declarai</code> <p>Initialized Declarai context.</p> Source code in <code>src/declarai/declarai.py</code> <pre><code>def azure_openai(\n    deployment_name: str,\n    azure_openai_key: str = None,\n    azure_openai_api_base: str = None,\n    api_version: str = None,\n    headers: dict = None,\n    timeout: int = None,\n    stream: bool = None,\n    request_timeout: int = None,\n) -&gt; Declarai:\n\"\"\"\n    Sets up a Declarai context for the Azure OpenAI provider.\n\n    Args:\n        deployment_name (str): Name of the deployment.\n        azure_openai_key (str, optional): Azure OpenAI key.\n        azure_openai_api_base (str, optional): Base API URL for Azure OpenAI.\n        api_version (str, optional): API version.\n        headers (dict, optional): Additional headers for the request.\n        timeout (int, optional): Timeout for the request.\n        stream (bool, optional): Whether to stream the response.\n        request_timeout (int, optional): Request timeout duration.\n\n    Returns:\n        DeclaraiContext: Initialized Declarai context.\n    \"\"\"\n    return Declarai(\n        provider=ProviderAzureOpenai,\n        model=deployment_name,\n        azure_openai_key=azure_openai_key,\n        azure_openai_api_base=azure_openai_api_base,\n        api_version=api_version,\n        headers=headers,\n        timeout=timeout,\n        stream=stream,\n        request_timeout=request_timeout,\n    )\n</code></pre>"},{"location":"reference/declarai/declarai/#declarai.declarai.magic","title":"magic","text":"<pre><code>magic(\n    return_name: Optional[str] = None,\n    *,\n    task_desc: Optional[str] = None,\n    input_desc: Optional[Dict[str, str]] = None,\n    output_desc: Optional[str] = None,\n    **kwargs: Optional[str]\n) -&gt; Any\n</code></pre> <p>This is an empty method used as a potential replacement for using the docstring for passing parameters to the LLM builder. It can also serve as a fake use of arguments in the defined functions as to simplify handling of lint rules for llms functions.</p> Example <pre><code>@openai.task\ndef add(a: int, b: int) -&gt; int:\n    return magic(a, b)\n</code></pre> Source code in <code>src/declarai/declarai.py</code> <pre><code>def magic(\n    return_name: Optional[str] = None,\n    *,\n    task_desc: Optional[str] = None,\n    input_desc: Optional[Dict[str, str]] = None,\n    output_desc: Optional[str] = None,\n    **kwargs\n) -&gt; Any:\n\"\"\"\n    This is an empty method used as a potential replacement for using the docstring for passing\n    parameters to the LLM builder. It can also serve as a fake use of arguments in the defined\n    functions as to simplify handling of lint rules for llms functions.\n\n    Example:\n        ```py\n        @openai.task\n        def add(a: int, b: int) -&gt; int:\n            return magic(a, b)\n        ```\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/declarai/declarai/#declarai.declarai.openai","title":"openai","text":"<pre><code>openai(\n    model: ModelsOpenai,\n    version: str = None,\n    openai_token: str = None,\n    headers: dict = None,\n    timeout: int = None,\n    stream: bool = None,\n    request_timeout: int = None,\n) -&gt; Declarai\n</code></pre> <p>Sets up a Declarai context for the OpenAI provider.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelsOpenai</code> <p>The model to be used.</p> required <code>version</code> <code>str</code> <p>Model version.</p> <code>None</code> <code>openai_token</code> <code>str</code> <p>OpenAI authentication token.</p> <code>None</code> <code>headers</code> <code>dict</code> <p>Additional headers for the request.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>Timeout for the request.</p> <code>None</code> <code>stream</code> <code>bool</code> <p>Whether to stream the response.</p> <code>None</code> <code>request_timeout</code> <code>int</code> <p>Request timeout duration.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Declarai</code> <code>Declarai</code> <p>Initialized Declarai context.</p> Source code in <code>src/declarai/declarai.py</code> <pre><code>def openai(\n    model: ModelsOpenai,\n    version: str = None,\n    openai_token: str = None,\n    headers: dict = None,\n    timeout: int = None,\n    stream: bool = None,\n    request_timeout: int = None,\n) -&gt; Declarai:\n\"\"\"\n    Sets up a Declarai context for the OpenAI provider.\n\n    Args:\n        model (ModelsOpenai): The model to be used.\n        version (str, optional): Model version.\n        openai_token (str, optional): OpenAI authentication token.\n        headers (dict, optional): Additional headers for the request.\n        timeout (int, optional): Timeout for the request.\n        stream (bool, optional): Whether to stream the response.\n        request_timeout (int, optional): Request timeout duration.\n\n    Returns:\n        Declarai: Initialized Declarai context.\n    \"\"\"\n    return Declarai(\n        provider=ProviderOpenai,\n        model=model,\n        version=version,\n        openai_token=openai_token,\n        headers=headers,\n        timeout=timeout,\n        stream=stream,\n        request_timeout=request_timeout,\n    )\n</code></pre>"},{"location":"reference/declarai/declarai/#declarai.declarai.register_llm","title":"register_llm","text":"<pre><code>register_llm(\n    provider: str, llm_cls: Type[LLM], model: str = None\n)\n</code></pre> <p>Registers an LLM. Args:     provider: Name of the LLM provider.     model: Specific model name (optional).     llm_cls: The LLM class to register.</p> Source code in <code>src/declarai/declarai.py</code> <pre><code>def register_llm(provider: str, llm_cls: Type[LLM], model: str = None):\n\"\"\"\n    Registers an LLM.\n    Args:\n        provider: Name of the LLM provider.\n        model: Specific model name (optional).\n        llm_cls: The LLM class to register.\n    \"\"\"\n    llm_registry.register(provider=provider, llm_cls=llm_cls, model=model)\n</code></pre>"},{"location":"reference/declarai/declarai/#declarai.declarai.register_operator","title":"register_operator","text":"<pre><code>register_operator(\n    provider: str,\n    operator_type: str,\n    operator_cls: Type[BaseOperator],\n    model: str = None,\n)\n</code></pre> <p>Registers an operator. Args:     provider: Name of the LLM provider.     operator_type: The type of operator (e.g., \"chat\", \"task\").     operator_cls: The operator class to register.     model: Specific model name</p> Source code in <code>src/declarai/declarai.py</code> <pre><code>def register_operator(\n    provider: str,\n    operator_type: str,\n    operator_cls: Type[BaseOperator],\n    model: str = None,\n):\n\"\"\"\n    Registers an operator.\n    Args:\n        provider: Name of the LLM provider.\n        operator_type: The type of operator (e.g., \"chat\", \"task\").\n        operator_cls: The operator class to register.\n        model: Specific model name\n    \"\"\"\n    operator_registry.register(\n        provider=provider,\n        operator_type=operator_type,\n        model=model,\n        operator_cls=operator_cls,\n    )\n</code></pre>"},{"location":"reference/declarai/task/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> task","text":""},{"location":"reference/declarai/task/#declarai.task","title":"task","text":"<p>Task interface</p> <p>Provides the most basic component to interact with an LLM. LLMs are often interacted with via an API. In order to provide prompts and receive predictions, we will need to create the following: - parse the provided python code - Translate the parsed data into the proper prompt for the LLM - Send the request to the LLM and parse the output back into python</p> <p>This class is an orchestrator that calls a parser and operators to perform the above tasks. while the parser is meant to be shared across cases, as python code has a consistent interface, the different LLM API providers as well as custom models have different APIs with different expected prompt structures. For that reason, there are multiple implementations of operators, depending on the required use case.</p> <p>Classes:</p> Name Description <code>FutureTask</code> <p>A FutureTask is a wrapper around the task that is returned from the <code>plan</code> method.</p> <code>Task</code> <p>Initializes the Task</p> <code>TaskDecorator</code> <p>The TaskDecorator is used to create a task. It is used as a decorator on a function that will be used as a task.</p>"},{"location":"reference/declarai/task/#declarai.task.FutureTask","title":"FutureTask","text":"<pre><code>FutureTask(\n    exec_func: Callable[[], Any],\n    kwargs: Dict[str, Any],\n    compiled_template: str,\n    populated_prompt: str,\n)\n</code></pre> <p>A FutureTask is a wrapper around the task that is returned from the <code>plan</code> method. It used to create a lazy execution of the task, and to provide additional information about the task. The only functionality that is provided by the FutureTask is the <code>__call__</code> method, which executes the task.</p> <p>Parameters:</p> Name Type Description Default <code>exec_func</code> <code>Callable[[], Any]</code> <p>the function to execute when the future task is called</p> required <code>kwargs</code> <code>Dict[str, Any]</code> <p>the kwargs that were passed to the task</p> required <code>compiled_template</code> <code>str</code> <p>the compiled template that was populated by the task</p> required <code>populated_prompt</code> <code>str</code> <p>the populated prompt that was populated by the task</p> required <p>Methods:</p> Name Description <code>__call__</code> <p>executes the task</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Calls the <code>exec_func</code> attribute of the FutureTask</p> <p>Attributes:</p> Name Type Description <code>compiled_template</code> <code>str</code> <p>Returns the compiled template that was populated by the task</p> <code>populated_prompt</code> <code>str</code> <p>Returns the populated prompt that was populated by the task</p> <code>task_kwargs</code> <code>Dict[str, Any]</code> <p>Returns the kwargs that were passed to the task</p> Source code in <code>src/declarai/task.py</code> <pre><code>def __init__(\n    self,\n    exec_func: Callable[[], Any],\n    kwargs: Dict[str, Any],\n    compiled_template: str,\n    populated_prompt: str,\n):\n    self.exec_func = exec_func\n    self.__populated_prompt = populated_prompt\n    self.__compiled_template = compiled_template\n    self.__kwargs = kwargs\n</code></pre>"},{"location":"reference/declarai/task/#declarai.task.FutureTask.compiled_template","title":"compiled_template  <code>property</code>","text":"<pre><code>compiled_template: str\n</code></pre> <p>Returns the compiled template that was populated by the task</p>"},{"location":"reference/declarai/task/#declarai.task.FutureTask.populated_prompt","title":"populated_prompt  <code>property</code>","text":"<pre><code>populated_prompt: str\n</code></pre> <p>Returns the populated prompt that was populated by the task</p>"},{"location":"reference/declarai/task/#declarai.task.FutureTask.task_kwargs","title":"task_kwargs  <code>property</code>","text":"<pre><code>task_kwargs: Dict[str, Any]\n</code></pre> <p>Returns the kwargs that were passed to the task</p>"},{"location":"reference/declarai/task/#declarai.task.FutureTask.__call__","title":"__call__","text":"<pre><code>__call__() -&gt; Any\n</code></pre> <p>Calls the <code>exec_func</code> attribute of the FutureTask Returns:     the response from the <code>exec_func</code></p> Source code in <code>src/declarai/task.py</code> <pre><code>def __call__(self) -&gt; Any:\n\"\"\"\n    Calls the `exec_func` attribute of the FutureTask\n    Returns:\n        the response from the `exec_func`\n    \"\"\"\n    return self.exec_func()\n</code></pre>"},{"location":"reference/declarai/task/#declarai.task.Task","title":"Task","text":"<pre><code>Task(\n    operator: BaseOperator,\n    middlewares: List[Type[TaskMiddleware]] = None,\n)\n</code></pre> <p>             Bases: <code>BaseTask</code></p> <p>Initializes the Task Args:     operator: the operator to use to interact with the LLM     middlewares: the middlewares to use while executing the task     **kwargs:</p> <p>Attributes:</p> Name Type Description <code>operator</code> <p>the operator to use to interact with the LLM</p> <code>_call_kwargs</code> <code>Dict[str, Any]</code> <p>the kwargs that were passed to the task are set as attributes on the task and passed to the middlewares</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Orchestrates the execution of the task.</p> <code>compile</code> <p>Compiles the prompt to be sent to the LLM. This is the first step in the process of interacting with the LLM.</p> <code>plan</code> <p>Populates the compiled template with the actual data.</p> <code>stream_handler</code> <p>A generator that yields each chunk from the stream and collects them in a buffer.</p> <p>Attributes:</p> Name Type Description <code>llm_params</code> <code>LLMParamsType</code> <p>Return the LLM parameters that are saved on the operator. These parameters are sent to the LLM when the task is</p> <code>llm_response</code> <code>LLMResponse</code> <p>The response from the LLM</p> <code>llm_stream_response</code> <code>Iterator[LLMResponse]</code> <p>The response from the LLM when streaming</p> Source code in <code>src/declarai/task.py</code> <pre><code>def __init__(\n    self, operator: BaseOperator, middlewares: List[Type[TaskMiddleware]] = None\n):\n    self.middlewares = middlewares\n    self.operator = operator\n</code></pre>"},{"location":"reference/declarai/task/#declarai.task.Task.llm_params","title":"llm_params  <code>property</code>","text":"<pre><code>llm_params: LLMParamsType\n</code></pre> <p>Return the LLM parameters that are saved on the operator. These parameters are sent to the LLM when the task is executed. Returns: The LLM parameters</p>"},{"location":"reference/declarai/task/#declarai.task.Task.llm_response","title":"llm_response  <code>instance-attribute</code>","text":"<pre><code>llm_response: LLMResponse\n</code></pre> <p>The response from the LLM</p>"},{"location":"reference/declarai/task/#declarai.task.Task.llm_stream_response","title":"llm_stream_response  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>llm_stream_response: Iterator[LLMResponse] = None\n</code></pre> <p>The response from the LLM when streaming</p>"},{"location":"reference/declarai/task/#declarai.task.Task.__call__","title":"__call__","text":"<pre><code>__call__(\n    *,\n    llm_params: LLMParamsType = None,\n    **kwargs: LLMParamsType\n) -&gt; Union[Any, Iterator[LLMResponse]]\n</code></pre> <p>Orchestrates the execution of the task. Args:     llm_params: the params to pass to the LLM. If provided, they will override the params that were passed during initialization     **kwargs: kwargs that are used to compile the template and populate the prompt.</p> <p>Returns: the user defined return type of the task</p> Source code in <code>src/declarai/task.py</code> <pre><code>def __call__(\n    self, *, llm_params: LLMParamsType = None, **kwargs\n) -&gt; Union[Any, Iterator[LLMResponse]]:\n\"\"\"\n    Orchestrates the execution of the task.\n    Args:\n        llm_params: the params to pass to the LLM. If provided, they will override the params that were passed during initialization\n        **kwargs: kwargs that are used to compile the template and populate the prompt.\n\n    Returns: the user defined return type of the task\n\n    \"\"\"\n    runtime_llm_params = (\n        llm_params or self.llm_params\n    )  # order is important! We prioritize runtime params that\n    # were passed\n    if runtime_llm_params:\n        kwargs[\"llm_params\"] = runtime_llm_params\n\n    self._call_kwargs = kwargs\n    return self._exec_middlewares(kwargs)\n</code></pre>"},{"location":"reference/declarai/task/#declarai.task.Task.compile","title":"compile","text":"<pre><code>compile(**kwargs) -&gt; Any\n</code></pre> <p>Compiles the prompt to be sent to the LLM. This is the first step in the process of interacting with the LLM. Can be used for debugging purposes as well, to see what the prompt will look like before sending it to the LLM. Args:     **kwargs: the data to populate the template with</p> <p>Returns:</p> Type Description <code>Any</code> <p>the compiled template</p> Source code in <code>src/declarai/task.py</code> <pre><code>def compile(self, **kwargs) -&gt; Any:\n\"\"\"\n    Compiles the prompt to be sent to the LLM. This is the first step in the process of interacting with the LLM.\n    Can be used for debugging purposes as well, to see what the prompt will look like before sending it to the LLM.\n    Args:\n        **kwargs: the data to populate the template with\n\n    Returns:\n         the compiled template\n\n    \"\"\"\n    return self.operator.compile(**kwargs)\n</code></pre>"},{"location":"reference/declarai/task/#declarai.task.Task.plan","title":"plan","text":"<pre><code>plan(**kwargs) -&gt; FutureTask\n</code></pre> <p>Populates the compiled template with the actual data. Args:     **kwargs: the data to populate the template with Returns:      a FutureTask that can be used to execute the task in a lazy manner</p> Source code in <code>src/declarai/task.py</code> <pre><code>def plan(self, **kwargs) -&gt; FutureTask:\n\"\"\"\n    Populates the compiled template with the actual data.\n    Args:\n        **kwargs: the data to populate the template with\n    Returns:\n         a FutureTask that can be used to execute the task in a lazy manner\n    \"\"\"\n\n    populated_prompt = self.compile(**kwargs)\n    return FutureTask(\n        self.__call__,\n        kwargs=kwargs,\n        compiled_template=self.compile(),\n        populated_prompt=populated_prompt,\n    )\n</code></pre>"},{"location":"reference/declarai/task/#declarai.task.Task.stream_handler","title":"stream_handler","text":"<pre><code>stream_handler(\n    stream: Iterator[LLMResponse],\n) -&gt; Iterator[LLMResponse]\n</code></pre> <p>A generator that yields each chunk from the stream and collects them in a buffer. After the stream is exhausted, it runs the cleanup logic.</p> Source code in <code>src/declarai/_base.py</code> <pre><code>def stream_handler(self, stream: Iterator[LLMResponse]) -&gt; Iterator[LLMResponse]:\n\"\"\"\n    A generator that yields each chunk from the stream and collects them in a buffer.\n    After the stream is exhausted, it runs the cleanup logic.\n    \"\"\"\n    response_buffer = []\n    for chunk in stream:\n        response_buffer.append(chunk)\n        yield chunk\n\n    # After the stream is exhausted, run the cleanup logic\n    self.stream_cleanup(response_buffer[-1])\n</code></pre>"},{"location":"reference/declarai/task/#declarai.task.TaskDecorator","title":"TaskDecorator","text":"<pre><code>TaskDecorator(llm: LLM)\n</code></pre> <p>The TaskDecorator is used to create a task. It is used as a decorator on a function that will be used as a task. Args:     llm_settings: the settings that define which LLM to use     **kwargs: additional llm_settings like open_ai_api_key etc. Methods:     task: the decorator that creates the task</p> <p>Methods:</p> Name Description <code>task</code> <p>The decorator that creates the task</p> Source code in <code>src/declarai/task.py</code> <pre><code>def __init__(self, llm: LLM):\n    self.llm = llm\n</code></pre>"},{"location":"reference/declarai/task/#declarai.task.TaskDecorator.task","title":"task","text":"<pre><code>task(\n    func: Optional[Callable] = None,\n    *,\n    middlewares: List[Type[TaskMiddleware]] = None,\n    llm_params: LLMParamsType = None,\n    streaming: bool = None\n)\n</code></pre> <p>The decorator that creates the task Args:     func: the function to decorate that represents the task     middlewares: middleware to use while executing the task     llm_params: llm_params to use when calling the llm     streaming: whether to stream the response from the llm or not</p> <p>Returns:</p> Type Description <code>Task</code> <p>the task that was created</p> Source code in <code>src/declarai/task.py</code> <pre><code>def task(\n    self,\n    func: Optional[Callable] = None,\n    *,\n    middlewares: List[Type[TaskMiddleware]] = None,\n    llm_params: LLMParamsType = None,\n    streaming: bool = None,\n):\n\"\"\"\n    The decorator that creates the task\n    Args:\n        func: the function to decorate that represents the task\n        middlewares: middleware to use while executing the task\n        llm_params: llm_params to use when calling the llm\n        streaming: whether to stream the response from the llm or not\n\n    Returns:\n        (Task): the task that was created\n\n    \"\"\"\n    operator_type = resolve_operator(self.llm, operator_type=\"task\")\n\n    def wrap(_func: Callable) -&gt; Task:\n        operator = operator_type(\n            parsed=PythonParser(_func),\n            llm=self.llm,\n            llm_params=llm_params,\n            streaming=streaming,\n        )\n        llm_task = Task(operator=operator, middlewares=middlewares)\n        llm_task.__name__ = _func.__name__\n        return llm_task\n\n    if func is None:\n        return wrap\n\n    return wrap(func)\n</code></pre>"},{"location":"reference/declarai/core/","title":"Index","text":""},{"location":"reference/declarai/core/#declarai.core","title":"core","text":"<p>Core settings for Declarai.</p> <p>Modules:</p> Name Description <code>core_settings</code> <p>This module contains the core settings for the declarai project.</p>"},{"location":"reference/declarai/core/core_settings/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> core_settings","text":""},{"location":"reference/declarai/core/core_settings/#declarai.core.core_settings","title":"core_settings","text":"<p>This module contains the core settings for the declarai project. In order to create proper separation from existing code on the client's environment, we require all environment variables used by <code>declarai</code> be prefixed with <code>DECLARAI_</code>. This way we do not interfere with any existing environment variables.</p>"},{"location":"reference/declarai/memory/","title":"Index","text":""},{"location":"reference/declarai/memory/#declarai.memory","title":"memory","text":"<p>Memory module for Declarai interactions that includes message history.</p> <p>Modules:</p> Name Description <code>base</code> <p>Base class for the memory module.</p> <code>file</code> <p>This module contains the FileMessageHistory class, which is used to store chat message history in a local file.</p> <code>in_memory</code> <p>This module contains the in-memory implementation of the chat message history.</p> <code>mongodb</code> <p>This module contains the MongoDBMessageHistory class, which is used to store chat message history in a MongoDB database.</p> <code>postgres</code> <p>This module contains the PostgresMessageHistory class, which is used to store chat message history in a PostgreSQL database.</p> <code>redis</code> <p>This module contains the RedisMessageHistory class, which is used to store chat message history in a Redis database.</p>"},{"location":"reference/declarai/memory/base/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> base","text":""},{"location":"reference/declarai/memory/base/#declarai.memory.base","title":"base","text":"<p>Base class for the memory module.</p> <p>Classes:</p> Name Description <code>BaseChatMessageHistory</code> <p>Abstract class to store the chat message history.</p>"},{"location":"reference/declarai/memory/base/#declarai.memory.base.BaseChatMessageHistory","title":"BaseChatMessageHistory","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract class to store the chat message history.</p> <p>See <code>ChatMessageHistory</code> for default implementation.</p> <p>Methods:</p> Name Description <code>add_message</code> <p>Add a Message object to the state.</p> <code>clear</code> <p>Remove all messages from the state</p> <p>Attributes:</p> Name Type Description <code>history</code> <code>List[Message]</code> <p>Return the chat message history</p>"},{"location":"reference/declarai/memory/base/#declarai.memory.base.BaseChatMessageHistory.history","title":"history  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>history: List[Message]\n</code></pre> <p>Return the chat message history</p> <p>Returns:</p> Type Description <code>List[Message]</code> <p>List of Message objects</p>"},{"location":"reference/declarai/memory/base/#declarai.memory.base.BaseChatMessageHistory.add_message","title":"add_message  <code>abstractmethod</code>","text":"<pre><code>add_message(message: Message) -&gt; None\n</code></pre> <p>Add a Message object to the state.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>Message object to add to the state</p> required Source code in <code>src/declarai/memory/base.py</code> <pre><code>@abstractmethod\ndef add_message(self, message: Message) -&gt; None:\n\"\"\"\n    Add a Message object to the state.\n\n    Args:\n        message: Message object to add to the state\n    \"\"\"\n</code></pre>"},{"location":"reference/declarai/memory/base/#declarai.memory.base.BaseChatMessageHistory.clear","title":"clear  <code>abstractmethod</code>","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Remove all messages from the state</p> Source code in <code>src/declarai/memory/base.py</code> <pre><code>@abstractmethod\ndef clear(self) -&gt; None:\n\"\"\"\n    Remove all messages from the state\n    \"\"\"\n</code></pre>"},{"location":"reference/declarai/memory/file/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> file","text":""},{"location":"reference/declarai/memory/file/#declarai.memory.file","title":"file","text":"<p>This module contains the FileMessageHistory class, which is used to store chat message history in a local file.</p> <p>Classes:</p> Name Description <code>FileMessageHistory</code> <p>Chat message history that stores history in a local file.</p>"},{"location":"reference/declarai/memory/file/#declarai.memory.file.FileMessageHistory","title":"FileMessageHistory","text":"<pre><code>FileMessageHistory(file_path: Optional[str] = None)\n</code></pre> <p>             Bases: <code>BaseChatMessageHistory</code></p> <p>Chat message history that stores history in a local file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Optional[str]</code> <p>path of the local file to store the messages. if not passed the messages will be stored in a temporary file, and a warning will be logged.</p> <code>None</code> <p>Methods:</p> Name Description <code>add_message</code> <p>Append the message to the record in the local file</p> <code>clear</code> <p>Clear session memory from the local file</p> <p>Attributes:</p> Name Type Description <code>history</code> <code>List[Message]</code> <p>Retrieve the messages from the local file</p> Source code in <code>src/declarai/memory/file.py</code> <pre><code>def __init__(self, file_path: Optional[str] = None):\n    super().__init__()\n    if not file_path:\n        # Create a temporary file and immediately close it to get its name.\n        temp = tempfile.NamedTemporaryFile(delete=False)\n        self.file_path = Path(temp.name)\n        self.file_path.write_text(json.dumps([]))\n        logger.warning(\n            \"No file path provided to store the messages. \"\n            f\"Messages will be stored in a temporary file path: {self.file_path}\"\n        )\n    else:\n        self.file_path = Path(file_path)\n\n    if not self.file_path.exists():\n        self.file_path.touch()\n        self.file_path.write_text(json.dumps([]))\n</code></pre>"},{"location":"reference/declarai/memory/file/#declarai.memory.file.FileMessageHistory.history","title":"history  <code>property</code>","text":"<pre><code>history: List[Message]\n</code></pre> <p>Retrieve the messages from the local file</p>"},{"location":"reference/declarai/memory/file/#declarai.memory.file.FileMessageHistory.add_message","title":"add_message","text":"<pre><code>add_message(message: Message) -&gt; None\n</code></pre> <p>Append the message to the record in the local file</p> Source code in <code>src/declarai/memory/file.py</code> <pre><code>def add_message(self, message: Message) -&gt; None:\n\"\"\"Append the message to the record in the local file\"\"\"\n    messages = self.history.copy()\n    messages.append(message)\n    messages_dict = [msg.dict() for msg in messages]\n    self.file_path.write_text(json.dumps(messages_dict))\n</code></pre>"},{"location":"reference/declarai/memory/file/#declarai.memory.file.FileMessageHistory.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clear session memory from the local file</p> Source code in <code>src/declarai/memory/file.py</code> <pre><code>def clear(self) -&gt; None:\n\"\"\"Clear session memory from the local file\"\"\"\n    self.file_path.write_text(json.dumps([]))\n</code></pre>"},{"location":"reference/declarai/memory/in_memory/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> in_memory","text":""},{"location":"reference/declarai/memory/in_memory/#declarai.memory.in_memory","title":"in_memory","text":"<p>This module contains the in-memory implementation of the chat message history.</p> <p>Classes:</p> Name Description <code>InMemoryMessageHistory</code> <p>This memory implementation stores all messages in memory in a list.</p>"},{"location":"reference/declarai/memory/in_memory/#declarai.memory.in_memory.InMemoryMessageHistory","title":"InMemoryMessageHistory","text":"<p>             Bases: <code>BaseChatMessageHistory</code>, <code>BaseModel</code></p> <p>This memory implementation stores all messages in memory in a list.</p> <p>Methods:</p> Name Description <code>add_message</code> <p>Adds a message to the list of messages stored in memory.</p> <p>Attributes:</p> Name Type Description <code>history</code> <code>List[Message]</code> <p>Returns the list of messages stored in memory.</p>"},{"location":"reference/declarai/memory/in_memory/#declarai.memory.in_memory.InMemoryMessageHistory.history","title":"history  <code>property</code>","text":"<pre><code>history: List[Message]\n</code></pre> <p>Returns the list of messages stored in memory. :return: List of messages</p>"},{"location":"reference/declarai/memory/in_memory/#declarai.memory.in_memory.InMemoryMessageHistory.add_message","title":"add_message","text":"<pre><code>add_message(message: Message) -&gt; None\n</code></pre> <p>Adds a message to the list of messages stored in memory. :param message: the message content and role</p> Source code in <code>src/declarai/memory/in_memory.py</code> <pre><code>def add_message(self, message: Message) -&gt; None:\n\"\"\"\n    Adds a message to the list of messages stored in memory.\n    :param message: the message content and role\n    \"\"\"\n    self.messages.append(message)\n</code></pre>"},{"location":"reference/declarai/memory/mongodb/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> mongodb","text":""},{"location":"reference/declarai/memory/mongodb/#declarai.memory.mongodb","title":"mongodb","text":"<p>This module contains the MongoDBMessageHistory class, which is used to store chat message history in a MongoDB database.</p> <p>Classes:</p> Name Description <code>MongoDBMessageHistory</code> <p>Chat message history that stores history in MongoDB.</p> <p>Attributes:</p> Name Type Description <code>DEFAULT_COLLECTION_NAME</code> <p>A collection name for the MongoDB database.</p> <code>DEFAULT_CONNECTION_STRING</code> <p>A connection string for a MongoDB database.</p> <code>DEFAULT_DBNAME</code> <p>A database name for the MongoDB database.</p>"},{"location":"reference/declarai/memory/mongodb/#declarai.memory.mongodb.DEFAULT_COLLECTION_NAME","title":"DEFAULT_COLLECTION_NAME  <code>module-attribute</code>","text":"<pre><code>DEFAULT_COLLECTION_NAME = 'message_store'\n</code></pre> <p>A collection name for the MongoDB database.</p>"},{"location":"reference/declarai/memory/mongodb/#declarai.memory.mongodb.DEFAULT_CONNECTION_STRING","title":"DEFAULT_CONNECTION_STRING  <code>module-attribute</code>","text":"<pre><code>DEFAULT_CONNECTION_STRING = 'mongodb://localhost:27017'\n</code></pre> <p>A connection string for a MongoDB database.</p>"},{"location":"reference/declarai/memory/mongodb/#declarai.memory.mongodb.DEFAULT_DBNAME","title":"DEFAULT_DBNAME  <code>module-attribute</code>","text":"<pre><code>DEFAULT_DBNAME = 'chat_history'\n</code></pre> <p>A database name for the MongoDB database.</p>"},{"location":"reference/declarai/memory/mongodb/#declarai.memory.mongodb.MongoDBMessageHistory","title":"MongoDBMessageHistory","text":"<pre><code>MongoDBMessageHistory(\n    session_id: str,\n    connection_string: str = DEFAULT_CONNECTION_STRING,\n    database_name: str = DEFAULT_DBNAME,\n    collection_name: str = DEFAULT_COLLECTION_NAME,\n)\n</code></pre> <p>             Bases: <code>BaseChatMessageHistory</code></p> <p>Chat message history that stores history in MongoDB.</p> <p>Parameters:</p> Name Type Description Default <code>connection_string</code> <code>str</code> <p>connection string to connect to MongoDB</p> <code>DEFAULT_CONNECTION_STRING</code> <code>session_id</code> <code>str</code> <p>Arbitrary key that is used to store the messages for a single chat session.</p> required <code>database_name</code> <code>str</code> <p>name of the database to use</p> <code>DEFAULT_DBNAME</code> <code>collection_name</code> <code>str</code> <p>name of the collection to use</p> <code>DEFAULT_COLLECTION_NAME</code> <p>Methods:</p> Name Description <code>add_message</code> <p>Append the message to the record in MongoDB</p> <code>clear</code> <p>Clear session memory from MongoDB</p> <p>Attributes:</p> Name Type Description <code>history</code> <code>List[Message]</code> <p>Retrieve the messages from MongoDB</p> Source code in <code>src/declarai/memory/mongodb.py</code> <pre><code>def __init__(\n    self,\n    session_id: str,\n    connection_string: str = DEFAULT_CONNECTION_STRING,\n    database_name: str = DEFAULT_DBNAME,\n    collection_name: str = DEFAULT_COLLECTION_NAME,\n):\n    try:\n        from pymongo import MongoClient\n    except ImportError:\n        raise ImportError(\n            \"Could not import pymongo python package. \"\n            \"Please install it with `pip install pymongo`.\"\n        )\n\n    self.connection_string = connection_string\n    self.session_id = session_id\n    self.database_name = database_name\n    self.collection_name = collection_name\n\n    self.client: MongoClient = MongoClient(connection_string)\n    self.db = self.client[database_name]\n    self.collection = self.db[collection_name]\n    self.collection.create_index(\"SessionId\")\n</code></pre>"},{"location":"reference/declarai/memory/mongodb/#declarai.memory.mongodb.MongoDBMessageHistory.history","title":"history  <code>property</code>","text":"<pre><code>history: List[Message]\n</code></pre> <p>Retrieve the messages from MongoDB</p>"},{"location":"reference/declarai/memory/mongodb/#declarai.memory.mongodb.MongoDBMessageHistory.add_message","title":"add_message","text":"<pre><code>add_message(message: Message) -&gt; None\n</code></pre> <p>Append the message to the record in MongoDB</p> Source code in <code>src/declarai/memory/mongodb.py</code> <pre><code>def add_message(self, message: Message) -&gt; None:\n\"\"\"Append the message to the record in MongoDB\"\"\"\n    self.collection.insert_one(\n        {\n            \"SessionId\": self.session_id,\n            \"History\": json.dumps(message.dict()),\n        }\n    )\n</code></pre>"},{"location":"reference/declarai/memory/mongodb/#declarai.memory.mongodb.MongoDBMessageHistory.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clear session memory from MongoDB</p> Source code in <code>src/declarai/memory/mongodb.py</code> <pre><code>def clear(self) -&gt; None:\n\"\"\"Clear session memory from MongoDB\"\"\"\n    self.collection.delete_many({\"SessionId\": self.session_id})\n</code></pre>"},{"location":"reference/declarai/memory/postgres/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> postgres","text":""},{"location":"reference/declarai/memory/postgres/#declarai.memory.postgres","title":"postgres","text":"<p>This module contains the PostgresMessageHistory class, which is used to store chat message history in a PostgreSQL database.</p> <p>Classes:</p> Name Description <code>PostgresMessageHistory</code> <p>Chat message history that stores history in a PostgreSQL database.</p> <p>Attributes:</p> Name Type Description <code>DEFAULT_CONNECTION_STRING</code> <p>A connection string for a PostgreSQL database.</p> <code>DEFAULT_TABLE_NAME</code> <p>A table name for the PostgreSQL database.</p>"},{"location":"reference/declarai/memory/postgres/#declarai.memory.postgres.DEFAULT_CONNECTION_STRING","title":"DEFAULT_CONNECTION_STRING  <code>module-attribute</code>","text":"<pre><code>DEFAULT_CONNECTION_STRING = (\n    \"postgresql://postgres:postgres@localhost:5432/postgres\"\n)\n</code></pre> <p>A connection string for a PostgreSQL database.</p>"},{"location":"reference/declarai/memory/postgres/#declarai.memory.postgres.DEFAULT_TABLE_NAME","title":"DEFAULT_TABLE_NAME  <code>module-attribute</code>","text":"<pre><code>DEFAULT_TABLE_NAME = 'message_store'\n</code></pre> <p>A table name for the PostgreSQL database.</p>"},{"location":"reference/declarai/memory/postgres/#declarai.memory.postgres.PostgresMessageHistory","title":"PostgresMessageHistory","text":"<pre><code>PostgresMessageHistory(\n    session_id: str,\n    connection_string: Optional[\n        str\n    ] = DEFAULT_CONNECTION_STRING,\n    table_name: str = DEFAULT_TABLE_NAME,\n)\n</code></pre> <p>             Bases: <code>BaseChatMessageHistory</code></p> <p>Chat message history that stores history in a PostgreSQL database.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>Arbitrary key that is used to store the messages for a single chat session.</p> required <code>connection_string</code> <code>Optional[str]</code> <p>Database connection string.</p> <code>DEFAULT_CONNECTION_STRING</code> <code>table_name</code> <code>str</code> <p>Name of the table to use.</p> <code>DEFAULT_TABLE_NAME</code> <p>Methods:</p> Name Description <code>__del__</code> <p>Destructor to close cursor and connection.</p> <code>_initialize_tables</code> <p>Initialize the tables if they don't exist.</p> <code>add_message</code> <p>Add a message to the database.</p> <code>clear</code> <p>Clear session memory from the database.</p> <code>close</code> <p>Close cursor and connection.</p> <p>Attributes:</p> Name Type Description <code>history</code> <code>List[Message]</code> <p>Retrieve the messages from the database.</p> Source code in <code>src/declarai/memory/postgres.py</code> <pre><code>def __init__(\n    self,\n    session_id: str,\n    connection_string: Optional[str] = DEFAULT_CONNECTION_STRING,\n    table_name: str = DEFAULT_TABLE_NAME,\n):\n    try:\n        import psycopg2  # pylint: disable=import-outside-toplevel\n    except ImportError:\n        raise ImportError(\n            \"Cannot import psycopg2.\"\n            \"Please install psycopg2 to use PostgresMessageHistory.\"\n        )\n    self.conn = psycopg2.connect(connection_string)\n    self.cursor = self.conn.cursor()\n    self.table_name = table_name\n    self.session_id = session_id\n    self._initialize_tables()\n</code></pre>"},{"location":"reference/declarai/memory/postgres/#declarai.memory.postgres.PostgresMessageHistory.history","title":"history  <code>property</code>","text":"<pre><code>history: List[Message]\n</code></pre> <p>Retrieve the messages from the database.</p>"},{"location":"reference/declarai/memory/postgres/#declarai.memory.postgres.PostgresMessageHistory.__del__","title":"__del__","text":"<pre><code>__del__()\n</code></pre> <p>Destructor to close cursor and connection.</p> Source code in <code>src/declarai/memory/postgres.py</code> <pre><code>def __del__(self):\n\"\"\"Destructor to close cursor and connection.\"\"\"\n    if hasattr(self, \"cursor\"):\n        self.cursor.close()\n    if hasattr(self, \"conn\"):\n        self.conn.close()\n</code></pre>"},{"location":"reference/declarai/memory/postgres/#declarai.memory.postgres.PostgresMessageHistory._initialize_tables","title":"_initialize_tables","text":"<pre><code>_initialize_tables()\n</code></pre> <p>Initialize the tables if they don't exist.</p> Source code in <code>src/declarai/memory/postgres.py</code> <pre><code>def _initialize_tables(self):\n\"\"\"Initialize the tables if they don't exist.\"\"\"\n    create_table_query = f\"\"\"CREATE TABLE IF NOT EXISTS {self.table_name} (\n        id SERIAL PRIMARY KEY,\n        session_id TEXT NOT NULL,\n        message JSONB NOT NULL\n    );\"\"\"\n    self.cursor.execute(create_table_query)\n    self.conn.commit()\n</code></pre>"},{"location":"reference/declarai/memory/postgres/#declarai.memory.postgres.PostgresMessageHistory.add_message","title":"add_message","text":"<pre><code>add_message(message: Message) -&gt; None\n</code></pre> <p>Add a message to the database.</p> Source code in <code>src/declarai/memory/postgres.py</code> <pre><code>def add_message(self, message: Message) -&gt; None:\n\"\"\"Add a message to the database.\"\"\"\n    from psycopg2 import sql\n\n    query = sql.SQL(\"INSERT INTO {} (session_id, message) VALUES (%s, %s);\").format(\n        sql.Identifier(self.table_name)\n    )\n    self.cursor.execute(query, (self.session_id, json.dumps(message.dict())))\n    self.conn.commit()\n</code></pre>"},{"location":"reference/declarai/memory/postgres/#declarai.memory.postgres.PostgresMessageHistory.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clear session memory from the database.</p> Source code in <code>src/declarai/memory/postgres.py</code> <pre><code>def clear(self) -&gt; None:\n\"\"\"Clear session memory from the database.\"\"\"\n    query = f\"DELETE FROM {self.table_name} WHERE session_id = %s;\"\n    self.cursor.execute(query, (self.session_id,))\n    self.conn.commit()\n</code></pre>"},{"location":"reference/declarai/memory/postgres/#declarai.memory.postgres.PostgresMessageHistory.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Close cursor and connection.</p> Source code in <code>src/declarai/memory/postgres.py</code> <pre><code>def close(self):\n\"\"\"Close cursor and connection.\"\"\"\n    self.cursor.close()\n    self.conn.close()\n</code></pre>"},{"location":"reference/declarai/memory/redis/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> redis","text":""},{"location":"reference/declarai/memory/redis/#declarai.memory.redis","title":"redis","text":"<p>This module contains the RedisMessageHistory class, which is used to store chat message history in a Redis database.</p> <p>Classes:</p> Name Description <code>RedisMessageHistory</code> <p>Chat message history that stores history in a Redis database.</p> <p>Attributes:</p> Name Type Description <code>DEFAULT_TABLE_NAME</code> <p>A table name for the Redis database.</p> <code>DEFAULT_URL</code> <p>A URL for the Redis database.</p>"},{"location":"reference/declarai/memory/redis/#declarai.memory.redis.DEFAULT_TABLE_NAME","title":"DEFAULT_TABLE_NAME  <code>module-attribute</code>","text":"<pre><code>DEFAULT_TABLE_NAME = 'message_store'\n</code></pre> <p>A table name for the Redis database.</p>"},{"location":"reference/declarai/memory/redis/#declarai.memory.redis.DEFAULT_URL","title":"DEFAULT_URL  <code>module-attribute</code>","text":"<pre><code>DEFAULT_URL = 'redis://localhost:6379/0'\n</code></pre> <p>A URL for the Redis database.</p>"},{"location":"reference/declarai/memory/redis/#declarai.memory.redis.RedisMessageHistory","title":"RedisMessageHistory","text":"<pre><code>RedisMessageHistory(\n    session_id: str,\n    url: str = DEFAULT_URL,\n    key_prefix: str = f\"{DEFAULT_TABLE_NAME}:\",\n    ttl: Optional[int] = None,\n)\n</code></pre> <p>             Bases: <code>BaseChatMessageHistory</code></p> <p>Chat message history that stores history in a Redis database.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>Arbitrary key that is used to store the messages for a single chat session.</p> required <code>url</code> <code>str</code> <p>URL to connect to the Redis server.</p> <code>DEFAULT_URL</code> <code>key_prefix</code> <code>str</code> <p>Prefix for the Redis key.</p> <code>f'{DEFAULT_TABLE_NAME}:'</code> <code>ttl</code> <code>Optional[int]</code> <p>Time-to-live for the message records.</p> <code>None</code> <p>Methods:</p> Name Description <code>add_message</code> <p>Append the message to the record in Redis</p> <code>clear</code> <p>Clear session memory from Redis</p> <p>Attributes:</p> Name Type Description <code>history</code> <code>List[Message]</code> <p>Retrieve the messages from Redis</p> <code>key</code> <code>str</code> <p>Construct the record key to use</p> Source code in <code>src/declarai/memory/redis.py</code> <pre><code>def __init__(\n    self,\n    session_id: str,\n    url: str = DEFAULT_URL,\n    key_prefix: str = f\"{DEFAULT_TABLE_NAME}:\",\n    ttl: Optional[int] = None,\n):\n    super().__init__()\n    try:\n        import redis  # pylint: disable=import-outside-toplevel\n    except ImportError:\n        raise ImportError(\n            \"Could not import redis python package. \"\n            \"Please install it with `pip install redis`.\"\n        )\n\n    self.redis_client = redis.StrictRedis.from_url(url)\n    self.session_id = session_id\n    self.key_prefix = key_prefix\n    self.ttl = ttl\n</code></pre>"},{"location":"reference/declarai/memory/redis/#declarai.memory.redis.RedisMessageHistory.history","title":"history  <code>property</code>","text":"<pre><code>history: List[Message]\n</code></pre> <p>Retrieve the messages from Redis</p>"},{"location":"reference/declarai/memory/redis/#declarai.memory.redis.RedisMessageHistory.key","title":"key  <code>property</code>","text":"<pre><code>key: str\n</code></pre> <p>Construct the record key to use</p>"},{"location":"reference/declarai/memory/redis/#declarai.memory.redis.RedisMessageHistory.add_message","title":"add_message","text":"<pre><code>add_message(message: Message) -&gt; None\n</code></pre> <p>Append the message to the record in Redis</p> Source code in <code>src/declarai/memory/redis.py</code> <pre><code>def add_message(self, message: Message) -&gt; None:\n\"\"\"Append the message to the record in Redis\"\"\"\n    self.redis_client.lpush(self.key, json.dumps(message.dict()))\n    if self.ttl:\n        self.redis_client.expire(self.key, self.ttl)\n</code></pre>"},{"location":"reference/declarai/memory/redis/#declarai.memory.redis.RedisMessageHistory.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clear session memory from Redis</p> Source code in <code>src/declarai/memory/redis.py</code> <pre><code>def clear(self) -&gt; None:\n\"\"\"Clear session memory from Redis\"\"\"\n    self.redis_client.delete(self.key)\n</code></pre>"},{"location":"reference/declarai/middleware/","title":"Index","text":""},{"location":"reference/declarai/middleware/#declarai.middleware","title":"middleware","text":"<p>Middleware module for Declarai.</p> <p>Middlewares are used to extend the functionality of the Declarai execution flow.</p> <p>Modules:</p> Name Description <code>base</code> <p>Base class for task middlewares.</p> <code>internal</code> <p>Middlewares offered by Declarai that does not require external dependencies.</p> <code>third_party</code> <p>Middlewares offered that requires external dependencies.</p>"},{"location":"reference/declarai/middleware/base/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> base","text":""},{"location":"reference/declarai/middleware/base/#declarai.middleware.base","title":"base","text":"<p>Base class for task middlewares.</p> <p>Classes:</p> Name Description <code>TaskMiddleware</code> <p>Base class for task middlewares. Middlewares are used to wrap a task and perform some actions before and after the task is executed.</p>"},{"location":"reference/declarai/middleware/base/#declarai.middleware.base.TaskMiddleware","title":"TaskMiddleware","text":"<pre><code>TaskMiddleware(\n    task: TaskType, kwargs: Dict[str, Any] = None\n)\n</code></pre> <p>Base class for task middlewares. Middlewares are used to wrap a task and perform some actions before and after the task is executed. Is mainly used for logging, but can be used for other purposes as well. Please see <code>LoggingMiddleware</code> for an example of a middleware. Args:     task: The task to wrap     kwargs: The keyword arguments to pass to the task Attributes:     _task: The task to wrap     _kwargs: The keyword arguments to pass to the task</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Once the middleware is called, it executes the task and returns the result.</p> <code>_stream</code> <p>Re-streams the streaming response while adding the after sideeffects execution to the generator</p> <code>after</code> <p>Executed after the task is executed. Should be used to perform some actions after the task is executed.</p> <code>before</code> <p>Executed before the task is executed. Should be used to perform some actions before the task is executed.</p> Source code in <code>src/declarai/middleware/base.py</code> <pre><code>def __init__(self, task: TaskType, kwargs: Dict[str, Any] = None):\n    self._task = task\n    self._kwargs = kwargs\n</code></pre>"},{"location":"reference/declarai/middleware/base/#declarai.middleware.base.TaskMiddleware.__call__","title":"__call__","text":"<pre><code>__call__() -&gt; Any\n</code></pre> <p>Once the middleware is called, it executes the task and returns the result. Before it executes the task, it calls the <code>before</code> method, and after it executes the task, it calls the <code>after</code> method. Returns:     The result of the task</p> Source code in <code>src/declarai/middleware/base.py</code> <pre><code>def __call__(self) -&gt; Any:\n\"\"\"\n    Once the middleware is called, it executes the task and returns the result.\n    Before it executes the task, it calls the `before` method, and after it executes the task, it calls the `after` method.\n    Returns:\n        The result of the task\n    \"\"\"\n    self.before(self._task)\n    # # If the task is streaming, handle it differently\n    if self._task.operator.streaming:\n        # Yield chunks from the task, then call the after method\n        return self._stream()\n    else:\n        # Non-streaming tasks can be handled as before\n        res = self._task._exec(self._kwargs)\n        self.after(self._task)\n        return res\n</code></pre>"},{"location":"reference/declarai/middleware/base/#declarai.middleware.base.TaskMiddleware._stream","title":"_stream","text":"<pre><code>_stream() -&gt; Iterator\n</code></pre> <p>Re-streams the streaming response while adding the after sideeffects execution to the generator Returns:</p> Source code in <code>src/declarai/middleware/base.py</code> <pre><code>def _stream(self) -&gt; Iterator:\n\"\"\"\n    Re-streams the streaming response while adding the after sideeffects execution to the generator\n    Returns:\n\n    \"\"\"\n    for chunk in self._task._exec(self._kwargs):\n        yield chunk\n    self.after(self._task)\n</code></pre>"},{"location":"reference/declarai/middleware/base/#declarai.middleware.base.TaskMiddleware.after","title":"after  <code>abstractmethod</code>","text":"<pre><code>after(task: TaskType) -&gt; None\n</code></pre> <p>Executed after the task is executed. Should be used to perform some actions after the task is executed. Args:     task: the task to execute</p> Source code in <code>src/declarai/middleware/base.py</code> <pre><code>@abstractmethod\ndef after(self, task: TaskType) -&gt; None:\n\"\"\"\n    Executed after the task is executed. Should be used to perform some actions after the task is executed.\n    Args:\n        task: the task to execute\n    \"\"\"\n</code></pre>"},{"location":"reference/declarai/middleware/base/#declarai.middleware.base.TaskMiddleware.before","title":"before  <code>abstractmethod</code>","text":"<pre><code>before(task: TaskType) -&gt; None\n</code></pre> <p>Executed before the task is executed. Should be used to perform some actions before the task is executed. Args:     task: the task to execute</p> Source code in <code>src/declarai/middleware/base.py</code> <pre><code>@abstractmethod\ndef before(self, task: TaskType) -&gt; None:\n\"\"\"\n    Executed before the task is executed. Should be used to perform some actions before the task is executed.\n    Args:\n        task: the task to execute\n    \"\"\"\n</code></pre>"},{"location":"reference/declarai/middleware/internal/","title":"Index","text":""},{"location":"reference/declarai/middleware/internal/#declarai.middleware.internal","title":"internal","text":"<p>Middlewares offered by Declarai that does not require external dependencies.</p> <p>Modules:</p> Name Description <code>log_middleware</code> <p>Logger Middleware</p>"},{"location":"reference/declarai/middleware/internal/log_middleware/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> log_middleware","text":""},{"location":"reference/declarai/middleware/internal/log_middleware/#declarai.middleware.internal.log_middleware","title":"log_middleware","text":"<p>Logger Middleware</p> <p>Classes:</p> Name Description <code>LoggingMiddleware</code> <p>Creates a Simple logging middleware for a given task.</p>"},{"location":"reference/declarai/middleware/internal/log_middleware/#declarai.middleware.internal.log_middleware.LoggingMiddleware","title":"LoggingMiddleware","text":"<p>             Bases: <code>TaskMiddleware</code></p> <p>Creates a Simple logging middleware for a given task.</p> Example <pre><code>@openai.task(middlewares=[LoggingMiddleware])\ndef generate_a_poem(title: str):\n'''\n    Generate a poem based on the given title\n    :return: The generated poem\n    '''\n    return declarai.magic(\"poem\", title)\n</code></pre> <p>Methods:</p> Name Description <code>__call__</code> <p>Once the middleware is called, it executes the task and returns the result.</p> <code>_stream</code> <p>Re-streams the streaming response while adding the after sideeffects execution to the generator</p> <code>after</code> <p>After execution of the task, log the task details.</p> <code>before</code> <p>Before execution of the task, set the start time.</p>"},{"location":"reference/declarai/middleware/internal/log_middleware/#declarai.middleware.internal.log_middleware.LoggingMiddleware.__call__","title":"__call__","text":"<pre><code>__call__() -&gt; Any\n</code></pre> <p>Once the middleware is called, it executes the task and returns the result. Before it executes the task, it calls the <code>before</code> method, and after it executes the task, it calls the <code>after</code> method. Returns:     The result of the task</p> Source code in <code>src/declarai/middleware/base.py</code> <pre><code>def __call__(self) -&gt; Any:\n\"\"\"\n    Once the middleware is called, it executes the task and returns the result.\n    Before it executes the task, it calls the `before` method, and after it executes the task, it calls the `after` method.\n    Returns:\n        The result of the task\n    \"\"\"\n    self.before(self._task)\n    # # If the task is streaming, handle it differently\n    if self._task.operator.streaming:\n        # Yield chunks from the task, then call the after method\n        return self._stream()\n    else:\n        # Non-streaming tasks can be handled as before\n        res = self._task._exec(self._kwargs)\n        self.after(self._task)\n        return res\n</code></pre>"},{"location":"reference/declarai/middleware/internal/log_middleware/#declarai.middleware.internal.log_middleware.LoggingMiddleware._stream","title":"_stream","text":"<pre><code>_stream() -&gt; Iterator\n</code></pre> <p>Re-streams the streaming response while adding the after sideeffects execution to the generator Returns:</p> Source code in <code>src/declarai/middleware/base.py</code> <pre><code>def _stream(self) -&gt; Iterator:\n\"\"\"\n    Re-streams the streaming response while adding the after sideeffects execution to the generator\n    Returns:\n\n    \"\"\"\n    for chunk in self._task._exec(self._kwargs):\n        yield chunk\n    self.after(self._task)\n</code></pre>"},{"location":"reference/declarai/middleware/internal/log_middleware/#declarai.middleware.internal.log_middleware.LoggingMiddleware.after","title":"after","text":"<pre><code>after(task: TaskType)\n</code></pre> <p>After execution of the task, log the task details. Args:     task: the task to be logged</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>the task details like execution time, task name, template, compiled template, result, time.</p> Source code in <code>src/declarai/middleware/internal/log_middleware.py</code> <pre><code>def after(self, task: TaskType):\n\"\"\"\n    After execution of the task, log the task details.\n    Args:\n        task: the task to be logged\n\n    Returns:\n        (Dict[str, Any]): the task details like execution time, task name, template, compiled template, result, time.\n\n    \"\"\"\n    end_time = time() - self.start_time\n    log_record = {\n        \"task_name\": task.__name__,\n        \"llm_model\": task.llm_response.model,\n        \"template\": str(task.compile()),\n        \"call_kwargs\": str(self._kwargs),\n        \"compiled_template\": str(task.compile(**self._kwargs)),\n        \"result\": task.llm_response.response,\n        \"time\": end_time,\n    }\n    logger.info(log_record)\n    print(log_record)\n</code></pre>"},{"location":"reference/declarai/middleware/internal/log_middleware/#declarai.middleware.internal.log_middleware.LoggingMiddleware.before","title":"before","text":"<pre><code>before(_)\n</code></pre> <p>Before execution of the task, set the start time.</p> Source code in <code>src/declarai/middleware/internal/log_middleware.py</code> <pre><code>def before(self, _):\n\"\"\"\n    Before execution of the task, set the start time.\n    \"\"\"\n    self.start_time = time()\n</code></pre>"},{"location":"reference/declarai/middleware/third_party/","title":"Index","text":""},{"location":"reference/declarai/middleware/third_party/#declarai.middleware.third_party","title":"third_party","text":"<p>Middlewares offered that requires external dependencies.</p> <p>Modules:</p> Name Description <code>wandb_monitor</code> <p>Wandb Monitor Middleware used to monitor the execution on wandb.</p>"},{"location":"reference/declarai/middleware/third_party/wandb_monitor/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> wandb_monitor","text":""},{"location":"reference/declarai/middleware/third_party/wandb_monitor/#declarai.middleware.third_party.wandb_monitor","title":"wandb_monitor","text":"<p>Wandb Monitor Middleware used to monitor the execution on wandb.</p> <p>Classes:</p> Name Description <code>WandDBMonitorCreator</code> <p>Creates a WandDBMonitor middleware for a given task.</p>"},{"location":"reference/declarai/middleware/third_party/wandb_monitor/#declarai.middleware.third_party.wandb_monitor.WandDBMonitorCreator","title":"WandDBMonitorCreator","text":"<p>Creates a WandDBMonitor middleware for a given task.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the run on wandb</p> required <code>project</code> <code>str</code> <p>The name of the project on wandb</p> required <code>key</code> <code>str</code> <p>The api key for wandb</p> required <p>Returns:</p> Type Description <code>WandDBMonitor</code> <p>A WandDBMonitor middleware</p> Example <pre><code>WandDBMonitor = WandDBMonitorCreator(\n     name=\"&lt;task name&gt;\",\n     project=\"&lt;project name&gt;\",\n     key=\"&lt;decorators-key&gt;\",\n )\n\n @openai.task(middlewares=[WandDBMonitor])\n def generate_a_poem(title: str):\n'''\n     Generate a poem based on the given title\n     :return: The generated poem\n     '''\n     return declarai.magic(\"poem\", title)\n</code></pre>"},{"location":"reference/declarai/operators/","title":"Index","text":""},{"location":"reference/declarai/operators/#declarai.operators","title":"operators","text":"<p>Operators are the main interface that interacts internally with the LLMs.</p> <p>Modules:</p> Name Description <code>llm</code> <p>This module defines the base classes for the LLM interface.</p> <code>message</code> <p>Message definition for the operators.</p> <code>openai_operators</code> <p>OpenAI operators and LLMs.</p> <code>operator</code> <p>Operator is a class that is used to wrap the compilation of prompts and the singular execution of the LLM.</p> <code>registry</code> <p>Registry for LLMs and Operators.</p> <code>templates</code> <p>This module contains the shared templates for the operators.</p> <code>utils</code> <p>Functions:</p> Name Description <code>resolve_llm</code> <p>Resolves an LLM instance based on the provider and model name.</p> <code>resolve_operator</code> <p>Resolves an operator based on the LLM instance and the operator type.</p> <p>Attributes:</p> Name Type Description <code>ModelsOpenai</code> <p>All official OpenAI models</p>"},{"location":"reference/declarai/operators/#declarai.operators.ModelsOpenai","title":"ModelsOpenai  <code>module-attribute</code>","text":"<pre><code>ModelsOpenai = Literal[\n    \"gpt-4\",\n    \"gpt-3.5-turbo\",\n    \"gpt-3.5-turbo-16k\",\n    \"text-davinci-003\",\n    \"text-davinci-002\",\n    \"code-davinci-002\",\n]\n</code></pre> <p>All official OpenAI models</p>"},{"location":"reference/declarai/operators/#declarai.operators.resolve_llm","title":"resolve_llm","text":"<pre><code>resolve_llm(\n    provider: str, model: str = None, **kwargs: str\n) -&gt; LLM\n</code></pre> <p>Resolves an LLM instance based on the provider and model name.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Name of the provider</p> required <code>model</code> <code>str</code> <p>Name of the model</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments to pass to the LLM initialization</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>llm</code> <code>LLM</code> <p>instance</p> Source code in <code>src/declarai/operators/__init__.py</code> <pre><code>def resolve_llm(provider: str, model: str = None, **kwargs) -&gt; LLM:\n\"\"\"\n    Resolves an LLM instance based on the provider and model name.\n\n    Args:\n        provider: Name of the provider\n        model: Name of the model\n        **kwargs: Additional arguments to pass to the LLM initialization\n\n    Returns:\n        llm (LLM): instance\n    \"\"\"\n    if provider == ProviderOpenai:\n        model = LLMSettings(\n            provider=provider,\n            model=model,\n            version=kwargs.pop(\"version\", None),\n            **kwargs,\n        ).model\n\n    llm_instance = llm_registry.resolve(provider, model, **kwargs)\n    return llm_instance\n</code></pre>"},{"location":"reference/declarai/operators/#declarai.operators.resolve_operator","title":"resolve_operator","text":"<pre><code>resolve_operator(llm_instance: LLM, operator_type: str)\n</code></pre> <p>Resolves an operator based on the LLM instance and the operator type.</p> <p>Parameters:</p> Name Type Description Default <code>llm_instance</code> <code>LLM</code> <p>instance of initialized LLM</p> required <code>operator_type</code> <code>Type[BaseOperator]</code> <p>task or chat</p> required <p>Returns:</p> Type Description <p>Operator type class</p> Source code in <code>src/declarai/operators/__init__.py</code> <pre><code>def resolve_operator(llm_instance: LLM, operator_type: str):\n\"\"\"\n    Resolves an operator based on the LLM instance and the operator type.\n\n    Args:\n        llm_instance: instance of initialized LLM\n        operator_type (Type[BaseOperator]): task or chat\n\n    Returns:\n        Operator type class\n\n    \"\"\"\n    return operator_registry.resolve(llm_instance, operator_type)\n</code></pre>"},{"location":"reference/declarai/operators/llm/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> llm","text":""},{"location":"reference/declarai/operators/llm/#declarai.operators.llm","title":"llm","text":"<p>This module defines the base classes for the LLM interface.</p> <p>Classes:</p> Name Description <code>BaseLLM</code> <p>The base LLM class that all LLMs should inherit from.</p> <code>BaseLLMParams</code> <p>The base LLM params that are common to all LLMs.</p> <code>LLMResponse</code> <p>The response from the LLM.</p> <code>LLMSettings</code> <p>The settings for the LLM. Defines the model and version to use.</p> <p>Attributes:</p> Name Type Description <code>LLM</code> <p>Type variable for LLM</p> <code>LLMParamsType</code> <p>Type variable for LLM params</p>"},{"location":"reference/declarai/operators/llm/#declarai.operators.llm.LLM","title":"LLM  <code>module-attribute</code>","text":"<pre><code>LLM = TypeVar('LLM', bound=BaseLLM)\n</code></pre> <p>Type variable for LLM</p>"},{"location":"reference/declarai/operators/llm/#declarai.operators.llm.LLMParamsType","title":"LLMParamsType  <code>module-attribute</code>","text":"<pre><code>LLMParamsType = TypeVar(\n    \"LLMParamsType\", bound=BaseLLMParams\n)\n</code></pre> <p>Type variable for LLM params</p>"},{"location":"reference/declarai/operators/llm/#declarai.operators.llm.BaseLLM","title":"BaseLLM","text":"<p>The base LLM class that all LLMs should inherit from.</p> <p>Methods:</p> Name Description <code>predict</code> <p>The predict method that all LLMs should implement.</p>"},{"location":"reference/declarai/operators/llm/#declarai.operators.llm.BaseLLM.predict","title":"predict  <code>abstractmethod</code>","text":"<pre><code>predict(*args, **kwargs) -&gt; LLMResponse\n</code></pre> <p>The predict method that all LLMs should implement. Args:     args:     *kwargs:</p> <p>Returns: llm response object</p> Source code in <code>src/declarai/operators/llm.py</code> <pre><code>@abstractmethod\ndef predict(self, *args, **kwargs) -&gt; LLMResponse:\n\"\"\"\n    The predict method that all LLMs should implement.\n    Args:\n        *args:\n        **kwargs:\n\n    Returns: llm response object\n\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/declarai/operators/llm/#declarai.operators.llm.BaseLLMParams","title":"BaseLLMParams","text":"<p>             Bases: <code>TypedDict</code></p> <p>The base LLM params that are common to all LLMs.</p>"},{"location":"reference/declarai/operators/llm/#declarai.operators.llm.LLMResponse","title":"LLMResponse","text":"<p>             Bases: <code>BaseModel</code></p> <p>The response from the LLM.</p> <p>Attributes:</p> Name Type Description <code>response</code> <code>str</code> <p>The raw response from the LLM</p> <code>model</code> <code>Optional[str]</code> <p>The model that was used to generate the response</p> <code>prompt_tokens</code> <code>Optional[int]</code> <p>The number of tokens in the prompt</p> <code>completion_tokens</code> <code>Optional[int]</code> <p>The number of tokens in the completion</p> <code>total_tokens</code> <code>Optional[int]</code> <p>The total number of tokens in the response</p>"},{"location":"reference/declarai/operators/llm/#declarai.operators.llm.LLMSettings","title":"LLMSettings","text":"<pre><code>LLMSettings(\n    provider: str,\n    model: str,\n    version: Optional[str] = None,\n    **_: Optional[str]\n)\n</code></pre> <p>The settings for the LLM. Defines the model and version to use.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>The provider of the model (openai, cohere, etc.)</p> required <code>model</code> <code>str</code> <p>The model to use (gpt-4, gpt-3.5-turbo, etc.)</p> required <code>version</code> <code>Optional[str]</code> <p>The version of the model to use (optional)</p> <code>None</code> <code>**_</code> <p>Any additional params that are specific to the provider that will be ignored.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>provider</code> <code>str</code> <p>The provider of the model (openai, cohere, etc.)</p> <code>model</code> <code>str</code> <p>The full model name to use.</p> <code>version</code> <p>The version of the model to use (optional)</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>Some model providers allow defining a base model as well as a sub-model.</p> Source code in <code>src/declarai/operators/llm.py</code> <pre><code>def __init__(\n    self,\n    provider: str,\n    model: str,\n    version: Optional[str] = None,\n    **_,\n):\n    self.provider = provider\n    self._model = model\n    self.version = version\n</code></pre>"},{"location":"reference/declarai/operators/llm/#declarai.operators.llm.LLMSettings.model","title":"model  <code>property</code>","text":"<pre><code>model: str\n</code></pre> <p>Some model providers allow defining a base model as well as a sub-model. Often the base model is an alias to latest model served on that model. for example, when sending gpt-3.5-turbo to OpenAI, the actual model will be one of the publicly available snapshots or an internally exposed version as described on their website: as of 27/07/2023 - https://platform.openai.com/docs/models/continuous-model-upgrades | With the release of gpt-3.5-turbo, some of our models are now being continually updated. | We also offer static model versions that developers can continue using for at least | three months after an updated model has been introduced.</p> <p>Another use-case for sub models is using your own fine-tuned models. As described in the documentation: https://platform.openai.com/docs/guides/fine-tuning/customize-your-model-name</p> <p>You will likely build your fine-tuned model names by concatenating the base model name with the fine-tuned model name, separated by a hyphen. For example gpt-3.5-turbo-declarai-text-classification-2023-03 or gpt-3.5-turbo:declarai:text-classification-2023-03</p> <p>In any case you can always pass the full model name in the model parameter and leave the sub_model parameter empty if you prefer.</p>"},{"location":"reference/declarai/operators/message/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> message","text":""},{"location":"reference/declarai/operators/message/#declarai.operators.message","title":"message","text":"<p>Message definition for the operators.</p> <p>Classes:</p> Name Description <code>Message</code> <p>Represents a message in the chat.</p> <code>MessageRole</code> <p>Message role enum for the Message class to indicate the role of the message in the chat.</p>"},{"location":"reference/declarai/operators/message/#declarai.operators.message.Message","title":"Message","text":"<p>             Bases: <code>BaseModel</code></p> <p>Represents a message in the chat.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <p>The message string</p> required <code>role</code> <p>The role of the message in the chat</p> required <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>The message string</p> <code>role</code> <code>MessageRole</code> <p>The role of the message in the chat</p>"},{"location":"reference/declarai/operators/message/#declarai.operators.message.MessageRole","title":"MessageRole","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Message role enum for the Message class to indicate the role of the message in the chat.</p> <p>Attributes:</p> Name Type Description <code>system</code> <code>str</code> <p>The message is the system message, usually used as the first message in the chat.</p> <code>user</code> <code>str</code> <p>Every message that is sent by the user.</p> <code>assistant</code> <code>str</code> <p>Every message that is sent by the assistant.</p> <code>function</code> <code>str</code> <p>Every message that is sent by the assistant that is a function call.</p>"},{"location":"reference/declarai/operators/operator/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> operator","text":""},{"location":"reference/declarai/operators/operator/#declarai.operators.operator","title":"operator","text":"<p>Operator is a class that is used to wrap the compilation of prompts and the singular execution of the LLM.</p> <p>Classes:</p> Name Description <code>BaseChatOperator</code> <p>Base class for chat operators.</p> <code>BaseOperator</code> <p>Operator is a class that is used to wrap the compilation of prompts and the singular execution of the LLM.</p>"},{"location":"reference/declarai/operators/operator/#declarai.operators.operator.BaseChatOperator","title":"BaseChatOperator","text":"<pre><code>BaseChatOperator(\n    system: Optional[str] = None,\n    greeting: Optional[str] = None,\n    parsed: PythonParser = None,\n    streaming: bool = None,\n    **kwargs: bool\n)\n</code></pre> <p>             Bases: <code>BaseOperator</code></p> <p>Base class for chat operators. It extends the <code>BaseOperator</code> class and adds additional attributes that are used for chat operators. See <code>BaseOperator</code> for more information.</p> <p>Parameters:</p> Name Type Description Default <code>system</code> <code>Optional[str]</code> <p>The system message that is used for the chat</p> <code>None</code> <code>greeting</code> <code>Optional[str]</code> <p>The greeting message that is used for the chat.</p> <code>None</code> <code>kwargs</code> <p>Enables passing all the required parameters for <code>BaseOperator</code></p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>system</code> <code>str</code> <p>The system message that is used for the chat</p> <code>greeting</code> <code>str</code> <p>The greeting message that is used for the chat.</p> <code>parsed_send_func</code> <code>PythonParser</code> <p>The parsed object that is used to compile the send function.</p> <p>Methods:</p> Name Description <code>compile_template</code> <p>Compiles the system prompt.</p> <code>parse_output</code> <p>Parses the raw output from the LLM into the desired format that was set in the parsed object.</p> <code>predict</code> <p>Executes prediction using the LLM.</p> <p>Attributes:</p> Name Type Description <code>streaming</code> <code>bool</code> <p>Returns whether the operator is streaming or not</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def __init__(\n    self,\n    system: Optional[str] = None,\n    greeting: Optional[str] = None,\n    parsed: PythonParser = None,\n    streaming: bool = None,\n    **kwargs,\n):\n    super().__init__(parsed=parsed, streaming=streaming, **kwargs)\n    self.system = system or self.parsed.docstring_freeform\n    self.greeting = greeting or getattr(self.parsed.decorated, \"greeting\", None)\n    self.parsed_send_func = (\n        PythonParser(self.parsed.decorated.send)\n        if getattr(self.parsed.decorated, \"send\", None)\n        else None\n    )\n</code></pre>"},{"location":"reference/declarai/operators/operator/#declarai.operators.operator.BaseChatOperator.streaming","title":"streaming  <code>property</code>","text":"<pre><code>streaming: bool\n</code></pre> <p>Returns whether the operator is streaming or not Returns:</p>"},{"location":"reference/declarai/operators/operator/#declarai.operators.operator.BaseChatOperator.compile_template","title":"compile_template","text":"<pre><code>compile_template() -&gt; Message\n</code></pre> <p>Compiles the system prompt. Returns: The compiled system message</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def compile_template(self) -&gt; Message:\n\"\"\"\n    Compiles the system prompt.\n    Returns: The compiled system message\n    \"\"\"\n    structured_template = StructuredOutputChatPrompt\n    if self.parsed_send_func:\n        output_schema = self._compile_output_prompt(structured_template)\n    else:\n        output_schema = None\n\n    if output_schema:\n        compiled_system_prompt = f\"{self.system}/n{output_schema}\"\n    else:\n        compiled_system_prompt = self.system\n    return Message(message=compiled_system_prompt, role=MessageRole.system)\n</code></pre>"},{"location":"reference/declarai/operators/operator/#declarai.operators.operator.BaseChatOperator.parse_output","title":"parse_output","text":"<pre><code>parse_output(output: str) -&gt; Any\n</code></pre> <p>Parses the raw output from the LLM into the desired format that was set in the parsed object. Args:     output: llm string output</p> <p>Returns:</p> Type Description <code>Any</code> <p>Any parsed output</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def parse_output(self, output: str) -&gt; Any:\n\"\"\"\n    Parses the raw output from the LLM into the desired format that was set in the parsed object.\n    Args:\n        output: llm string output\n\n    Returns:\n        Any parsed output\n    \"\"\"\n    return self.parsed.parse(output)\n</code></pre>"},{"location":"reference/declarai/operators/operator/#declarai.operators.operator.BaseChatOperator.predict","title":"predict","text":"<pre><code>predict(\n    *,\n    llm_params: Optional[LLMParamsType] = None,\n    **kwargs: object\n) -&gt; Union[LLMResponse, Iterator[LLMResponse]]\n</code></pre> <p>Executes prediction using the LLM. It first compiles the prompts using the <code>compile</code> method, and then executes the LLM with the compiled prompts and the llm_params. Args:     llm_params: The parameters that are passed during runtime. If provided, they will override the ones provided during initialization.     **kwargs: The keyword arguments to pass to the <code>compile</code> method. Used to format the prompts placeholders.</p> <p>Returns:</p> Type Description <code>Union[LLMResponse, Iterator[LLMResponse]]</code> <p>The response from the LLM</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def predict(\n    self, *, llm_params: Optional[LLMParamsType] = None, **kwargs: object\n) -&gt; Union[LLMResponse, Iterator[LLMResponse]]:\n\"\"\"\n    Executes prediction using the LLM.\n    It first compiles the prompts using the `compile` method, and then executes the LLM with the compiled prompts and the llm_params.\n    Args:\n        llm_params: The parameters that are passed during runtime. If provided, they will override the ones provided during initialization.\n        **kwargs: The keyword arguments to pass to the `compile` method. Used to format the prompts placeholders.\n\n    Returns:\n        The response from the LLM\n    \"\"\"\n    llm_params = llm_params or self.llm_params  # Order is important -\n    if self.streaming is not None:\n        llm_params[\"stream\"] = self.streaming  # streaming should be the last param\n    # provided params during execution should override the ones provided during initialization\n    return self.llm.predict(**self.compile(**kwargs), **llm_params)\n</code></pre>"},{"location":"reference/declarai/operators/operator/#declarai.operators.operator.BaseOperator","title":"BaseOperator","text":"<pre><code>BaseOperator(\n    llm: LLM,\n    parsed: PythonParser,\n    llm_params: LLMParamsType = None,\n    streaming: bool = None,\n    **kwargs: Dict\n)\n</code></pre> <p>Operator is a class that is used to wrap the compilation of prompts and the singular execution of the LLM. Args:     llm: The LLM to use for the operator     parsed (PythonParser): The parsed object that is used to compile the prompts     llm_params: The parameters to pass to the LLM     streaming: Whether to use streaming or not     kwargs: Enables passing of additional parameters to the operator Attributes:     llm (LLM): The LLM to use for the operator     parsed (PythonParser): The parsed object that is used to compile the prompts     llm_params (LLMParamsType): The parameters that were passed during initialization of the operator</p> <p>Methods:</p> Name Description <code>compile</code> <p>Compiles the prompts using the parsed object and returns the compiled prompts</p> <code>predict</code> <p>Executes the LLM with the compiled prompts and the llm_params</p> <code>parse_output</code> <p>Parses the output of the LLM</p> <p>Methods:</p> Name Description <code>compile</code> <p>Implements the compile method of the BaseOperator class.</p> <code>parse_output</code> <p>Parses the raw output from the LLM into the desired format that was set in the parsed object.</p> <code>predict</code> <p>Executes prediction using the LLM.</p> <p>Attributes:</p> Name Type Description <code>streaming</code> <code>bool</code> <p>Returns whether the operator is streaming or not</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def __init__(\n    self,\n    llm: LLM,\n    parsed: PythonParser,\n    llm_params: LLMParamsType = None,\n    streaming: bool = None,\n    **kwargs: Dict,\n):\n    self.llm = llm\n    self.parsed = parsed\n    self.llm_params = llm_params or {}\n    self._call_streaming = streaming\n</code></pre>"},{"location":"reference/declarai/operators/operator/#declarai.operators.operator.BaseOperator.streaming","title":"streaming  <code>property</code>","text":"<pre><code>streaming: bool\n</code></pre> <p>Returns whether the operator is streaming or not Returns:</p>"},{"location":"reference/declarai/operators/operator/#declarai.operators.operator.BaseOperator.compile","title":"compile","text":"<pre><code>compile(**kwargs) -&gt; CompiledTemplate\n</code></pre> <p>Implements the compile method of the BaseOperator class. Args:     **kwargs:</p> <p>Returns:</p> Type Description <code>CompiledTemplate</code> <p>Dict[str, List[Message]]: A dictionary containing a list of messages.</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def compile(self, **kwargs) -&gt; CompiledTemplate:\n\"\"\"\n    Implements the compile method of the BaseOperator class.\n    Args:\n        **kwargs:\n\n    Returns:\n        Dict[str, List[Message]]: A dictionary containing a list of messages.\n\n    \"\"\"\n    template = self.compile_template()\n    if kwargs:\n        template[-1].message = format_prompt_msg(\n            _string=template[-1].message, **kwargs\n        )\n    return {\"messages\": template}\n</code></pre>"},{"location":"reference/declarai/operators/operator/#declarai.operators.operator.BaseOperator.parse_output","title":"parse_output","text":"<pre><code>parse_output(output: str) -&gt; Any\n</code></pre> <p>Parses the raw output from the LLM into the desired format that was set in the parsed object. Args:     output: llm string output</p> <p>Returns:</p> Type Description <code>Any</code> <p>Any parsed output</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def parse_output(self, output: str) -&gt; Any:\n\"\"\"\n    Parses the raw output from the LLM into the desired format that was set in the parsed object.\n    Args:\n        output: llm string output\n\n    Returns:\n        Any parsed output\n    \"\"\"\n    return self.parsed.parse(output)\n</code></pre>"},{"location":"reference/declarai/operators/operator/#declarai.operators.operator.BaseOperator.predict","title":"predict","text":"<pre><code>predict(\n    *,\n    llm_params: Optional[LLMParamsType] = None,\n    **kwargs: object\n) -&gt; Union[LLMResponse, Iterator[LLMResponse]]\n</code></pre> <p>Executes prediction using the LLM. It first compiles the prompts using the <code>compile</code> method, and then executes the LLM with the compiled prompts and the llm_params. Args:     llm_params: The parameters that are passed during runtime. If provided, they will override the ones provided during initialization.     **kwargs: The keyword arguments to pass to the <code>compile</code> method. Used to format the prompts placeholders.</p> <p>Returns:</p> Type Description <code>Union[LLMResponse, Iterator[LLMResponse]]</code> <p>The response from the LLM</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def predict(\n    self, *, llm_params: Optional[LLMParamsType] = None, **kwargs: object\n) -&gt; Union[LLMResponse, Iterator[LLMResponse]]:\n\"\"\"\n    Executes prediction using the LLM.\n    It first compiles the prompts using the `compile` method, and then executes the LLM with the compiled prompts and the llm_params.\n    Args:\n        llm_params: The parameters that are passed during runtime. If provided, they will override the ones provided during initialization.\n        **kwargs: The keyword arguments to pass to the `compile` method. Used to format the prompts placeholders.\n\n    Returns:\n        The response from the LLM\n    \"\"\"\n    llm_params = llm_params or self.llm_params  # Order is important -\n    if self.streaming is not None:\n        llm_params[\"stream\"] = self.streaming  # streaming should be the last param\n    # provided params during execution should override the ones provided during initialization\n    return self.llm.predict(**self.compile(**kwargs), **llm_params)\n</code></pre>"},{"location":"reference/declarai/operators/registry/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> registry","text":""},{"location":"reference/declarai/operators/registry/#declarai.operators.registry","title":"registry","text":"<p>Registry for LLMs and Operators.</p> <p>Classes:</p> Name Description <code>LLMRegistry</code> <p>Registry for LLMs.</p> <code>OperatorRegistry</code> <p>Registry for Operators.</p> <p>Functions:</p> Name Description <code>register_llm</code> <p>A decorator that registers an LLM class to the LLM registry.</p> <code>register_operator</code> <p>A decorator that registers an operator class to the operator registry.</p> <p>Attributes:</p> Name Type Description <code>llm_registry</code> <p>The global LLM registry.</p> <code>operator_registry</code> <p>The global Operator registry.</p>"},{"location":"reference/declarai/operators/registry/#declarai.operators.registry.llm_registry","title":"llm_registry  <code>module-attribute</code>","text":"<pre><code>llm_registry = LLMRegistry()\n</code></pre> <p>The global LLM registry.</p>"},{"location":"reference/declarai/operators/registry/#declarai.operators.registry.operator_registry","title":"operator_registry  <code>module-attribute</code>","text":"<pre><code>operator_registry = OperatorRegistry()\n</code></pre> <p>The global Operator registry.</p>"},{"location":"reference/declarai/operators/registry/#declarai.operators.registry.LLMRegistry","title":"LLMRegistry","text":"<pre><code>LLMRegistry()\n</code></pre> <p>Registry for LLMs. The registry will have a nested structure: {provider: {model: llm_cls}} But it will also support a direct provider-to-llm_cls mapping for generic LLMs. But it will also support a direct provider-to-llm_cls mapping for generic LLMs.</p> <p>Methods:</p> Name Description <code>register</code> <p>Registers an LLM.</p> <code>resolve</code> <p>Resolves an LLM. If the model is not specified, the default model of the given provider will be used.</p> Source code in <code>src/declarai/operators/registry.py</code> <pre><code>def __init__(self):\n    self._registry = defaultdict(dict)\n</code></pre>"},{"location":"reference/declarai/operators/registry/#declarai.operators.registry.LLMRegistry.register","title":"register","text":"<pre><code>register(\n    provider: str,\n    llm_cls: Type[LLM],\n    model: Optional[str] = \"default\",\n)\n</code></pre> <p>Registers an LLM. Args:     provider: the name of the LLM provider.     llm_cls: the LLM class to register.     model: the specific model name.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>src/declarai/operators/registry.py</code> <pre><code>def register(\n    self, provider: str, llm_cls: Type[LLM], model: Optional[str] = \"default\"\n):\n\"\"\"\n    Registers an LLM.\n    Args:\n        provider: the name of the LLM provider.\n        llm_cls: the LLM class to register.\n        model: the specific model name.\n\n    Returns:\n        None\n\n    \"\"\"\n    self._registry[provider][model] = llm_cls\n</code></pre>"},{"location":"reference/declarai/operators/registry/#declarai.operators.registry.LLMRegistry.resolve","title":"resolve","text":"<pre><code>resolve(\n    provider: str,\n    model: Optional[str] = None,\n    **kwargs: Optional[str]\n) -&gt; LLM\n</code></pre> <p>Resolves an LLM. If the model is not specified, the default model of the given provider will be used. Args:     provider: the name of the LLM provider.     model: the specific model name.     **kwargs: Additional keyword arguments to pass to the LLM initialization.</p> <p>Returns:</p> Type Description <code>LLM</code> <p>An LLM instance.</p> Source code in <code>src/declarai/operators/registry.py</code> <pre><code>def resolve(self, provider: str, model: Optional[str] = None, **kwargs) -&gt; LLM:\n\"\"\"\n    Resolves an LLM. If the model is not specified, the default model of the given provider will be used.\n    Args:\n        provider: the name of the LLM provider.\n        model: the specific model name.\n        **kwargs: Additional keyword arguments to pass to the LLM initialization.\n\n    Returns:\n        An LLM instance.\n    \"\"\"\n    if not model:\n        model = \"default\"\n    provider_registry = self._registry.get(provider, {})\n\n    llm_cls = provider_registry.get(model)\n    if not llm_cls:\n        # If the specific model isn't found, fall back to the default.\n        llm_cls = provider_registry.get(\"default\")\n\n    if not llm_cls:\n        raise NotImplementedError(\n            f\"LLMProvider : {provider} or model: {model} not implemented\"\n        )\n    try:\n        return llm_cls(model=model, **kwargs)\n    except TypeError:\n        return llm_cls(**kwargs)\n</code></pre>"},{"location":"reference/declarai/operators/registry/#declarai.operators.registry.OperatorRegistry","title":"OperatorRegistry","text":"<pre><code>OperatorRegistry()\n</code></pre> <p>Registry for Operators. The registry will have a nested structure: {provider: {operator_type: {model: operator_cls}}} It will support a direct provider-to-operator_cls mapping for generic Operators.</p> <p>Methods:</p> Name Description <code>register</code> <p>Registers an operator.</p> <code>resolve</code> <p>Resolves an operator.</p> Source code in <code>src/declarai/operators/registry.py</code> <pre><code>def __init__(self):\n    self._registry = defaultdict(lambda: defaultdict(dict))\n</code></pre>"},{"location":"reference/declarai/operators/registry/#declarai.operators.registry.OperatorRegistry.register","title":"register","text":"<pre><code>register(\n    provider: str,\n    operator_type: str,\n    operator_cls: Type[BaseOperator],\n    model: str = \"default\",\n)\n</code></pre> <p>Registers an operator.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>the name of the operator provider.</p> required <code>operator_type</code> <code>str</code> <p>the type of the operator.</p> required <code>operator_cls</code> <code>Type[BaseOperator]</code> <p>the operator class to register.</p> required <code>model</code> <code>str</code> <p>the specific model name that the operator is registered to.</p> <code>'default'</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>src/declarai/operators/registry.py</code> <pre><code>def register(\n    self,\n    provider: str,\n    operator_type: str,\n    operator_cls: Type[BaseOperator],\n    model: str = \"default\",\n):\n\"\"\"\n    Registers an operator.\n\n    Args:\n        provider: the name of the operator provider.\n        operator_type: the type of the operator.\n        operator_cls: the operator class to register.\n        model: the specific model name that the operator is registered to.\n\n    Returns:\n        None\n\n    \"\"\"\n    self._registry[provider][operator_type][model] = operator_cls\n</code></pre>"},{"location":"reference/declarai/operators/registry/#declarai.operators.registry.OperatorRegistry.resolve","title":"resolve","text":"<pre><code>resolve(\n    llm_instance: LLM, operator_type: str\n) -&gt; Type[BaseOperator]\n</code></pre> <p>Resolves an operator.</p> <p>Parameters:</p> Name Type Description Default <code>llm_instance</code> <code>LLM</code> <p>An LLM instance.</p> required <code>operator_type</code> <code>str</code> <p>A string representing the type of the operator.</p> required <p>Returns:</p> Type Description <code>Type[BaseOperator]</code> <p>An operator class.</p> Source code in <code>src/declarai/operators/registry.py</code> <pre><code>def resolve(self, llm_instance: LLM, operator_type: str) -&gt; Type[BaseOperator]:\n\"\"\"\n    Resolves an operator.\n\n    Args:\n        llm_instance: An LLM instance.\n        operator_type: A string representing the type of the operator.\n\n    Returns:\n        An operator class.\n\n    \"\"\"\n    default_model = \"default\"\n    provider = llm_instance.provider\n    operator_registry = self._registry.get(provider, {})\n\n    specific_operator_registry = operator_registry.get(operator_type, {})\n    operator_cls = specific_operator_registry.get(llm_instance.model)\n\n    if not operator_cls:\n        # If the specific model isn't found, fall back to the default.\n        operator_cls = specific_operator_registry.get(default_model)\n\n    if not operator_cls:\n        raise NotImplementedError(\n            f\"Operator type : {operator_type} for provider: {provider} or model: {llm_instance.model} not implemented\"\n        )\n\n    return operator_cls\n</code></pre>"},{"location":"reference/declarai/operators/registry/#declarai.operators.registry.register_llm","title":"register_llm","text":"<pre><code>register_llm(\n    provider: str, model: Optional[str] = \"default\"\n)\n</code></pre> <p>A decorator that registers an LLM class to the LLM registry.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>the name of the LLM provider.</p> required <code>model</code> <code>Optional[str]</code> <p>the specific model name.</p> <code>'default'</code> <p>Returns:</p> Type Description <p>A decorator that registers the decorated LLM class to the LLM registry.</p> Source code in <code>src/declarai/operators/registry.py</code> <pre><code>def register_llm(provider: str, model: Optional[str] = \"default\"):\n\"\"\"\n    A decorator that registers an LLM class to the LLM registry.\n\n    Args:\n        provider: the name of the LLM provider.\n        model: the specific model name.\n\n    Returns:\n        A decorator that registers the decorated LLM class to the LLM registry.\n    \"\"\"\n\n    def decorator(cls):\n        llm_registry.register(provider, cls, model)\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"reference/declarai/operators/registry/#declarai.operators.registry.register_operator","title":"register_operator","text":"<pre><code>register_operator(\n    provider: str,\n    operator_type: str,\n    model: Optional[str] = \"default\",\n)\n</code></pre> <p>A decorator that registers an operator class to the operator registry.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>the name of the operator provider.</p> required <code>operator_type</code> <code>str</code> <p>the string representing the type of the operator.</p> required <code>model</code> <code>Optional[str]</code> <p>the specific model name.</p> <code>'default'</code> <p>Returns:</p> Type Description <p>A decorator that registers the decorated operator class to the operator registry.</p> Source code in <code>src/declarai/operators/registry.py</code> <pre><code>def register_operator(\n    provider: str, operator_type: str, model: Optional[str] = \"default\"\n):\n\"\"\"\n    A decorator that registers an operator class to the operator registry.\n\n    Args:\n        provider: the name of the operator provider.\n        operator_type: the string representing the type of the operator.\n        model: the specific model name.\n\n    Returns:\n        A decorator that registers the decorated operator class to the operator registry.\n\n    \"\"\"\n\n    def decorator(cls):\n        operator_registry.register(provider, operator_type, cls, model)\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"reference/declarai/operators/utils/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> utils","text":""},{"location":"reference/declarai/operators/utils/#declarai.operators.utils","title":"utils","text":"<p>Functions:</p> Name Description <code>can_be_jinja</code> <p>Checks if a string can be compiled using the jinja2 template engine.</p> <code>format_prompt_msg</code> <p>Formats a string using the jinja2 template engine if possible, otherwise uses the python string format.</p>"},{"location":"reference/declarai/operators/utils/#declarai.operators.utils.can_be_jinja","title":"can_be_jinja","text":"<pre><code>can_be_jinja(string: str) -&gt; bool\n</code></pre> <p>Checks if a string can be compiled using the jinja2 template engine.</p> Source code in <code>src/declarai/operators/utils.py</code> <pre><code>def can_be_jinja(string: str) -&gt; bool:\n\"\"\"\n    Checks if a string can be compiled using the jinja2 template engine.\n    \"\"\"\n    if \"{{\" in string or \"{%\" in string or \"{#\" in string:\n        try:\n            jinja2.Template(string)\n            return True\n        except jinja2.exceptions.TemplateSyntaxError:\n            return False\n    else:\n        return False\n</code></pre>"},{"location":"reference/declarai/operators/utils/#declarai.operators.utils.format_prompt_msg","title":"format_prompt_msg","text":"<pre><code>format_prompt_msg(_string: str, **kwargs: str) -&gt; str\n</code></pre> <p>Formats a string using the jinja2 template engine if possible, otherwise uses the python string format. Args:     _string: The string to format     **kwargs: The kwargs to pass to the template</p> <p>Returns: The formatted string</p> Source code in <code>src/declarai/operators/utils.py</code> <pre><code>def format_prompt_msg(_string: str, **kwargs) -&gt; str:\n\"\"\"\n    Formats a string using the jinja2 template engine if possible, otherwise uses the python string format.\n    Args:\n        _string: The string to format\n        **kwargs: The kwargs to pass to the template\n\n    Returns: The formatted string\n    \"\"\"\n    if can_be_jinja(_string):\n        return jinja2.Template(_string).render(**kwargs)\n    else:\n        return _string.format(**kwargs)\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/","title":"Index","text":""},{"location":"reference/declarai/operators/openai_operators/#declarai.operators.openai_operators","title":"openai_operators","text":"<p>OpenAI operators and LLMs.</p> <p>Modules:</p> Name Description <code>chat_operator</code> <p>Chat implementation of OpenAI operator.</p> <code>openai_llm</code> <p>LLM implementation for OpenAI</p> <code>settings</code> <p>Environment level configurations for working with openai and Azure openai providers.</p> <code>task_operator</code> <p>Task implementation for openai operator.</p>"},{"location":"reference/declarai/operators/openai_operators/chat_operator/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> chat_operator","text":""},{"location":"reference/declarai/operators/openai_operators/chat_operator/#declarai.operators.openai_operators.chat_operator","title":"chat_operator","text":"<p>Chat implementation of OpenAI operator.</p> <p>Classes:</p> Name Description <code>AzureOpenAIChatOperator</code> <p>Chat implementation of OpenAI operator. This is a child of the BaseChatOperator class. See the BaseChatOperator class for further documentation.</p> <code>OpenAIChatOperator</code> <p>Chat implementation of OpenAI operator. This is a child of the BaseChatOperator class. See the BaseChatOperator class for further documentation.</p>"},{"location":"reference/declarai/operators/openai_operators/chat_operator/#declarai.operators.openai_operators.chat_operator.AzureOpenAIChatOperator","title":"AzureOpenAIChatOperator","text":"<p>             Bases: <code>OpenAIChatOperator</code></p> <p>Chat implementation of OpenAI operator. This is a child of the BaseChatOperator class. See the BaseChatOperator class for further documentation.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <code>AzureOpenAILLM</code> <p>AzureOpenAILLM</p> <p>Methods:</p> Name Description <code>compile_template</code> <p>Compiles the system prompt.</p> <code>parse_output</code> <p>Parses the raw output from the LLM into the desired format that was set in the parsed object.</p> <code>predict</code> <p>Executes prediction using the LLM.</p> <p>Attributes:</p> Name Type Description <code>streaming</code> <code>bool</code> <p>Returns whether the operator is streaming or not</p>"},{"location":"reference/declarai/operators/openai_operators/chat_operator/#declarai.operators.openai_operators.chat_operator.AzureOpenAIChatOperator.streaming","title":"streaming  <code>property</code>","text":"<pre><code>streaming: bool\n</code></pre> <p>Returns whether the operator is streaming or not Returns:</p>"},{"location":"reference/declarai/operators/openai_operators/chat_operator/#declarai.operators.openai_operators.chat_operator.AzureOpenAIChatOperator.compile_template","title":"compile_template","text":"<pre><code>compile_template() -&gt; Message\n</code></pre> <p>Compiles the system prompt. Returns: The compiled system message</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def compile_template(self) -&gt; Message:\n\"\"\"\n    Compiles the system prompt.\n    Returns: The compiled system message\n    \"\"\"\n    structured_template = StructuredOutputChatPrompt\n    if self.parsed_send_func:\n        output_schema = self._compile_output_prompt(structured_template)\n    else:\n        output_schema = None\n\n    if output_schema:\n        compiled_system_prompt = f\"{self.system}/n{output_schema}\"\n    else:\n        compiled_system_prompt = self.system\n    return Message(message=compiled_system_prompt, role=MessageRole.system)\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/chat_operator/#declarai.operators.openai_operators.chat_operator.AzureOpenAIChatOperator.parse_output","title":"parse_output","text":"<pre><code>parse_output(output: str) -&gt; Any\n</code></pre> <p>Parses the raw output from the LLM into the desired format that was set in the parsed object. Args:     output: llm string output</p> <p>Returns:</p> Type Description <code>Any</code> <p>Any parsed output</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def parse_output(self, output: str) -&gt; Any:\n\"\"\"\n    Parses the raw output from the LLM into the desired format that was set in the parsed object.\n    Args:\n        output: llm string output\n\n    Returns:\n        Any parsed output\n    \"\"\"\n    return self.parsed.parse(output)\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/chat_operator/#declarai.operators.openai_operators.chat_operator.AzureOpenAIChatOperator.predict","title":"predict","text":"<pre><code>predict(\n    *,\n    llm_params: Optional[LLMParamsType] = None,\n    **kwargs: object\n) -&gt; Union[LLMResponse, Iterator[LLMResponse]]\n</code></pre> <p>Executes prediction using the LLM. It first compiles the prompts using the <code>compile</code> method, and then executes the LLM with the compiled prompts and the llm_params. Args:     llm_params: The parameters that are passed during runtime. If provided, they will override the ones provided during initialization.     **kwargs: The keyword arguments to pass to the <code>compile</code> method. Used to format the prompts placeholders.</p> <p>Returns:</p> Type Description <code>Union[LLMResponse, Iterator[LLMResponse]]</code> <p>The response from the LLM</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def predict(\n    self, *, llm_params: Optional[LLMParamsType] = None, **kwargs: object\n) -&gt; Union[LLMResponse, Iterator[LLMResponse]]:\n\"\"\"\n    Executes prediction using the LLM.\n    It first compiles the prompts using the `compile` method, and then executes the LLM with the compiled prompts and the llm_params.\n    Args:\n        llm_params: The parameters that are passed during runtime. If provided, they will override the ones provided during initialization.\n        **kwargs: The keyword arguments to pass to the `compile` method. Used to format the prompts placeholders.\n\n    Returns:\n        The response from the LLM\n    \"\"\"\n    llm_params = llm_params or self.llm_params  # Order is important -\n    if self.streaming is not None:\n        llm_params[\"stream\"] = self.streaming  # streaming should be the last param\n    # provided params during execution should override the ones provided during initialization\n    return self.llm.predict(**self.compile(**kwargs), **llm_params)\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/chat_operator/#declarai.operators.openai_operators.chat_operator.OpenAIChatOperator","title":"OpenAIChatOperator","text":"<p>             Bases: <code>BaseChatOperator</code></p> <p>Chat implementation of OpenAI operator. This is a child of the BaseChatOperator class. See the BaseChatOperator class for further documentation.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <code>OpenAILLM</code> <p>OpenAILLM</p> <p>Methods:</p> Name Description <code>compile_template</code> <p>Compiles the system prompt.</p> <code>parse_output</code> <p>Parses the raw output from the LLM into the desired format that was set in the parsed object.</p> <code>predict</code> <p>Executes prediction using the LLM.</p> <p>Attributes:</p> Name Type Description <code>streaming</code> <code>bool</code> <p>Returns whether the operator is streaming or not</p>"},{"location":"reference/declarai/operators/openai_operators/chat_operator/#declarai.operators.openai_operators.chat_operator.OpenAIChatOperator.streaming","title":"streaming  <code>property</code>","text":"<pre><code>streaming: bool\n</code></pre> <p>Returns whether the operator is streaming or not Returns:</p>"},{"location":"reference/declarai/operators/openai_operators/chat_operator/#declarai.operators.openai_operators.chat_operator.OpenAIChatOperator.compile_template","title":"compile_template","text":"<pre><code>compile_template() -&gt; Message\n</code></pre> <p>Compiles the system prompt. Returns: The compiled system message</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def compile_template(self) -&gt; Message:\n\"\"\"\n    Compiles the system prompt.\n    Returns: The compiled system message\n    \"\"\"\n    structured_template = StructuredOutputChatPrompt\n    if self.parsed_send_func:\n        output_schema = self._compile_output_prompt(structured_template)\n    else:\n        output_schema = None\n\n    if output_schema:\n        compiled_system_prompt = f\"{self.system}/n{output_schema}\"\n    else:\n        compiled_system_prompt = self.system\n    return Message(message=compiled_system_prompt, role=MessageRole.system)\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/chat_operator/#declarai.operators.openai_operators.chat_operator.OpenAIChatOperator.parse_output","title":"parse_output","text":"<pre><code>parse_output(output: str) -&gt; Any\n</code></pre> <p>Parses the raw output from the LLM into the desired format that was set in the parsed object. Args:     output: llm string output</p> <p>Returns:</p> Type Description <code>Any</code> <p>Any parsed output</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def parse_output(self, output: str) -&gt; Any:\n\"\"\"\n    Parses the raw output from the LLM into the desired format that was set in the parsed object.\n    Args:\n        output: llm string output\n\n    Returns:\n        Any parsed output\n    \"\"\"\n    return self.parsed.parse(output)\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/chat_operator/#declarai.operators.openai_operators.chat_operator.OpenAIChatOperator.predict","title":"predict","text":"<pre><code>predict(\n    *,\n    llm_params: Optional[LLMParamsType] = None,\n    **kwargs: object\n) -&gt; Union[LLMResponse, Iterator[LLMResponse]]\n</code></pre> <p>Executes prediction using the LLM. It first compiles the prompts using the <code>compile</code> method, and then executes the LLM with the compiled prompts and the llm_params. Args:     llm_params: The parameters that are passed during runtime. If provided, they will override the ones provided during initialization.     **kwargs: The keyword arguments to pass to the <code>compile</code> method. Used to format the prompts placeholders.</p> <p>Returns:</p> Type Description <code>Union[LLMResponse, Iterator[LLMResponse]]</code> <p>The response from the LLM</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def predict(\n    self, *, llm_params: Optional[LLMParamsType] = None, **kwargs: object\n) -&gt; Union[LLMResponse, Iterator[LLMResponse]]:\n\"\"\"\n    Executes prediction using the LLM.\n    It first compiles the prompts using the `compile` method, and then executes the LLM with the compiled prompts and the llm_params.\n    Args:\n        llm_params: The parameters that are passed during runtime. If provided, they will override the ones provided during initialization.\n        **kwargs: The keyword arguments to pass to the `compile` method. Used to format the prompts placeholders.\n\n    Returns:\n        The response from the LLM\n    \"\"\"\n    llm_params = llm_params or self.llm_params  # Order is important -\n    if self.streaming is not None:\n        llm_params[\"stream\"] = self.streaming  # streaming should be the last param\n    # provided params during execution should override the ones provided during initialization\n    return self.llm.predict(**self.compile(**kwargs), **llm_params)\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/openai_llm/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> openai_llm","text":""},{"location":"reference/declarai/operators/openai_operators/openai_llm/#declarai.operators.openai_operators.openai_llm","title":"openai_llm","text":"<p>LLM implementation for OpenAI</p> <p>Classes:</p> Name Description <code>AzureOpenAILLM</code> <p>Azure OpenAI LLM implementation that uses openai sdk to make predictions with Azure's OpenAI.</p> <code>BaseOpenAILLM</code> <p>OpenAI LLM implementation that uses openai sdk to make predictions.</p> <code>OpenAIError</code> <p>Generic OpenAI error class when working with OpenAI provider.</p> <code>OpenAILLM</code> <p>OpenAI LLM implementation that uses openai sdk to make predictions.</p> <code>OpenAILLMParams</code> <p>OpenAI LLM Params when running execution</p> <p>Functions:</p> Name Description <code>handle_streaming_response</code> <p>Accumulate chunk deltas into a full response. Returns the full message.</p>"},{"location":"reference/declarai/operators/openai_operators/openai_llm/#declarai.operators.openai_operators.openai_llm.AzureOpenAILLM","title":"AzureOpenAILLM","text":"<pre><code>AzureOpenAILLM(\n    azure_openai_key: str,\n    azure_openai_api_base: str,\n    model: str,\n    api_version: str = None,\n    headers: dict = None,\n    timeout: int = None,\n    stream: bool = None,\n    request_timeout: int = None,\n)\n</code></pre> <p>             Bases: <code>BaseOpenAILLM</code></p> <p>Azure OpenAI LLM implementation that uses openai sdk to make predictions with Azure's OpenAI. Args:     azure_openai_key: Azure OpenAI API key     azure_openai_api_base: Azure OpenAI endpoint     model: Deployment name for the model in Azure     api_version: Azure API version     headers: Headers to use for the request     timeout: Timeout to use for the request     stream: Stream to use for the request     request_timeout: Request timeout to use for the request</p> <p>Methods:</p> Name Description <code>predict</code> <p>Predicts the next message using OpenAI</p> <p>Attributes:</p> Name Type Description <code>streaming</code> <code>bool</code> <p>Returns whether the LLM is streaming or not</p> Source code in <code>src/declarai/operators/openai_operators/openai_llm.py</code> <pre><code>def __init__(\n    self,\n    azure_openai_key: str,\n    azure_openai_api_base: str,\n    model: str,\n    api_version: str = None,\n    headers: dict = None,\n    timeout: int = None,\n    stream: bool = None,\n    request_timeout: int = None,\n):\n    model = model or DEPLOYMENT_NAME\n    api_key = azure_openai_key or AZURE_OPENAI_KEY\n    api_version = api_version or AZURE_API_VERSION\n    api_base = azure_openai_api_base or AZURE_OPENAI_API_BASE\n\n    super().__init__(\n        api_key,\n        \"azure\",\n        model,\n        headers,\n        timeout,\n        stream,\n        request_timeout,\n        engine=model,\n        api_version=api_version,\n        api_base=api_base,\n    )\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/openai_llm/#declarai.operators.openai_operators.openai_llm.AzureOpenAILLM.streaming","title":"streaming  <code>property</code>","text":"<pre><code>streaming: bool\n</code></pre> <p>Returns whether the LLM is streaming or not Returns:     bool: True if the LLM is streaming, False otherwise</p>"},{"location":"reference/declarai/operators/openai_operators/openai_llm/#declarai.operators.openai_operators.openai_llm.AzureOpenAILLM.predict","title":"predict","text":"<pre><code>predict(\n    messages: List[Message],\n    model: str = None,\n    temperature: float = 0,\n    max_tokens: int = 3000,\n    top_p: float = 1,\n    frequency_penalty: int = 0,\n    presence_penalty: int = 0,\n    stream: bool = None,\n) -&gt; Union[Iterator[LLMResponse], LLMResponse]\n</code></pre> <p>Predicts the next message using OpenAI Args:     stream: if to stream the response     messages: List of messages that are used as context for the prediction     model: the model to use for the prediction     temperature: the temperature to use for the prediction     max_tokens: the maximum number of tokens to use for the prediction     top_p: the top p to use for the prediction     frequency_penalty: the frequency penalty to use for the prediction     presence_penalty: the presence penalty to use for the prediction</p> <p>Returns:</p> Name Type Description <code>LLMResponse</code> <code>Union[Iterator[LLMResponse], LLMResponse]</code> <p>The response from the LLM</p> Source code in <code>src/declarai/operators/openai_operators/openai_llm.py</code> <pre><code>def predict(\n    self,\n    messages: List[Message],\n    model: str = None,\n    temperature: float = 0,\n    max_tokens: int = 3000,\n    top_p: float = 1,\n    frequency_penalty: int = 0,\n    presence_penalty: int = 0,\n    stream: bool = None,\n) -&gt; Union[Iterator[LLMResponse], LLMResponse]:\n\"\"\"\n    Predicts the next message using OpenAI\n    Args:\n        stream: if to stream the response\n        messages: List of messages that are used as context for the prediction\n        model: the model to use for the prediction\n        temperature: the temperature to use for the prediction\n        max_tokens: the maximum number of tokens to use for the prediction\n        top_p: the top p to use for the prediction\n        frequency_penalty: the frequency penalty to use for the prediction\n        presence_penalty: the presence penalty to use for the prediction\n\n    Returns:\n        LLMResponse: The response from the LLM\n\n    \"\"\"\n    if stream is None:\n        stream = self.stream\n    openai_messages = [{\"role\": m.role, \"content\": m.message} for m in messages]\n    res = self.openai.ChatCompletion.create(\n        model=model or self.model,\n        messages=openai_messages,\n        temperature=temperature,\n        max_tokens=max_tokens,\n        top_p=top_p,\n        frequency_penalty=frequency_penalty,\n        presence_penalty=presence_penalty,\n        api_key=self.api_key,\n        api_type=self.api_type,\n        stream=stream,\n        **self._kwargs,\n    )\n\n    if stream:\n        return handle_streaming_response(res)\n\n    else:\n        return LLMResponse(\n            response=res.choices[0][\"message\"][\"content\"],\n            model=res.model,\n            prompt_tokens=res[\"usage\"][\"prompt_tokens\"],\n            completion_tokens=res[\"usage\"][\"completion_tokens\"],\n            total_tokens=res[\"usage\"][\"total_tokens\"],\n            raw_response=res.to_dict_recursive(),\n        )\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/openai_llm/#declarai.operators.openai_operators.openai_llm.BaseOpenAILLM","title":"BaseOpenAILLM","text":"<pre><code>BaseOpenAILLM(\n    api_key: str,\n    api_type: str,\n    model_name: str,\n    headers: dict = None,\n    timeout: int = None,\n    stream: bool = None,\n    request_timeout: int = None,\n    **kwargs: int\n)\n</code></pre> <p>             Bases: <code>BaseLLM</code></p> <p>OpenAI LLM implementation that uses openai sdk to make predictions. Args:     openai_token: OpenAI API key     model: OpenAI model name Attributes:     openai (openai): OpenAI SDK     model (str): OpenAI model name</p> <p>Methods:</p> Name Description <code>predict</code> <p>Predicts the next message using OpenAI</p> <p>Attributes:</p> Name Type Description <code>streaming</code> <code>bool</code> <p>Returns whether the LLM is streaming or not</p> Source code in <code>src/declarai/operators/openai_operators/openai_llm.py</code> <pre><code>def __init__(\n    self,\n    api_key: str,\n    api_type: str,\n    model_name: str,\n    headers: dict = None,\n    timeout: int = None,\n    stream: bool = None,\n    request_timeout: int = None,\n    **kwargs,\n):\n    self._kwargs = {\n        \"headers\": headers,\n        \"timeout\": timeout,\n        \"request_timeout\": request_timeout,\n        **kwargs,\n    }\n    self.openai = openai\n    self.api_key = api_key\n    self.api_type = api_type\n    self.stream = stream\n    self.model = model_name\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/openai_llm/#declarai.operators.openai_operators.openai_llm.BaseOpenAILLM.streaming","title":"streaming  <code>property</code>","text":"<pre><code>streaming: bool\n</code></pre> <p>Returns whether the LLM is streaming or not Returns:     bool: True if the LLM is streaming, False otherwise</p>"},{"location":"reference/declarai/operators/openai_operators/openai_llm/#declarai.operators.openai_operators.openai_llm.BaseOpenAILLM.predict","title":"predict","text":"<pre><code>predict(\n    messages: List[Message],\n    model: str = None,\n    temperature: float = 0,\n    max_tokens: int = 3000,\n    top_p: float = 1,\n    frequency_penalty: int = 0,\n    presence_penalty: int = 0,\n    stream: bool = None,\n) -&gt; Union[Iterator[LLMResponse], LLMResponse]\n</code></pre> <p>Predicts the next message using OpenAI Args:     stream: if to stream the response     messages: List of messages that are used as context for the prediction     model: the model to use for the prediction     temperature: the temperature to use for the prediction     max_tokens: the maximum number of tokens to use for the prediction     top_p: the top p to use for the prediction     frequency_penalty: the frequency penalty to use for the prediction     presence_penalty: the presence penalty to use for the prediction</p> <p>Returns:</p> Name Type Description <code>LLMResponse</code> <code>Union[Iterator[LLMResponse], LLMResponse]</code> <p>The response from the LLM</p> Source code in <code>src/declarai/operators/openai_operators/openai_llm.py</code> <pre><code>def predict(\n    self,\n    messages: List[Message],\n    model: str = None,\n    temperature: float = 0,\n    max_tokens: int = 3000,\n    top_p: float = 1,\n    frequency_penalty: int = 0,\n    presence_penalty: int = 0,\n    stream: bool = None,\n) -&gt; Union[Iterator[LLMResponse], LLMResponse]:\n\"\"\"\n    Predicts the next message using OpenAI\n    Args:\n        stream: if to stream the response\n        messages: List of messages that are used as context for the prediction\n        model: the model to use for the prediction\n        temperature: the temperature to use for the prediction\n        max_tokens: the maximum number of tokens to use for the prediction\n        top_p: the top p to use for the prediction\n        frequency_penalty: the frequency penalty to use for the prediction\n        presence_penalty: the presence penalty to use for the prediction\n\n    Returns:\n        LLMResponse: The response from the LLM\n\n    \"\"\"\n    if stream is None:\n        stream = self.stream\n    openai_messages = [{\"role\": m.role, \"content\": m.message} for m in messages]\n    res = self.openai.ChatCompletion.create(\n        model=model or self.model,\n        messages=openai_messages,\n        temperature=temperature,\n        max_tokens=max_tokens,\n        top_p=top_p,\n        frequency_penalty=frequency_penalty,\n        presence_penalty=presence_penalty,\n        api_key=self.api_key,\n        api_type=self.api_type,\n        stream=stream,\n        **self._kwargs,\n    )\n\n    if stream:\n        return handle_streaming_response(res)\n\n    else:\n        return LLMResponse(\n            response=res.choices[0][\"message\"][\"content\"],\n            model=res.model,\n            prompt_tokens=res[\"usage\"][\"prompt_tokens\"],\n            completion_tokens=res[\"usage\"][\"completion_tokens\"],\n            total_tokens=res[\"usage\"][\"total_tokens\"],\n            raw_response=res.to_dict_recursive(),\n        )\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/openai_llm/#declarai.operators.openai_operators.openai_llm.OpenAIError","title":"OpenAIError","text":"<p>             Bases: <code>Exception</code></p> <p>Generic OpenAI error class when working with OpenAI provider.</p>"},{"location":"reference/declarai/operators/openai_operators/openai_llm/#declarai.operators.openai_operators.openai_llm.OpenAILLM","title":"OpenAILLM","text":"<pre><code>OpenAILLM(\n    openai_token: str = None,\n    model: str = None,\n    headers: dict = None,\n    timeout: int = None,\n    stream: bool = None,\n    request_timeout: int = None,\n)\n</code></pre> <p>             Bases: <code>BaseOpenAILLM</code></p> <p>OpenAI LLM implementation that uses openai sdk to make predictions. Args:     openai_token: OpenAI API key     model: OpenAI model name     headers: Headers to use for the request     timeout: Timeout to use for the request     stream: Stream to use for the request     request_timeout: Request timeout to use for the request</p> <p>Methods:</p> Name Description <code>predict</code> <p>Predicts the next message using OpenAI</p> <p>Attributes:</p> Name Type Description <code>streaming</code> <code>bool</code> <p>Returns whether the LLM is streaming or not</p> Source code in <code>src/declarai/operators/openai_operators/openai_llm.py</code> <pre><code>def __init__(\n    self,\n    openai_token: str = None,\n    model: str = None,\n    headers: dict = None,\n    timeout: int = None,\n    stream: bool = None,\n    request_timeout: int = None,\n):\n    openai_token = openai_token or OPENAI_API_KEY\n    model = model or OPENAI_MODEL\n    if not openai_token:\n        raise OpenAIError(\n            \"Missing an OpenAI API key\"\n            \"In order to work with OpenAI, you will need to provide an API key\"\n            \"either by setting the DECLARAI_OPENAI_API_KEY or by providing\"\n            \"the API key via the init interface.\"\n        )\n    if not model:\n        raise OpenAIError(\n            \"Missing an OpenAI model\"\n            \"In order to work with OpenAI, you will need to provide a model\"\n            \"either by setting the DECLARAI_OPENAI_MODEL or by providing\"\n            \"the model via the init interface.\"\n        )\n    super().__init__(\n        openai_token, \"openai\", model, headers, timeout, stream, request_timeout\n    )\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/openai_llm/#declarai.operators.openai_operators.openai_llm.OpenAILLM.streaming","title":"streaming  <code>property</code>","text":"<pre><code>streaming: bool\n</code></pre> <p>Returns whether the LLM is streaming or not Returns:     bool: True if the LLM is streaming, False otherwise</p>"},{"location":"reference/declarai/operators/openai_operators/openai_llm/#declarai.operators.openai_operators.openai_llm.OpenAILLM.predict","title":"predict","text":"<pre><code>predict(\n    messages: List[Message],\n    model: str = None,\n    temperature: float = 0,\n    max_tokens: int = 3000,\n    top_p: float = 1,\n    frequency_penalty: int = 0,\n    presence_penalty: int = 0,\n    stream: bool = None,\n) -&gt; Union[Iterator[LLMResponse], LLMResponse]\n</code></pre> <p>Predicts the next message using OpenAI Args:     stream: if to stream the response     messages: List of messages that are used as context for the prediction     model: the model to use for the prediction     temperature: the temperature to use for the prediction     max_tokens: the maximum number of tokens to use for the prediction     top_p: the top p to use for the prediction     frequency_penalty: the frequency penalty to use for the prediction     presence_penalty: the presence penalty to use for the prediction</p> <p>Returns:</p> Name Type Description <code>LLMResponse</code> <code>Union[Iterator[LLMResponse], LLMResponse]</code> <p>The response from the LLM</p> Source code in <code>src/declarai/operators/openai_operators/openai_llm.py</code> <pre><code>def predict(\n    self,\n    messages: List[Message],\n    model: str = None,\n    temperature: float = 0,\n    max_tokens: int = 3000,\n    top_p: float = 1,\n    frequency_penalty: int = 0,\n    presence_penalty: int = 0,\n    stream: bool = None,\n) -&gt; Union[Iterator[LLMResponse], LLMResponse]:\n\"\"\"\n    Predicts the next message using OpenAI\n    Args:\n        stream: if to stream the response\n        messages: List of messages that are used as context for the prediction\n        model: the model to use for the prediction\n        temperature: the temperature to use for the prediction\n        max_tokens: the maximum number of tokens to use for the prediction\n        top_p: the top p to use for the prediction\n        frequency_penalty: the frequency penalty to use for the prediction\n        presence_penalty: the presence penalty to use for the prediction\n\n    Returns:\n        LLMResponse: The response from the LLM\n\n    \"\"\"\n    if stream is None:\n        stream = self.stream\n    openai_messages = [{\"role\": m.role, \"content\": m.message} for m in messages]\n    res = self.openai.ChatCompletion.create(\n        model=model or self.model,\n        messages=openai_messages,\n        temperature=temperature,\n        max_tokens=max_tokens,\n        top_p=top_p,\n        frequency_penalty=frequency_penalty,\n        presence_penalty=presence_penalty,\n        api_key=self.api_key,\n        api_type=self.api_type,\n        stream=stream,\n        **self._kwargs,\n    )\n\n    if stream:\n        return handle_streaming_response(res)\n\n    else:\n        return LLMResponse(\n            response=res.choices[0][\"message\"][\"content\"],\n            model=res.model,\n            prompt_tokens=res[\"usage\"][\"prompt_tokens\"],\n            completion_tokens=res[\"usage\"][\"completion_tokens\"],\n            total_tokens=res[\"usage\"][\"total_tokens\"],\n            raw_response=res.to_dict_recursive(),\n        )\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/openai_llm/#declarai.operators.openai_operators.openai_llm.OpenAILLMParams","title":"OpenAILLMParams","text":"<p>             Bases: <code>BaseLLMParams</code></p> <p>OpenAI LLM Params when running execution</p> <p>Attributes:</p> Name Type Description <code>temperature</code> <code>Optional[float]</code> <p>the temperature to use for the prediction</p> <code>max_tokens</code> <code>Optional[int]</code> <p>the maximum number of tokens to use for the prediction</p> <code>top_p</code> <code>Optional[float]</code> <p>the top p to use for the prediction</p> <code>frequency_penalty</code> <code>Optional[int]</code> <p>the frequency penalty to use for the prediction</p> <code>presence_penalty</code> <code>Optional[int]</code> <p>the presence penalty to use for the prediction</p>"},{"location":"reference/declarai/operators/openai_operators/openai_llm/#declarai.operators.openai_operators.openai_llm.handle_streaming_response","title":"handle_streaming_response","text":"<pre><code>handle_streaming_response(\n    api_response: OpenAIObject,\n) -&gt; Iterator[LLMResponse]\n</code></pre> <p>Accumulate chunk deltas into a full response. Returns the full message.</p> Source code in <code>src/declarai/operators/openai_operators/openai_llm.py</code> <pre><code>def handle_streaming_response(api_response: OpenAIObject) -&gt; Iterator[LLMResponse]:\n\"\"\"\n    Accumulate chunk deltas into a full response. Returns the full message.\n    \"\"\"\n    response = {\"role\": None, \"response\": \"\", \"raw_response\": \"\"}\n\n    chunk: OpenAIObject\n    for chunk in api_response:  # noqa\n        response[\"raw_response\"] = chunk.to_dict_recursive()\n        delta = chunk.choices[0][\"delta\"]\n        response[\"model\"] = chunk.model\n        if chunk.get(\"usage\"):\n            response[\"prompt_tokens\"] = chunk.usage.get(\"prompt_tokens\")\n            response[\"completion_tokens\"] = chunk.usage.get(\"completion_tokens\")\n            response[\"total_tokens\"] = chunk.usage.get(\"total_tokens\")\n\n        if \"role\" in delta:\n            response[\"role\"] = delta[\"role\"]\n\n        if delta.get(\"function_call\"):\n            fn_call = delta.get(\"function_call\")\n            if \"function_call\" not in response[\"data\"]:\n                response[\"data\"][\"function_call\"] = {\"name\": None, \"arguments\": \"\"}\n            if \"name\" in fn_call:\n                response[\"data\"][\"function_call\"][\"name\"] = fn_call.name\n            if \"arguments\" in fn_call:\n                response[\"data\"][\"function_call\"][\"arguments\"] += (\n                    fn_call.arguments or \"\"\n                )\n\n        if \"content\" in delta:\n            response[\"response\"] += delta.content or \"\"\n\n        yield LLMResponse(**response)\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/settings/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> settings","text":""},{"location":"reference/declarai/operators/openai_operators/settings/#declarai.operators.openai_operators.settings","title":"settings","text":"<p>Environment level configurations for working with openai and Azure openai providers.</p> <p>Attributes:</p> Name Type Description <code>AZURE_API_VERSION</code> <code>str</code> <p>API version for Azure openai provider.</p> <code>AZURE_OPENAI_API_BASE</code> <code>str</code> <p>Endpoint for Azure openai provider.</p> <code>AZURE_OPENAI_KEY</code> <code>str</code> <p>API key for Azure openai provider.</p> <code>DEPLOYMENT_NAME</code> <code>str</code> <p>Deployment name for the model in Azure openai provider.</p> <code>OPENAI_API_KEY</code> <code>str</code> <p>API key for openai provider.</p> <code>OPENAI_MODEL</code> <code>str</code> <p>Model name for openai provider.</p>"},{"location":"reference/declarai/operators/openai_operators/settings/#declarai.operators.openai_operators.settings.AZURE_API_VERSION","title":"AZURE_API_VERSION  <code>module-attribute</code>","text":"<pre><code>AZURE_API_VERSION: str = getenv(\n    f\"{DECLARAI_PREFIX}_AZURE_API_VERSION\",\n    getenv(\"AZURE_API_VERSION\", \"2023-05-15\"),\n)\n</code></pre> <p>API version for Azure openai provider.</p>"},{"location":"reference/declarai/operators/openai_operators/settings/#declarai.operators.openai_operators.settings.AZURE_OPENAI_API_BASE","title":"AZURE_OPENAI_API_BASE  <code>module-attribute</code>","text":"<pre><code>AZURE_OPENAI_API_BASE: str = getenv(\n    f\"{DECLARAI_PREFIX}_AZURE_OPENAI_API_BASE\",\n    getenv(\"AZURE_OPENAI_API_BASE\", \"\"),\n)\n</code></pre> <p>Endpoint for Azure openai provider.</p>"},{"location":"reference/declarai/operators/openai_operators/settings/#declarai.operators.openai_operators.settings.AZURE_OPENAI_KEY","title":"AZURE_OPENAI_KEY  <code>module-attribute</code>","text":"<pre><code>AZURE_OPENAI_KEY: str = getenv(\n    f\"{DECLARAI_PREFIX}_AZURE_OPENAI_KEY\",\n    getenv(\"AZURE_OPENAI_KEY\", \"\"),\n)\n</code></pre> <p>API key for Azure openai provider.</p>"},{"location":"reference/declarai/operators/openai_operators/settings/#declarai.operators.openai_operators.settings.DEPLOYMENT_NAME","title":"DEPLOYMENT_NAME  <code>module-attribute</code>","text":"<pre><code>DEPLOYMENT_NAME: str = getenv(\n    f\"{DECLARAI_PREFIX}_AZURE_OPENAI_DEPLOYMENT_NAME\",\n    getenv(\"DEPLOYMENT_NAME\", \"\"),\n)\n</code></pre> <p>Deployment name for the model in Azure openai provider.</p>"},{"location":"reference/declarai/operators/openai_operators/settings/#declarai.operators.openai_operators.settings.OPENAI_API_KEY","title":"OPENAI_API_KEY  <code>module-attribute</code>","text":"<pre><code>OPENAI_API_KEY: str = getenv(\n    f\"{DECLARAI_PREFIX}_OPENAI_API_KEY\",\n    getenv(\"OPENAI_API_KEY\", \"\"),\n)\n</code></pre> <p>API key for openai provider.</p>"},{"location":"reference/declarai/operators/openai_operators/settings/#declarai.operators.openai_operators.settings.OPENAI_MODEL","title":"OPENAI_MODEL  <code>module-attribute</code>","text":"<pre><code>OPENAI_MODEL: str = getenv(\n    f\"{DECLARAI_PREFIX}_OPENAI_MODEL\", \"gpt-3.5-turbo\"\n)\n</code></pre> <p>Model name for openai provider.</p>"},{"location":"reference/declarai/operators/openai_operators/task_operator/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> task_operator","text":""},{"location":"reference/declarai/operators/openai_operators/task_operator/#declarai.operators.openai_operators.task_operator","title":"task_operator","text":"<p>Task implementation for openai operator.</p> <p>Classes:</p> Name Description <code>AzureOpenAITaskOperator</code> <p>Task implementation for openai operator that uses Azure as the llm provider.</p> <code>OpenAITaskOperator</code> <p>Task implementation for openai operator. This is a child of the BaseOperator class. See the BaseOperator class for further documentation.</p>"},{"location":"reference/declarai/operators/openai_operators/task_operator/#declarai.operators.openai_operators.task_operator.AzureOpenAITaskOperator","title":"AzureOpenAITaskOperator","text":"<p>             Bases: <code>OpenAITaskOperator</code></p> <p>Task implementation for openai operator that uses Azure as the llm provider.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <code>AzureOpenAILLM</code> <p>AzureOpenAILLM</p> <p>Methods:</p> Name Description <code>_compile_input_placeholder</code> <p>Creates a placeholder for the input of the function.</p> <code>compile</code> <p>Implements the compile method of the BaseOperator class.</p> <code>compile_template</code> <p>Unique compilation method for the OpenAITaskOperator class.</p> <code>parse_output</code> <p>Parses the raw output from the LLM into the desired format that was set in the parsed object.</p> <code>predict</code> <p>Executes prediction using the LLM.</p> <p>Attributes:</p> Name Type Description <code>streaming</code> <code>bool</code> <p>Returns whether the operator is streaming or not</p>"},{"location":"reference/declarai/operators/openai_operators/task_operator/#declarai.operators.openai_operators.task_operator.AzureOpenAITaskOperator.streaming","title":"streaming  <code>property</code>","text":"<pre><code>streaming: bool\n</code></pre> <p>Returns whether the operator is streaming or not Returns:</p>"},{"location":"reference/declarai/operators/openai_operators/task_operator/#declarai.operators.openai_operators.task_operator.AzureOpenAITaskOperator._compile_input_placeholder","title":"_compile_input_placeholder","text":"<pre><code>_compile_input_placeholder() -&gt; str\n</code></pre> <p>Creates a placeholder for the input of the function. The input format is based on the function input schema.</p> <p>Example</p> <p>for example a function signature of:     <pre><code>def foo(a: int, b: str, c: float = 1.0):\n</code></pre></p> <p>will result in the following placeholder: <pre><code>    Inputs:\n    a: {a}\n    b: {b}\n    c: {c}\n</code></pre></p> Source code in <code>src/declarai/operators/openai_operators/task_operator.py</code> <pre><code>def _compile_input_placeholder(self) -&gt; str:\n\"\"\"\n    Creates a placeholder for the input of the function.\n    The input format is based on the function input schema.\n\n    !!! example\n        for example a function signature of:\n            ```py\n            def foo(a: int, b: str, c: float = 1.0):\n            ```\n\n        will result in the following placeholder:\n        ```md\n            Inputs:\n            a: {a}\n            b: {b}\n            c: {c}\n        ```\n    \"\"\"\n    inputs = \"\"\n\n    if not self.parsed.signature_kwargs.keys():\n        return inputs\n\n    for i, param in enumerate(self.parsed.signature_kwargs.keys()):\n        if i == 0:\n            inputs += INPUT_LINE_TEMPLATE.format(param=param)\n            continue\n        inputs += NEW_LINE_INPUT_LINE_TEMPLATE.format(param=param)\n\n    return INPUTS_TEMPLATE.format(inputs=inputs)\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/task_operator/#declarai.operators.openai_operators.task_operator.AzureOpenAITaskOperator.compile","title":"compile","text":"<pre><code>compile(**kwargs) -&gt; CompiledTemplate\n</code></pre> <p>Implements the compile method of the BaseOperator class. Args:     **kwargs:</p> <p>Returns:</p> Type Description <code>CompiledTemplate</code> <p>Dict[str, List[Message]]: A dictionary containing a list of messages.</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def compile(self, **kwargs) -&gt; CompiledTemplate:\n\"\"\"\n    Implements the compile method of the BaseOperator class.\n    Args:\n        **kwargs:\n\n    Returns:\n        Dict[str, List[Message]]: A dictionary containing a list of messages.\n\n    \"\"\"\n    template = self.compile_template()\n    if kwargs:\n        template[-1].message = format_prompt_msg(\n            _string=template[-1].message, **kwargs\n        )\n    return {\"messages\": template}\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/task_operator/#declarai.operators.openai_operators.task_operator.AzureOpenAITaskOperator.compile_template","title":"compile_template","text":"<pre><code>compile_template() -&gt; CompiledTemplate\n</code></pre> <p>Unique compilation method for the OpenAITaskOperator class. Uses the InstructFunctionTemplate and StructuredOutputInstructionPrompt templates to create a message. And the _compile_input_placeholder method to create a placeholder for the input of the function. Returns:     Dict[str, List[Message]]: A dictionary containing a list of messages.</p> Source code in <code>src/declarai/operators/openai_operators/task_operator.py</code> <pre><code>def compile_template(self) -&gt; CompiledTemplate:\n\"\"\"\n    Unique compilation method for the OpenAITaskOperator class.\n    Uses the InstructFunctionTemplate and StructuredOutputInstructionPrompt templates to create a message.\n    And the _compile_input_placeholder method to create a placeholder for the input of the function.\n    Returns:\n        Dict[str, List[Message]]: A dictionary containing a list of messages.\n\n    \"\"\"\n    instruction_template = InstructFunctionTemplate\n    structured_template = StructuredOutputInstructionPrompt\n    output_schema = self._compile_output_prompt(structured_template)\n\n    messages = []\n    if output_schema:\n        messages.append(Message(message=output_schema, role=MessageRole.system))\n\n    if not can_be_jinja(self.parsed.docstring_freeform):\n        instruction_message = instruction_template.format(\n            input_instructions=self.parsed.docstring_freeform,\n            input_placeholder=self._compile_input_placeholder(),\n        )\n    else:\n        instruction_message = self.parsed.docstring_freeform\n\n    messages.append(Message(message=instruction_message, role=MessageRole.user))\n    return messages\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/task_operator/#declarai.operators.openai_operators.task_operator.AzureOpenAITaskOperator.parse_output","title":"parse_output","text":"<pre><code>parse_output(output: str) -&gt; Any\n</code></pre> <p>Parses the raw output from the LLM into the desired format that was set in the parsed object. Args:     output: llm string output</p> <p>Returns:</p> Type Description <code>Any</code> <p>Any parsed output</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def parse_output(self, output: str) -&gt; Any:\n\"\"\"\n    Parses the raw output from the LLM into the desired format that was set in the parsed object.\n    Args:\n        output: llm string output\n\n    Returns:\n        Any parsed output\n    \"\"\"\n    return self.parsed.parse(output)\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/task_operator/#declarai.operators.openai_operators.task_operator.AzureOpenAITaskOperator.predict","title":"predict","text":"<pre><code>predict(\n    *,\n    llm_params: Optional[LLMParamsType] = None,\n    **kwargs: object\n) -&gt; Union[LLMResponse, Iterator[LLMResponse]]\n</code></pre> <p>Executes prediction using the LLM. It first compiles the prompts using the <code>compile</code> method, and then executes the LLM with the compiled prompts and the llm_params. Args:     llm_params: The parameters that are passed during runtime. If provided, they will override the ones provided during initialization.     **kwargs: The keyword arguments to pass to the <code>compile</code> method. Used to format the prompts placeholders.</p> <p>Returns:</p> Type Description <code>Union[LLMResponse, Iterator[LLMResponse]]</code> <p>The response from the LLM</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def predict(\n    self, *, llm_params: Optional[LLMParamsType] = None, **kwargs: object\n) -&gt; Union[LLMResponse, Iterator[LLMResponse]]:\n\"\"\"\n    Executes prediction using the LLM.\n    It first compiles the prompts using the `compile` method, and then executes the LLM with the compiled prompts and the llm_params.\n    Args:\n        llm_params: The parameters that are passed during runtime. If provided, they will override the ones provided during initialization.\n        **kwargs: The keyword arguments to pass to the `compile` method. Used to format the prompts placeholders.\n\n    Returns:\n        The response from the LLM\n    \"\"\"\n    llm_params = llm_params or self.llm_params  # Order is important -\n    if self.streaming is not None:\n        llm_params[\"stream\"] = self.streaming  # streaming should be the last param\n    # provided params during execution should override the ones provided during initialization\n    return self.llm.predict(**self.compile(**kwargs), **llm_params)\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/task_operator/#declarai.operators.openai_operators.task_operator.OpenAITaskOperator","title":"OpenAITaskOperator","text":"<p>             Bases: <code>BaseOperator</code></p> <p>Task implementation for openai operator. This is a child of the BaseOperator class. See the BaseOperator class for further documentation. Implements the compile method which compiles a parsed function into a message. Uses the OpenAILLM to generate a response based on the given template.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <code>OpenAILLM</code> <p>OpenAILLM</p> <p>Methods:</p> Name Description <code>_compile_input_placeholder</code> <p>Creates a placeholder for the input of the function.</p> <code>compile</code> <p>Implements the compile method of the BaseOperator class.</p> <code>compile_template</code> <p>Unique compilation method for the OpenAITaskOperator class.</p> <code>parse_output</code> <p>Parses the raw output from the LLM into the desired format that was set in the parsed object.</p> <code>predict</code> <p>Executes prediction using the LLM.</p> <p>Attributes:</p> Name Type Description <code>streaming</code> <code>bool</code> <p>Returns whether the operator is streaming or not</p>"},{"location":"reference/declarai/operators/openai_operators/task_operator/#declarai.operators.openai_operators.task_operator.OpenAITaskOperator.streaming","title":"streaming  <code>property</code>","text":"<pre><code>streaming: bool\n</code></pre> <p>Returns whether the operator is streaming or not Returns:</p>"},{"location":"reference/declarai/operators/openai_operators/task_operator/#declarai.operators.openai_operators.task_operator.OpenAITaskOperator._compile_input_placeholder","title":"_compile_input_placeholder","text":"<pre><code>_compile_input_placeholder() -&gt; str\n</code></pre> <p>Creates a placeholder for the input of the function. The input format is based on the function input schema.</p> <p>Example</p> <p>for example a function signature of:     <pre><code>def foo(a: int, b: str, c: float = 1.0):\n</code></pre></p> <p>will result in the following placeholder: <pre><code>    Inputs:\n    a: {a}\n    b: {b}\n    c: {c}\n</code></pre></p> Source code in <code>src/declarai/operators/openai_operators/task_operator.py</code> <pre><code>def _compile_input_placeholder(self) -&gt; str:\n\"\"\"\n    Creates a placeholder for the input of the function.\n    The input format is based on the function input schema.\n\n    !!! example\n        for example a function signature of:\n            ```py\n            def foo(a: int, b: str, c: float = 1.0):\n            ```\n\n        will result in the following placeholder:\n        ```md\n            Inputs:\n            a: {a}\n            b: {b}\n            c: {c}\n        ```\n    \"\"\"\n    inputs = \"\"\n\n    if not self.parsed.signature_kwargs.keys():\n        return inputs\n\n    for i, param in enumerate(self.parsed.signature_kwargs.keys()):\n        if i == 0:\n            inputs += INPUT_LINE_TEMPLATE.format(param=param)\n            continue\n        inputs += NEW_LINE_INPUT_LINE_TEMPLATE.format(param=param)\n\n    return INPUTS_TEMPLATE.format(inputs=inputs)\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/task_operator/#declarai.operators.openai_operators.task_operator.OpenAITaskOperator.compile","title":"compile","text":"<pre><code>compile(**kwargs) -&gt; CompiledTemplate\n</code></pre> <p>Implements the compile method of the BaseOperator class. Args:     **kwargs:</p> <p>Returns:</p> Type Description <code>CompiledTemplate</code> <p>Dict[str, List[Message]]: A dictionary containing a list of messages.</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def compile(self, **kwargs) -&gt; CompiledTemplate:\n\"\"\"\n    Implements the compile method of the BaseOperator class.\n    Args:\n        **kwargs:\n\n    Returns:\n        Dict[str, List[Message]]: A dictionary containing a list of messages.\n\n    \"\"\"\n    template = self.compile_template()\n    if kwargs:\n        template[-1].message = format_prompt_msg(\n            _string=template[-1].message, **kwargs\n        )\n    return {\"messages\": template}\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/task_operator/#declarai.operators.openai_operators.task_operator.OpenAITaskOperator.compile_template","title":"compile_template","text":"<pre><code>compile_template() -&gt; CompiledTemplate\n</code></pre> <p>Unique compilation method for the OpenAITaskOperator class. Uses the InstructFunctionTemplate and StructuredOutputInstructionPrompt templates to create a message. And the _compile_input_placeholder method to create a placeholder for the input of the function. Returns:     Dict[str, List[Message]]: A dictionary containing a list of messages.</p> Source code in <code>src/declarai/operators/openai_operators/task_operator.py</code> <pre><code>def compile_template(self) -&gt; CompiledTemplate:\n\"\"\"\n    Unique compilation method for the OpenAITaskOperator class.\n    Uses the InstructFunctionTemplate and StructuredOutputInstructionPrompt templates to create a message.\n    And the _compile_input_placeholder method to create a placeholder for the input of the function.\n    Returns:\n        Dict[str, List[Message]]: A dictionary containing a list of messages.\n\n    \"\"\"\n    instruction_template = InstructFunctionTemplate\n    structured_template = StructuredOutputInstructionPrompt\n    output_schema = self._compile_output_prompt(structured_template)\n\n    messages = []\n    if output_schema:\n        messages.append(Message(message=output_schema, role=MessageRole.system))\n\n    if not can_be_jinja(self.parsed.docstring_freeform):\n        instruction_message = instruction_template.format(\n            input_instructions=self.parsed.docstring_freeform,\n            input_placeholder=self._compile_input_placeholder(),\n        )\n    else:\n        instruction_message = self.parsed.docstring_freeform\n\n    messages.append(Message(message=instruction_message, role=MessageRole.user))\n    return messages\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/task_operator/#declarai.operators.openai_operators.task_operator.OpenAITaskOperator.parse_output","title":"parse_output","text":"<pre><code>parse_output(output: str) -&gt; Any\n</code></pre> <p>Parses the raw output from the LLM into the desired format that was set in the parsed object. Args:     output: llm string output</p> <p>Returns:</p> Type Description <code>Any</code> <p>Any parsed output</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def parse_output(self, output: str) -&gt; Any:\n\"\"\"\n    Parses the raw output from the LLM into the desired format that was set in the parsed object.\n    Args:\n        output: llm string output\n\n    Returns:\n        Any parsed output\n    \"\"\"\n    return self.parsed.parse(output)\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/task_operator/#declarai.operators.openai_operators.task_operator.OpenAITaskOperator.predict","title":"predict","text":"<pre><code>predict(\n    *,\n    llm_params: Optional[LLMParamsType] = None,\n    **kwargs: object\n) -&gt; Union[LLMResponse, Iterator[LLMResponse]]\n</code></pre> <p>Executes prediction using the LLM. It first compiles the prompts using the <code>compile</code> method, and then executes the LLM with the compiled prompts and the llm_params. Args:     llm_params: The parameters that are passed during runtime. If provided, they will override the ones provided during initialization.     **kwargs: The keyword arguments to pass to the <code>compile</code> method. Used to format the prompts placeholders.</p> <p>Returns:</p> Type Description <code>Union[LLMResponse, Iterator[LLMResponse]]</code> <p>The response from the LLM</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def predict(\n    self, *, llm_params: Optional[LLMParamsType] = None, **kwargs: object\n) -&gt; Union[LLMResponse, Iterator[LLMResponse]]:\n\"\"\"\n    Executes prediction using the LLM.\n    It first compiles the prompts using the `compile` method, and then executes the LLM with the compiled prompts and the llm_params.\n    Args:\n        llm_params: The parameters that are passed during runtime. If provided, they will override the ones provided during initialization.\n        **kwargs: The keyword arguments to pass to the `compile` method. Used to format the prompts placeholders.\n\n    Returns:\n        The response from the LLM\n    \"\"\"\n    llm_params = llm_params or self.llm_params  # Order is important -\n    if self.streaming is not None:\n        llm_params[\"stream\"] = self.streaming  # streaming should be the last param\n    # provided params during execution should override the ones provided during initialization\n    return self.llm.predict(**self.compile(**kwargs), **llm_params)\n</code></pre>"},{"location":"reference/declarai/operators/templates/","title":"Index","text":""},{"location":"reference/declarai/operators/templates/#declarai.operators.templates","title":"templates","text":"<p>This module contains the shared templates for the operators.</p> <p>Modules:</p> Name Description <code>chain_of_thought</code> <p>Chain of thoughts templates.</p> <code>instruct_function</code> <p>Instruct Function Template</p> <code>output_prompt</code> <p>The logic for constructing the output prompt.</p> <code>output_structure</code> <p>The prompt templates for the format of the output.</p>"},{"location":"reference/declarai/operators/templates/chain_of_thought/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> chain_of_thought","text":""},{"location":"reference/declarai/operators/templates/chain_of_thought/#declarai.operators.templates.chain_of_thought","title":"chain_of_thought","text":"<p>Chain of thoughts templates.</p> <p>Attributes:</p> Name Type Description <code>ChainOfThoughtsTemplate</code> <p>.</p>"},{"location":"reference/declarai/operators/templates/chain_of_thought/#declarai.operators.templates.chain_of_thought.ChainOfThoughtsTemplate","title":"ChainOfThoughtsTemplate  <code>module-attribute</code>","text":"<pre><code>ChainOfThoughtsTemplate = \"The following task should be done in {num_steps} steps:\\nUse the output of the previous step as the input of the next step.\\n{steps}\\n\\nLet's think step by step\"\n</code></pre> <p>.</p>"},{"location":"reference/declarai/operators/templates/instruct_function/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> instruct_function","text":""},{"location":"reference/declarai/operators/templates/instruct_function/#declarai.operators.templates.instruct_function","title":"instruct_function","text":"<p>Instruct Function Template</p> <p>Attributes:</p> Name Type Description <code>InstructFunctionTemplate</code> <p>.</p>"},{"location":"reference/declarai/operators/templates/instruct_function/#declarai.operators.templates.instruct_function.InstructFunctionTemplate","title":"InstructFunctionTemplate  <code>module-attribute</code>","text":"<pre><code>InstructFunctionTemplate = (\n    \"{input_instructions}\\n{input_placeholder}\\n\"\n)\n</code></pre> <p>.</p>"},{"location":"reference/declarai/operators/templates/output_prompt/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> output_prompt","text":""},{"location":"reference/declarai/operators/templates/output_prompt/#declarai.operators.templates.output_prompt","title":"output_prompt","text":"<p>The logic for constructing the output prompt. These methods accept all \"return\" related properties of the python function and build a string output prompt from them.</p> <p>Functions:</p> Name Description <code>compile_output_prompt</code> <p>Compiles the output prompt for given function properties.</p> <code>compile_unstructured_template</code> <p>Compiles the output prompt for unstructured output but where still a return type is expected (for example int, float).</p>"},{"location":"reference/declarai/operators/templates/output_prompt/#declarai.operators.templates.output_prompt.compile_output_prompt","title":"compile_output_prompt","text":"<pre><code>compile_output_prompt(\n    str_schema: str,\n    return_type: str,\n    return_docstring: str,\n    return_magic: str = None,\n    structured: Optional[bool] = True,\n    structured_template: Optional[str] = None,\n) -&gt; str\n</code></pre> <p>Compiles the output prompt for given function properties. Args:     str_schema: tbd     return_type: tbd     return_docstring: tbd     return_magic: tbd     structured: tbd     structured_template: tbd</p> <p>Returns:</p> Source code in <code>src/declarai/operators/templates/output_prompt.py</code> <pre><code>def compile_output_prompt(\n    str_schema: str,\n    return_type: str,\n    return_docstring: str,\n    return_magic: str = None,\n    structured: Optional[bool] = True,\n    structured_template: Optional[str] = None,\n) -&gt; str:\n\"\"\"\n    Compiles the output prompt for given function properties.\n    Args:\n        str_schema: tbd\n        return_type: tbd\n        return_docstring: tbd\n        return_magic: tbd\n        structured: tbd\n        structured_template: tbd\n\n    Returns:\n\n    \"\"\"\n    str_schema = str_schema or return_magic\n\n    if not structured:\n        return compile_unstructured_template(return_type, return_docstring)\n\n    return compile_output_schema_template(\n        str_schema, return_type, return_docstring, structured_template\n    )\n</code></pre>"},{"location":"reference/declarai/operators/templates/output_prompt/#declarai.operators.templates.output_prompt.compile_unstructured_template","title":"compile_unstructured_template","text":"<pre><code>compile_unstructured_template(\n    return_type: str, return_docstring: str\n) -&gt; str\n</code></pre> <p>Compiles the output prompt for unstructured output but where still a return type is expected (for example int, float). Args:     return_type: the type of the return value     return_docstring: the description of the return value</p> <p>Returns:</p> Source code in <code>src/declarai/operators/templates/output_prompt.py</code> <pre><code>def compile_unstructured_template(return_type: str, return_docstring: str) -&gt; str:\n\"\"\"\n    Compiles the output prompt for unstructured output but where still a return type is expected (for example int, float).\n    Args:\n        return_type: the type of the return value\n        return_docstring: the description of the return value\n\n    Returns:\n\n    \"\"\"\n    if return_type == \"str\":\n        return \"\"\n    output_prompt = \"\"\n    if return_type:\n        output_prompt += f\"respond only with the value of type {return_type}:\"\n    if return_docstring:\n        output_prompt += f\"  # {return_docstring}\"\n\n    return output_prompt\n</code></pre>"},{"location":"reference/declarai/operators/templates/output_structure/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> output_structure","text":""},{"location":"reference/declarai/operators/templates/output_structure/#declarai.operators.templates.output_structure","title":"output_structure","text":"<p>The prompt templates for the format of the output.</p> <p>Attributes:</p> Name Type Description <code>StructuredOutputChatPrompt</code> <p>.</p> <code>StructuredOutputInstructionPrompt</code> <p>.</p>"},{"location":"reference/declarai/operators/templates/output_structure/#declarai.operators.templates.output_structure.StructuredOutputChatPrompt","title":"StructuredOutputChatPrompt  <code>module-attribute</code>","text":"<pre><code>StructuredOutputChatPrompt = \"Your responses should be a JSON structure with a single key named '{return_name}', nothing else. The expected format is: {output_schema}\"\n</code></pre> <p>.</p>"},{"location":"reference/declarai/operators/templates/output_structure/#declarai.operators.templates.output_structure.StructuredOutputInstructionPrompt","title":"StructuredOutputInstructionPrompt  <code>module-attribute</code>","text":"<pre><code>StructuredOutputInstructionPrompt = \"You are a REST api endpoint.You only answer in JSON structures\\nwith a single key named '{return_name}', nothing else.\\nThe expected format is:\\n{output_schema}\"\n</code></pre> <p>.</p>"},{"location":"reference/declarai/python_parser/","title":"Index","text":""},{"location":"reference/declarai/python_parser/#declarai.python_parser","title":"python_parser","text":"<p>Internal package for parsing User Python code into a ParsedFunction object.</p> <p>Modules:</p> Name Description <code>docstring_parsers</code> <code>parser</code> <p>PythonParser</p> <code>type_annotation_to_schema</code>"},{"location":"reference/declarai/python_parser/magic_parser/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> magic_parser","text":""},{"location":"reference/declarai/python_parser/magic_parser/#declarai.python_parser.magic_parser","title":"magic_parser","text":""},{"location":"reference/declarai/python_parser/parser/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> parser","text":""},{"location":"reference/declarai/python_parser/parser/#declarai.python_parser.parser","title":"parser","text":"<p>PythonParser An interface to extract different parts of the provided python code into a simple metadata object.</p> <p>Classes:</p> Name Description <code>PythonParser</code> <p>A unified interface for accessing python parsed data.</p>"},{"location":"reference/declarai/python_parser/parser/#declarai.python_parser.parser.PythonParser","title":"PythonParser","text":"<pre><code>PythonParser(decorated: Any)\n</code></pre> <p>A unified interface for accessing python parsed data.</p> <p>Attributes:</p> Name Type Description <code>has_any_return_defs</code> <code>bool</code> <p>A return definition is any of the following:</p> <code>has_structured_return_type</code> <code>bool</code> <p>Except for the following types, a dedicated output parsing</p> Source code in <code>src/declarai/python_parser/parser.py</code> <pre><code>def __init__(self, decorated: Any):\n    self.is_func = inspect.isfunction(decorated)\n    self.is_class = inspect.isclass(decorated)\n    self.decorated = decorated\n\n    # Static attributes:\n    self.name = self.decorated.__name__\n\n    self._signature = inspect.signature(self.decorated)\n    self.signature_return_type = self.signature_return.type_\n\n    docstring = inspect.getdoc(self.decorated)\n    self._parsed_docstring = ReSTDocstringParser(docstring or \"\")\n    self.docstring_freeform = self._parsed_docstring.freeform\n    self.docstring_params = self._parsed_docstring.params\n    self.docstring_return = self._parsed_docstring.returns\n</code></pre>"},{"location":"reference/declarai/python_parser/parser/#declarai.python_parser.parser.PythonParser.has_any_return_defs","title":"has_any_return_defs  <code>cached</code> <code>property</code>","text":"<pre><code>has_any_return_defs: bool\n</code></pre> <p>A return definition is any of the following: - return type annotation - return reference in docstring - return referenced in magic placeholder  # TODO: Address magic reference as well.</p>"},{"location":"reference/declarai/python_parser/parser/#declarai.python_parser.parser.PythonParser.has_structured_return_type","title":"has_structured_return_type  <code>cached</code> <code>property</code>","text":"<pre><code>has_structured_return_type: bool\n</code></pre> <p>Except for the following types, a dedicated output parsing behavior is required to return the expected return type of the task.</p>"},{"location":"reference/declarai/python_parser/type_annotation_to_schema/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> type_annotation_to_schema","text":""},{"location":"reference/declarai/python_parser/type_annotation_to_schema/#declarai.python_parser.type_annotation_to_schema","title":"type_annotation_to_schema","text":"<p>Functions:</p> Name Description <code>type_annotation_to_str_schema</code> <p>This method accepts arbitrary types defined in the return annotation of a functions.</p>"},{"location":"reference/declarai/python_parser/type_annotation_to_schema/#declarai.python_parser.type_annotation_to_schema.type_annotation_to_str_schema","title":"type_annotation_to_str_schema","text":"<pre><code>type_annotation_to_str_schema(type_) -&gt; Optional[str]\n</code></pre> <p>This method accepts arbitrary types defined in the return annotation of a functions. Then creates a string representation of the annotation schema to be passed to the model.</p> Source code in <code>src/declarai/python_parser/type_annotation_to_schema.py</code> <pre><code>def type_annotation_to_str_schema(type_) -&gt; Optional[str]:\n\"\"\"\n    This method accepts arbitrary types defined in the return annotation of a functions.\n    Then creates a string representation of the annotation schema to be passed to the model.\n    \"\"\"\n    if type_.__module__ == \"builtins\":\n        if type_ in (str, int, float, bool):\n            return type_.__name__\n\n    if isinstance(type_, typing._GenericAlias):\n        root_name = type_._name\n        if not root_name:\n            if type_.__origin__ == typing.Union:\n                root_name = \"Union\"\n        properties = []\n        for sub_type in type_.__args__:\n            resolved_schema = resolve_to_json_schema(sub_type)\n            properties.append(resolve_pydantic_schema_recursive(resolved_schema))\n\n        if len(properties) &gt; 1:\n            resolved_str_schema = f\"{root_name}[{properties[0]}, {properties[1]}]\"\n        else:\n            resolved_str_schema = f\"{root_name}[{properties[0]}]\"\n        return schema_to_string_for_prompt(resolved_str_schema)\n\n    resolved_schema = resolve_to_json_schema(type_)\n    resolved_schema = resolve_pydantic_schema_recursive(resolved_schema)\n    str_schema = json.dumps(resolved_schema, indent=4)\n    return schema_to_string_for_prompt(str_schema)\n</code></pre>"},{"location":"reference/declarai/python_parser/types/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> types","text":""},{"location":"reference/declarai/python_parser/types/#declarai.python_parser.types","title":"types","text":""},{"location":"reference/declarai/python_parser/docstring_parsers/","title":"Index","text":""},{"location":"reference/declarai/python_parser/docstring_parsers/#declarai.python_parser.docstring_parsers","title":"docstring_parsers","text":"<p>Modules:</p> Name Description <code>reST</code> <code>types</code>"},{"location":"reference/declarai/python_parser/docstring_parsers/types/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> types","text":""},{"location":"reference/declarai/python_parser/docstring_parsers/types/#declarai.python_parser.docstring_parsers.types","title":"types","text":"<p>Classes:</p> Name Description <code>BaseDocStringParser</code> <p>Base class for docstring parsers.</p>"},{"location":"reference/declarai/python_parser/docstring_parsers/types/#declarai.python_parser.docstring_parsers.types.BaseDocStringParser","title":"BaseDocStringParser","text":"<p>             Bases: <code>ABC</code></p> <p>Base class for docstring parsers.</p> <p>Attributes:</p> Name Type Description <code>freeform</code> <code>DocstringFreeform</code> <p>Return the freeform docstring</p> <code>params</code> <code>DocstringParams</code> <p>Return the params/arguments docstring</p> <code>returns</code> <code>DocstringReturn</code> <p>Return the return docstring</p>"},{"location":"reference/declarai/python_parser/docstring_parsers/types/#declarai.python_parser.docstring_parsers.types.BaseDocStringParser.freeform","title":"freeform  <code>property</code>","text":"<pre><code>freeform: DocstringFreeform\n</code></pre> <p>Return the freeform docstring</p>"},{"location":"reference/declarai/python_parser/docstring_parsers/types/#declarai.python_parser.docstring_parsers.types.BaseDocStringParser.params","title":"params  <code>property</code>","text":"<pre><code>params: DocstringParams\n</code></pre> <p>Return the params/arguments docstring</p>"},{"location":"reference/declarai/python_parser/docstring_parsers/types/#declarai.python_parser.docstring_parsers.types.BaseDocStringParser.returns","title":"returns  <code>property</code>","text":"<pre><code>returns: DocstringReturn\n</code></pre> <p>Return the return docstring</p>"},{"location":"reference/declarai/python_parser/docstring_parsers/reST/","title":"Index","text":""},{"location":"reference/declarai/python_parser/docstring_parsers/reST/#declarai.python_parser.docstring_parsers.reST","title":"reST","text":"<p>Modules:</p> Name Description <code>parser</code>"},{"location":"reference/declarai/python_parser/docstring_parsers/reST/parser/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> parser","text":""},{"location":"reference/declarai/python_parser/docstring_parsers/reST/parser/#declarai.python_parser.docstring_parsers.reST.parser","title":"parser","text":"<p>Classes:</p> Name Description <code>ReSTDocstringParser</code> <p>As recommended by (PEP 287)[https://peps.python.org/pep-0287/],</p>"},{"location":"reference/declarai/python_parser/docstring_parsers/reST/parser/#declarai.python_parser.docstring_parsers.reST.parser.ReSTDocstringParser","title":"ReSTDocstringParser","text":"<pre><code>ReSTDocstringParser(docstring: str)\n</code></pre> <p>             Bases: <code>BaseDocStringParser</code></p> <p>As recommended by (PEP 287)[https://peps.python.org/pep-0287/], the recommended docstring format is the reStructuredText format (shortform - reST).</p> Source code in <code>src/declarai/python_parser/docstring_parsers/reST/parser.py</code> <pre><code>def __init__(self, docstring: str):\n    self.docstring = docstring\n</code></pre>"}]}